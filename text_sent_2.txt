accelerat ing he world research
printer opaque this an introduction to statistical learning with applications in de kang yuan related papers download pdf pack of he best relat ed papers int roduct ion st at ist ical learning yi yao an int roduct ion to st at ist ical learning wit applicat ions in islr sixt print ing ym xue springer text in st at ist ics an int roduct ion st at ist ical learning springer text in st at ist ics an int fabio lopes
this is page ii printer opaque this to our parents alison and michael james chiara nappi and edward witten valerie and patrick hastie vera and sami tibshirani and to our families michael daniel and catherine ari samantha timothy and lynda charlie ryan julie and cheryl
this is page iii printer opaque this preface statistical learning refers to set of tools for modeling and understanding complex datasets
it is recently developed area in statistics and blends with parallel developments in computer science and in particular machine learning
the field encompasses many methods such as the lasso and sparse regression classification and regression trees and boosting and support vector machines
with the explosion of big data problems statistical learning has become very hot field in many scientific areas as well as marketing finance and other business disciplines
people with statistical learning skills are in high demand
one of the first books in this area the elements of statistical learning esl hastie tibshirani and friedman was published in with second edition in
esl has become popular text not only in statistics but also in related fields
one of the reasons for esl's popularity is its relatively accessible style
but esl is intended for individuals with advanced training in the mathematical sciences
an introduction to statistical learning isl arose from the perceived need for broader and less technical treatment of these topics
in this new book we cover many of the same topics as esl but we concentrate more on the applications of the methods and less on the mathematical details
we have created labs illustrating how to implement each of the statistical learning methods using the popular statistical software package
these labs provide the reader with valuable hands on experience
this book is appropriate for advanced undergraduates or master's students in statistics or related quantitative fields or for individuals in other
this is page iv printer opaque this disciplines who wish to use statistical learning tools to analyze their data
it can be used as textbook for course spanning one or two semesters
we would like to thank several readers for valuable comments on preliminary drafts of this book pallavi basu alexandra chouldechova patrick danaher will fithian luella fu sam gross max grazier g'sell courtney paulson xinghao qiao elisa sheng noah simon kean ming tan xin lu tan
it's tough to make predictions especially about the future yogi berra
this is page printer opaque this contents preface iii introduction statistical learning what is statistical learning
why estimate
how do we estimate
the trade off between prediction accuracy and model interpretability
supervised versus unsupervised learning
regression versus classification problems
assessing model accuracy
measuring the quality of fit
the bias variance trade off
the classification setting
lab introduction to
basic commands
indexing data
loading data
additional graphical and numerical summaries
ii contents linear regression simple linear regression
estimating the coefficients
assessing the accuracy of the coefficient estimates assessing the accuracy of the model
multiple linear regression
estimating the regression coefficients
some important questions
other considerations in the regression model
qualitative predictors
extensions of the linear model
potential problems
the marketing plan
comparison of linear regression with nearest neighbors lab linear regression
simple linear regression
multiple linear regression
interaction terms
non linear transformations of the predictors
qualitative predictors
writing functions
classification an overview of classification
why not linear regression
logistic regression
the logistic model
estimating the regression coefficients
making predictions
multiple logistic regression
logistic regression for response classes
linear discriminant analysis
using bayes theorem for classification
linear discriminant analysis for
linear discriminant analysis for
quadratic discriminant analysis
comparison of classification methods
lab logistic regression lda qda and knn
the stock market data
logistic regression
linear discriminant analysis
quadratic discriminant analysis
nearest neighbors
contents iii an application to caravan insurance data
resampling methods cross validation
the validation set approach
leave one out cross validation
fold cross validation
bias variance trade off for fold cross validation cross validation on classification problems
the bootstrap
lab cross validation and the bootstrap
the validation set approach
leave one out cross validation
fold cross validation
the bootstrap
linear model selection and regularization subset selection
best subset selection
stepwise selection
choosing the optimal model
shrinkage methods
ridge regression
the lasso
selecting the tuning parameter
dimension reduction methods
principal components regression
partial least squares
considerations in high dimensions
high dimensional data
what goes wrong in high dimensions
regression in high dimensions
interpreting results in high dimensions
lab subset selection methods
best subset selection
forward and backward stepwise selection
choosing among models using the validation set approach and cross validation
lab ridge regression and the lasso
ridge regression
the lasso
lab pcr and pls regression
principal components regression
iv contents partial least squares
moving beyond linearity polynomial regression
step functions
basis functions
regression splines
piecewise polynomials
constraints and splines
the spline basis representation
choosing the number and locations of the knots
comparison to polynomial regression
smoothing splines
an overview of smoothing splines
choosing the smoothing parameter bb
local regression
generalized additive models
gams for regression problems
gams for classification problems
lab non linear modeling
polynomial regression and step functions
tree based methods the basics of decision trees
regression trees
classification trees
trees versus linear models
advantages and disadvantages of trees
bagging random forests boosting
random forests
lab decision trees
fitting classification trees
fitting regression trees
bagging and random forests
support vector machines maximal margin classifier
contents what is hyperplane
classification using separating hyperplane
the maximal margin classifier
construction of the maximal margin classifier
the non separable case
support vector classifiers
overview of the support vector classifier
details of the support vector classifier
support vector machines
classification with non linear decision boundaries
the support vector machine
an application to the heart disease data
svms with more than two classes
one versus one classification
one versus all classification
relationship to logistic regression
lab support vector machines
support vector classifier
support vector machine
roc curves
svm with multiple classes
application to gene expression data
unsupervised learning the challenge of unsupervised learning
principal components analysis
what are principal components
another interpretation of principal components
more on pca
other uses for principal components
clustering methods
means clustering
hierarchical clustering
practical issues in clustering
lab principal components analysis
lab clustering
means clustering
hierarchical clustering
lab nci data example
pca on the nci data
clustering the observations of the nci data
this is page printer opaque this introduction an overview of statistical learning statistical learning refers to vast set of tools for understanding data
these tools can be classified as supervised or unsupervised
broadly speaking supervised statistical learning involves building statistical model for predicting or estimating an output based on one or more inputs
problems of this nature occur in fields as diverse as business medicine astrophysics and public policy
with unsupervised statistical learning there are inputs but no supervising output nevertheless we can learn relationships and structure from such data
to provide an illustration of some applications of statistical learning we briefly discuss three real world data sets that are considered in this book
wage data in this application which we refer to as the wage data set throughout this book we examine number of factors that relate to wages for group of males from the atlantic region of the united states
in particular we wish to understand the association between an employee's age and education as well as the calendar year on his wage
consider for example the lefthand panel of figure which displays wage versus age for each of the individuals in the data set
there is evidence that wage increases with age but then decreases again after approximately age
the blue line which provides an estimate of the average wage for given age makes this trend
wage data which contains income survey information for males from the central atlantic region of the united states
left wage as function of age
on average wage increases with age until about years of age at which point it begins to decline
center wage as function of year
there is slow but steady increase of approximately in the average wage between and
right boxplots displaying wage as function of education with indicating the lowest level no high school diploma and the highest level an advanced graduate degree
on average wage increases with the level of education clearer
given an employee's age we can use this curve to predict his wage
however it is also clear from figure that there is significant amount of variability associated with this average value and so age alone is unlikely to provide an accurate prediction of particular man's wage
we also have information regarding each employee's education level and the year in which the wage was earned
the center and right hand panels of figure which display wage as function of both year and education indicate that both of these factors are associated with wage
wages increase by approximately in roughly linear or straight line fashion between and though this rise is very slight relative to the variability in the data
wages are also typically greater for individuals with higher education levels men with the lowest education level tend to have substantially lower wages than those with the highest education level
clearly the most accurate prediction of given man's wage will be obtained by combining his age his education and the year
in chapter we discuss linear regression which can be used to predict wage from this data set
ideally we should predict wage in way that accounts for the non linear relationship between wage and age
in chapter we discuss class of approaches for addressing this problem
left boxplots of the previous day's percentage change in the index for the days for which the market increased or decreased obtained from the smarket data
center and right same as left panel but the percentage changes for two and three days previous are shown
stock market data the wage data involves predicting continuous or quantitative output value
this is often referred to as regression problem
however in certain cases we may instead wish to predict non numerical value that is categorical or qualitative output
for example in chapter we examine stock market data set that contains the daily movements in the standard poor's stock index over five year period between and
we refer to this as the smarket data
the goal is to predict whether the index will increase or decrease on given day using the past five days percentage changes in the index
here the statistical learning problem does not involve predicting numerical value
instead it involves predicting whether given day's stock market performance will fall into the up bucket or the down bucket
this is known as classification problem
model that could accurately predict the direction in which the market will move would be very useful
the left hand panel of figure displays two boxplots of the previous day's percentage changes in the stock index one for the days for which the market increased on the subsequent day and one for the days for which the market decreased
the two plots look almost identical suggesting that there is no simple strategy for using yesterday's movement in the to predict today's returns
the remaining panels which display boxplots for the percentage changes two and three days previous to today similarly indicate little association between past and present returns
of course this lack of pattern is to be expected in the presence of strong correlations between successive days returns one could adopt simple trading strategy
we fit quadratic discriminant analysis model to the subset of the smarket data corresponding to the time period and predicted the probability of stock market decrease using the data
on average the predicted probability of decrease is higher for the days in which the market does decrease
based on these results we are able to correctly predict the direction of movement in the market of the time to generate profits from the market
nevertheless in chapter we explore these data using several different statistical learning methods
interestingly there are hints of some weak trends in the data that suggest that at least for this five year period it is possible to correctly predict the direction of movement in the market approximately of the time figure
gene expression data the previous two applications illustrate data sets with both input and output variables
however another important class of problems involves situations in which we only observe input variables with no corresponding output
for example in marketing setting we might have demographic information for number of current or potential customers
we may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics
this is known as clustering problem
unlike in the previous examples here we are not trying to predict an output variable
we devote chapter to discussion of statistical learning methods for problems in which no natural output variable is available
we consider the nci data set which consists of gene expression measurements for each of cancer cell lines
instead of predicting particular output variable we are interested in determining whether there are groups or clusters among the cell lines based on their gene expression measurements
this is difficult question to address in part because there are thousands
left representation of the nci gene expression data set in two dimensional space and
each point corresponds to one of the cell lines
there appear to be four groups of cell lines which we have represented using different colors
right same as left panel except that we have represented each of the different types of cancer using different colored symbol
cell lines corresponding to the same cancer type tend to be nearby in the two dimensional space of gene expression measurements per cell line making it hard to visualize the data
the left hand panel of figure addresses this problem by representing each of the cell lines using just two numbers and
these are the first two principal components of the data which summarize the expression measurements for each cell line down to two numbers or dimensions
while it is likely that this dimension reduction has resulted in some loss of information it is now possible to visually examine the data for evidence of clustering
deciding on the number of clusters is often difficult problem
but the left hand panel of figure suggests at least four groups of cell lines which we have represented using separate colors
we can now examine the cell lines within each cluster for similarities in their types of cancer in order to better understand the relationship between gene expression levels and cancer
in this particular data set it turns out that the cell lines correspond to different types of cancer
however this information was not used to create the left hand panel of figure
the right hand panel of figure is identical to the left hand panel except that the cancer types are shown using distinct colored symbols
there is clear evidence that cell lines with the same cancer type tend to be located near each other in this two dimensional representation
in addition even though the cancer information was not used to produce the left hand panel the clustering obtained does bear some resemblance to some of the actual cancer types observed
introduction in the right hand panel
this provides some independent verification of the accuracy of our clustering analysis
brief history of statistical learning though the term statistical learning is fairly new many of the concepts that underlie the field were developed long ago
at the beginning of the nineteenth century legendre and gauss published papers on the method of least squares which implemented the earliest form of what is now known as linear regression
the approach was first successfully applied to problems in astronomy
linear regression is used for predicting quantitative values such as an individual's salary
in order to predict qualitative values such as whether patient survives or dies or whether the stock market increases or decreases fisher proposed linear discriminant analysis in
in the various authors put forth an alternative approach logistic regression
in the early nelder and wedderburn coined the term generalized linear models for an entire class of statistical learning methods that include both linear and logistic regression as special cases
by the end of the many more techniques for learning from data were available
however they were almost exclusively linear methods because fitting non linear relationships was computationally infeasible at the time
by the computing technology had finally improved sufficiently that non linear methods were no longer computationally prohibitive
in mid breiman friedman olshen and stone introduced classification and regression trees and were among the first to demonstrate the power of detailed practical implementation of method including cross validation for model selection
hastie and tibshirani coined the term generalized additive models in for class of non linear extensions to generalized linear models and also provided practical software implementation
since that time inspired by the advent of machine learning and other disciplines statistical learning has emerged as new subfield in statistics focused on supervised and unsupervised modeling and prediction
in recent years progress in statistical learning has been marked by the increasing availability of powerful and relatively user friendly software such as the popular and freely available system
this has the potential to continue the transformation of the field from set of techniques used and developed by statisticians and computer scientists to an essential toolkit for much broader community
introduction this book the elements of statistical learning esl by hastie tibshirani and friedman was first published in
since that time it has become an important reference on the fundamentals of statistical machine learning
its success derives from its comprehensive and detailed treatment of many important topics in statistical learning as well as the fact that relative to many upper level statistics textbooks it is accessible to wide audience
however the greatest factor behind the success of esl has been its topical nature
at the time of its publication interest in the field of statistical learning was starting to explode
esl provided one of the first accessible and comprehensive introductions to the topic
since esl was first published the field of statistical learning has continued to flourish
the field's expansion has taken two forms
the most obvious growth has involved the development of new and improved statistical learning approaches aimed at answering range of scientific questions across number of fields
however the field of statistical learning has also expanded its audience
in the increases in computational power generated surge of interest in the field from non statisticians who were eager to use cutting edge statistical tools to analyze their data
unfortunately the highly technical nature of these approaches meant that the user community remained primarily restricted to experts in statistics computer science and related fields with the training and time to understand and implement them
in recent years new and improved software packages have significantly eased the implementation burden for many statistical learning methods
at the same time there has been growing recognition across number of fields from business to health care to genetics to the social sciences and beyond that statistical learning is powerful tool with important practical applications
as result the field has moved from one of primarily academic interest to mainstream discipline with an enormous potential audience
this trend will surely continue with the increasing availability of enormous quantities of data and the software to analyze it
the purpose of an introduction to statistical learning isl is to facilitate the transition of statistical learning from an academic to mainstream field
isl is not intended to replace esl which is far more comprehensive text both in terms of the number of approaches considered and the depth to which they are explored
we consider esl to be an important companion for professionals with graduate degrees in statistics machine learning or related fields who need to understand the technical details behind statistical learning approaches
however the community of users of statistical learning techniques has expanded to include individuals with wider range of interests and backgrounds
therefore we believe that there is now place for less technical and more accessible version of esl
introduction in teaching these topics over the years we have discovered that they are of interest to master's and phd students in fields as disparate as business administration biology and computer science as well as to quantitativelyoriented upper division undergraduates
it is important for this diverse group to be able to understand the models intuitions and strengths and weaknesses of the various approaches
but for this audience many of the technical details behind statistical learning methods such as optimization algorithms and theoretical properties are not of primary interest
we believe that these students do not need deep understanding of these aspects in order to become informed users of the various methodologies and in order to contribute to their chosen fields through the use of statistical learning tools
we have written isl with the following four objectives in mind
many statistical learning methods are relevant and useful in wide range of academic and non academic disciplines beyond just the statistical sciences
we believe that many contemporary statistical learning procedures should and will become as widely available and used as is currently the case for classical methods such as linear regression
as result rather than attempting to consider every possible approach an impossible task we have concentrated on presenting the methods that we believe are most widely applicable
statistical learning should not be viewed as series of black boxes
no single approach will perform well in all possible applications
without understanding all of the cogs inside the box or the interaction between those cogs it is impossible to select the best box
hence we have attempted to carefully describe the model intuition assumptions and trade offs behind each of the methods that we consider
while it is important to know what job is performed by each cog it is not necessary to have the skills to construct the machine inside the box
thus we have minimized discussion of technical details related to fitting procedures and theoretical properties
we assume that the reader is comfortable with basic mathematical concepts but we do not assume graduate degree in the mathematical sciences
for instance we have almost completely avoided the use of matrix algebra and it is possible to understand the entire book without detailed knowledge of matrices and vectors
we presume that the reader is interested in applying statistical learning methods to real world problems
in order to facilitate this as well as to motivate the techniques discussed we have devoted section within each chapter to computer labs
in each lab we walk the reader through realistic application of the methods considered in that chapter
when we have taught this material in our courses
introduction we have allocated roughly one third of classroom time to working through the labs and we have found them to be extremely useful
many of the less computationally oriented students who were initially intimidated by r's command level interface got the hang of things over the course of the quarter or semester
we have used because it is freely available and is powerful enough to implement all of the methods discussed in the book
it also has optional packages that can be downloaded to implement literally thousands of additional methods
most importantly is the language of choice for academic statisticians and new approaches often become available in years before they are implemented in commercial packages
however the labs in isl are self contained and can be skipped if the reader wishes to use different software package or does not wish to apply the methods discussed to real world problems
who should read this book
this book is intended for anyone who is interested in using modern statistical methods for modeling and prediction from data
this group includes scientists engineers data analysts or quants but also less technical individuals with degrees in non quantitative fields such as the social sciences or business
we expect that the reader will have had at least one elementary course in statistics
background in linear regression is also useful though not required since we review the key concepts behind linear regression in chapter
the mathematical level of this book is modest and detailed knowledge of matrix operations is not required
this book provides an introduction to the statistical programming language
previous exposure to programming language such as matlab or python is useful but not required
we have successfully taught material at this level to master's and phd students in business computer science biology earth sciences psychology and many other areas of the physical and social sciences
this book could also be appropriate for advanced undergraduates who have already taken course on linear regression
in the context of more mathematically rigorous course in which esl serves as the primary textbook isl could be used as supplementary text for teaching computational aspects of the various approaches
notation and simple matrix algebra choosing notation for textbook is always difficult task
for the most part we adopt the same notational conventions as esl
introduction we will use to represent the number of distinct data points or observations in our sample
we will let denote the number of variables that are available for use in making predictions
for example the wage data set consists of twelve variables for people so we have observations and variables such as year age wage and more
note that throughout this book we indicate variable names using colored font variable name
in some examples might be quite large such as on the order of thousands or even millions this situation arises quite often for example in the analysis of modern biological data or web based advertising data
in general we will let xij represent the value of the jth variable for the ith observation where and
throughout this book will be used to index the samples or observations from to and will be used to index the variables from to
we let denote matrix whose th element is xij
that is eb ec ec ec

ed
xn xn xnp for readers who are unfamiliar with matrices it is useful to visualize as spreadsheet of numbers with rows and columns
at times we will be interested in the rows of which we write as xn
here xi is vector of length containing the variable measurements for the ith observation
that is eb xi ec xi ec xi ec

ed
xip vectors are by default represented as columns
for example for the wage data xi is vector of length twelve consisting of year age wage and other values for the ith individual
at other times we will instead be interested in the columns of which we write as xp
each is vector of length
that is eb ec ec xj ec

ed
xnj for example for the wage data contains the values for year
using this notation the matrix can be written as xp
introduction or eb ec xt ec ec

ed
xtn the notation denotes the transpose of matrix or vector
so for example eb xn ec xn ec xt ec
ed
xnp while xti xi xi xip
we use yi to denote the ith observation of the variable on which we wish to make predictions such as wage
hence we write the set of all observations in vector form as eb ec ec ec

ed
yn then our observed data consists of xn yn where each xi is vector of length
if then xi is simply scalar
in this text vector of length will always be denoted in lower case bold
eb ec ec ec

ed
an however vectors that are not of length such as feature vectors of length as in will be denoted in lower case normal font
scalars will also be denoted in lower case normal font
in the rare cases in which these two uses for lower case normal font lead to ambiguity we will clarify which use is intended
matrices will be denoted using bold capitals such as
random variables will be denoted using capital normal font
regardless of their dimensions
occasionally we will want to indicate the dimension of particular object
to indicate that an object is scalar we will use the notation
to indicate that it is vector of length we will use rk or rn if it is of length
we will indicate that an object is matrix using rr
introduction we have avoided using matrix algebra whenever possible
however in few instances it becomes too cumbersome to avoid it entirely
in these rare instances it is important to understand the concept of multiplying two matrices
suppose that rr and rd
then the product of and is denoted ab
the th element of ab is computed by by the corresponding element multiplying each element of the ith row of of the jth column of
that is ab ij aik bkj
as an example consider and
then ab
note that this operation produces an matrix
it is only possible to compute ab if the number of columns of is the same as the number of rows of
organization of this book chapter introduces the basic terminology and concepts behind statistical learning
this chapter also presents the nearest neighbor classifier very simple method that works surprisingly well on many problems
chapters and cover classical linear methods for regression and classification
in particular chapter reviews linear regression the fundamental starting point for all regression methods
in chapter we discuss two of the most important classical classification methods logistic regression and linear discriminant analysis
central problem in all statistical learning situations involves choosing the best method for given application
hence in chapter we introduce cross validation and the bootstrap which can be used to estimate the accuracy of number of different methods in order to choose the best one
much of the recent research in statistical learning has concentrated on non linear methods
however linear methods often have advantages over their non linear competitors in terms of interpretability and sometimes also accuracy
hence in chapter we consider host of linear methods both classical and more modern which offer potential improvements over standard linear regression
these include stepwise selection ridge regression principal components regression partial least squares and the lasso
the remaining chapters move into the world of non linear statistical learning
we first introduce in chapter number of non linear methods that work well for problems with single input variable
we then show how these methods can be used to fit non linear additive models for
introduction which there is more than one input
in chapter we investigate tree based methods including bagging boosting and random forests
support vector machines set of approaches for performing both linear and non linear classification are discussed in chapter
finally in chapter we consider setting in which we have input variables but no output variable
in particular we present principal components analysis means clustering and hierarchical clustering
at the end of each chapter we present one or more lab sections in which we systematically work through applications of the various methods discussed in that chapter
these labs demonstrate the strengths and weaknesses of the various approaches and also provide useful reference for the syntax required to implement the various methods
the reader may choose to work through the labs at his or her own pace or the labs may be the focus of group sessions as part of classroom environment
within each lab we present the results that we obtained when we performed the lab at the time of writing this book
however new versions of are continuously released and over time the packages called in the labs will be updated
therefore in the future it is possible that the results shown in the lab sections may no longer correspond precisely to the results obtained by the reader who performs the labs
as necessary we will post updates to the labs on the book website
we use the symbol to denote sections or exercises that contain more challenging concepts
these can be easily skipped by readers who do not wish to delve as deeply into the material or who lack the mathematical background
data sets used in labs and exercises in this textbook we illustrate statistical learning methods using applications from marketing finance biology and other areas
the islr package available on the book website contains number of data sets that are required in order to perform the labs and exercises associated with this book
one other data set is contained in the mass library and yet another is part of the base distribution
table contains summary of the data sets required to perform the labs and exercises
couple of these data sets are also available as text files on the book website for use in chapter
book website the website for this book is located at
introduction name description auto gas mileage horsepower and other information for cars
boston housing values and other information about boston suburbs
caravan information about individuals offered caravan insurance
carseats information about car seat sales in stores
college demographic characteristics tuition and more for usa colleges
default customer default records for credit card company
hitters records and salaries for baseball players
khan gene expression measurements for four cancer types
nci gene expression measurements for cancer cell lines
oj sales information for citrus hill and minute maid orange juice
portfolio past values of financial assets for use in portfolio allocation
smarket daily percentage returns for over five year period
usarrests crime statistics per residents in states of usa
wage income survey data for males in central atlantic region of usa
weekly weekly stock market returns for years
list of data sets needed to perform the labs and exercises in this textbook
all data sets are available in the islr library with the exception of boston part of mass and usarrests part of the base distribution www statlearning com it contains number of resources including the package associated with this book and some additional data sets
acknowledgements few of the plots in this book were taken from esl figures and
all other plots are new to this book
this is page printer opaque this statistical learning what is statistical learning
in order to motivate our study of statistical learning we begin with simple example
suppose that we are statistical consultants hired by client to provide advice on how to improve sales of particular product
the advertising data set consists of the sales of that product in different markets along with advertising budgets for the product in each of those markets for three different media tv radio and newspaper
the data are displayed in figure
it is not possible for our client to directly increase sales of the product
on the other hand they can control the advertising expenditure in each of the three media
therefore if we determine that there is an association between advertising and sales then we can instruct our client to adjust advertising budgets thereby indirectly increasing sales
in other words our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets
in this setting the advertising budgets are input variables while sales input variable is an output variable
the input variables are typically denoted using the output variable symbol with subscript to distinguish them
so might be the tv budget the radio budget and the newspaper budget
the inputs go by different names such as predictors independent variables features or predictor sometimes just variables
the output variable in this case sales is independent often called the response or dependent variable and is typically denoted variable using the symbol
throughout this book we will use all of these terms feature interchangeably variable response dependent variable
the advertising data set
the plot displays sales in thousands of units as function of tv radio and newspaper budgets in thousands of dollars for different markets
in each plot we show the simple least squares fit of sales to that variable as described in chapter
in other words each blue line represents simple model that can be used to predict sales using tv radio and newspaper respectively
more generally suppose that we observe quantitative response and different predictors xp
we assume that there is some relationship between and xp which can be written in the very general form
here is some fixed but unknown function of xp and is random error term which is independent of and has mean zero
in this formula error term tion represents the systematic information that provides about systematic as another example consider the left panel of figure plot of income versus years of education for individuals in the income data set
the plot suggests that one might be able to predict income using years of education
however the function that connects the input variable to the output variable is in general unknown
in this situation one must estimate based on the observed points
since income is simulated data set is known and is shown by the blue curve in the right panel of figure
the vertical lines represent the error terms
we note that some of the observations lie above the blue curve and some lie below it overall the errors have approximately mean zero
in general the function may involve more than one input variable
in figure we plot income as function of years of education and seniority
here is two dimensional surface that must be estimated based on the observed data
what is statistical learning
the income data set
left the red dots are the observed values of income in tens of thousands of dollars and years of education for individuals
right the blue curve represents the true underlying relationship between income and years of education which is generally unknown but is known in this case because the data were simulated
the black lines represent the error associated with each observation
note that some errors are positive if an observation lies above the blue curve and some are negative if an observation lies below the curve
overall these errors have approximately mean zero
in essence statistical learning refers to set of approaches for estimating
in this chapter we outline some of the key theoretical concepts that arise in estimating as well as tools for evaluating the estimates obtained
why estimate
there are two main reasons that we may wish to estimate for prediction and inference
we discuss each in turn
prediction in many situations set of inputs are readily available but the output cannot be easily obtained
in this setting since the error term averages to zero we can predict using where represents our estimate for and represents the resulting prediction for
in this setting is often treated as black box in the sense that one is not typically concerned with the exact form of provided that it yields accurate predictions for
as an example suppose that xp are characteristics of patient's blood sample that can be easily measured in lab and is variable
the plot displays income as function of years of education and seniority in the income data set
the blue surface represents the true underlying relationship between income and years of education and seniority which is known since the data are simulated
the red dots indicate the observed values of these quantities for individuals encoding the patient's risk for severe adverse reaction to particular drug
it is natural to seek to predict using since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction that is patients for whom the estimate of is high
the accuracy of as prediction for depends on two quantities which we will call the reducible error and the irreducible error
in general reducible error will not be perfect estimate for and this inaccuracy will introduce irreducible error some error
this error is reducible because we can potentially improve the accuracy of by using the most appropriate statistical learning technique to estimate
however even if it were possible to form perfect estimate for so that our estimated response took the form our prediction would still have some error in it
this is because is also function of which by definition cannot be predicted using
therefore variability associated with also affects the accuracy of our predictions
this is known as the irreducible error because no matter how well we estimate we cannot reduce the error introduced by
why is the irreducible error larger than zero
the quantity may contain unmeasured variables that are useful in predicting since we don't measure them cannot use them for its prediction
the quantity may also contain unmeasurable variation
for example the risk of an adverse reaction might vary for given patient on given day depending on manufacturing variation in the drug itself or the patient's general feeling of well being on that day
consider given estimate and set of predictors which yields the prediction
assume for moment that both and are fixed
what is statistical learning
then it is easy to show that var reducible irreducible where represents the average or expected value of the squared expected value difference between the predicted and actual value of and var represents the variance associated with the error term variance the focus of this book is on techniques for estimating with the aim of minimizing the reducible error
it is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for
this bound is almost always unknown in practice
inference we are often interested in understanding the way that is affected as xp change
in this situation we wish to estimate but our goal is not necessarily to make predictions for
we instead want to understand the relationship between and or more specifically to understand how changes as function of xp
now cannot be treated as black box because we need to know its exact form
it is often the case that only small fraction of the available predictors are substantially associated with
identifying the few important predictors among large set of possible variables can be extremely useful depending on the application
some predictors may have positive relationship with in the sense that increasing the predictor is associated with increasing values of
other predictors may have the opposite relationship
depending on the complexity of the relationship between the response and given predictor may also depend on the values of the other predictors
historically most methods for estimating have taken linear form
in some situations such an assumption is reasonable or even desirable
but often the true relationship is more complicated in which case linear model may not provide an accurate representation of the relationship between the input and output variables
in this book we will see number of examples that fall into the prediction setting the inference setting or combination of the two
statistical learning for instance consider company that is interested in conducting direct marketing campaign
the goal is to identify individuals who will respond positively to mailing based on observations of demographic variables measured on each individual
in this case the demographic variables serve as predictors and response to the marketing campaign either positive or negative serves as the outcome
the company is not interested in obtaining deep understanding of the relationships between each individual predictor and the response instead the company simply wants an accurate model to predict the response using the predictors
this is an example of modeling for prediction
in contrast consider the advertising data illustrated in figure
one may be interested in answering questions such as which media contribute to sales which media generate the biggest boost in sales
or how much increase in sales is associated with given increase in tv advertising
this situation falls into the inference paradigm
another example involves modeling the brand of product that customer might purchase based on variables such as price store location discount levels competition price and so forth
in this situation one might really be most interested in how each of the individual variables affects the probability of purchase
for instance what effect will changing the price of product have on sales
this is an example of modeling for inference
finally some modeling could be conducted both for prediction and inference
for example in real estate setting one may seek to relate values of homes to inputs such as crime rate zoning distance from river air quality schools income level of community size of houses and so forth
in this case one might be interested in how the individual input variables affect the prices that is how much extra will house be worth if it has view of the river
this is an inference problem
alternatively one may simply be interested in predicting the value of home given its characteristics is this house underor over valued
this is prediction problem
depending on whether our ultimate goal is prediction inference or combination of the two different methods for estimating may be appropriate
for example linear models allow for relatively simple and interlinear model pretable inference but may not yield as accurate predictions as some other approaches
in contrast some of the highly non linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for but this comes at the expense of less interpretable model for which inference is more challenging
what is statistical learning
how do we estimate
throughout this book we explore many linear and non linear approaches for estimating
however these methods generally share certain characteristics
we provide an overview of these shared characteristics in this section
we will always assume that we have observed set of different data points
for example in figure we observed data points
these observations are called the training data because we will use these training data observations to train or teach our method how to estimate
let xij represent the value of the jth predictor or input for observation where and
correspondingly let yi represent the response variable for the ith observation
then our training data consist of xn yn where xi xi xi xip
our goal is to apply statistical learning method to the training data in order to estimate the unknown function
in other words we want to find function such that for any observation
broadly speaking most statistical learning methods for this task can be characterized as either parametric or non parametric
we now briefly discuss these parametric two types of approaches non parametric parametric methods parametric methods involve two step model based approach
first we make an assumption about the functional form or shape of
for example one very simple assumption is that is linear in
xp
this is linear model which will be discussed extensively in chapter
once we have assumed that is linear the problem of estimating is greatly simplified
instead of having to estimate an entirely arbitrary dimensional function one only needs to estimate the coefficients
after model has been selected we need procedure that uses the training data to fit or train the model
in the case of the linear model fit we need to estimate the parameters
that is we train want to find values of these parameters such that

the most common approach to fitting the model is referred to as ordinary least squares which we discuss in chapter
howleast squares ever least squares is one of many possible ways way to fit the linear model
in chapter we discuss other approaches for estimating the parameters in
linear model fit by least squares to the income data from figure
the observations are shown in red and the yellow plane indicates the least squares fit to the data
the model based approach just described is referred to as parametric it reduces the problem of estimating down to one of estimating set of parameters
assuming parametric form for simplifies the problem of estimating because it is generally much easier to estimate set of parameters such as in the linear model than it is to fit an entirely arbitrary function
the potential disadvantage of parametric approach is that the model we choose will usually not match the true unknown form of
if the chosen model is too far from the true then our estimate will be poor
we can try to address this problem by choosing flexible models that can fit many different possible functional forms flexible for
but in general fitting more flexible model requires estimating greater number of parameters
these more complex models can lead to phenomenon known as overfitting the data which essentially means they overfitting follow the errors or noise too closely
these issues are discussed throughnoise out this book
figure shows an example of the parametric approach applied to the income data from figure
we have fit linear model of the form income education seniority
since we have assumed linear relationship between the response and the two predictors the entire fitting problem reduces to estimating and which we do using least squares linear regression
comparing figure to figure we can see that the linear fit given in figure is not quite right the true has some curvature that is not captured in the linear fit
however the linear fit still appears to do reasonable job of capturing the positive relationship between years of education and income as well as the slightly less positive relationship between seniority and income
what is statistical learning
smooth thin plate spline fit to the income data from figure is shown in yellow the observations are displayed in red
splines are discussed in chapter may be that with such small number of observations this is the best we can do
non parametric methods non parametric methods do not make explicit assumptions about the functional form of
instead they seek an estimate of that gets as close to the data points as possible without being too rough or wiggly
such approaches can have major advantage over parametric approaches by avoiding the assumption of particular functional form for they have the potential to accurately fit wider range of possible shapes for
any parametric approach brings with it the possibility that the functional form used to estimate is very different from the true in which case the resulting model will not fit the data well
in contrast non parametric approaches completely avoid this danger since essentially no assumption about the form of is made
but non parametric approaches do suffer from major disadvantage since they do not reduce the problem of estimating to small number of parameters very large number of observations far more than is typically needed for parametric approach is required in order to obtain an accurate estimate for
an example of non parametric approach to fitting the income data is shown in figure
thin plate spline is used to estimate
this apthin plate spline proach does not impose any pre specified model on
it instead attempts to produce an estimate for that is as close as possible to the observed data subject to the fit that is the yellow surface in figure being smooth
in this case the non parametric fit has produced remarkably accurate estimate of the true shown in figure
in order to fit thin plate spline the data analyst must select level of smoothness
figure shows
rough thin plate spline fit to the income data from figure
this fit makes zero errors on the training data the same thin plate spline fit using lower level of smoothness allowing for rougher fit
the resulting estimate fits the observed data perfectly
however the spline fit shown in figure is far more variable than the true function from figure
this is an example of overfitting the data which we discussed previously
it is an undesirable situation because the fit obtained will not yield accurate estimates of the response on new observations that were not part of the original training data set
we discuss methods for choosing the correct amount of smoothness in chapter
splines are discussed in chapter
as we have seen there are advantages and disadvantages to parametric and non parametric methods for statistical learning
we explore both types of methods throughout this book
the trade off between prediction accuracy and model interpretability of the many methods that we examine in this book some are less flexible or more restrictive in the sense that they can produce just relatively small range of shapes to estimate
for example linear regression is relatively inflexible approach because it can only generate linear functions such as the lines shown in figure or the plane shown in figure
other methods such as the thin plate splines shown in figures and are considerably more flexible because they can generate much wider range of possible shapes to estimate
one might reasonably ask the following question why would we ever choose to use more restrictive method instead of very flexible approach
there are several reasons that we might prefer more restrictive model
if we are mainly interested in inference then restrictive models are much
what is statistical learning
representation of the tradeoff between flexibility and interpretability using different statistical learning methods
in general as the flexibility of method increases its interpretability decreases more interpretable
for instance when inference is the goal the linear model may be good choice since it will be quite easy to understand the relationship between and xp
in contrast very flexible approaches such as the splines discussed in chapter and displayed in figures and and the boosting methods discussed in chapter can lead to such complicated estimates of that it is difficult to understand how any individual predictor is associated with the response
figure provides an illustration of the trade off between flexibility and interpretability for some of the methods that we cover in this book
least squares linear regression discussed in chapter is relatively inflexible but is quite interpretable
the lasso discussed in chapter relies upon the lasso linear model but uses an alternative fitting procedure for estimating the coefficients
the new procedure is more restrictive in estimating the coefficients and sets number of them to exactly zero
hence in this sense the lasso is less flexible approach than linear regression
it is also more interpretable than linear regression because in the final model the response variable will only be related to small subset of the predictors namely those with nonzero coefficient estimates
generalized additive models gams discussed in chapter instead extend the lingeneralized ear model to allow for certain non linear relationships
consequently additive model gams are more flexible than linear regression
they are also somewhat less interpretable than linear regression because the relationship between each predictor and the response is now modeled using curve
finally fully
statistical learning non linear methods such as bagging boosting and support vector machines bagging with non linear kernels discussed in chapters and are highly flexible boosting approaches that are harder to interpret support vector we have established that when inference is the goal there are clear ad machine vantages to using simple and relatively inflexible statistical learning methods
in some settings however we are only interested in prediction and the interpretability of the predictive model is simply not of interest
for instance if we seek to develop an algorithm to predict the price of stock our sole requirement for the algorithm is that it predict accurately interpretability is not concern
in this setting we might expect that it will be best to use the most flexible model available
surprisingly this is not always the case
we will often obtain more accurate predictions using less flexible method
this phenomenon which may seem counterintuitive at first glance has to do with the potential for overfitting in highly flexible methods
we saw an example of overfitting in figure
we will discuss this very important concept further in section and throughout this book
supervised versus unsupervised learning most statistical learning problems fall into one of two categories supervised supervised or unsupervised
the examples that we have discussed so far in this chapunsupervised ter all fall into the supervised learning domain
for each observation of the predictor measurement xi there is an associated response measurement yi
we wish to fit model that relates the response to the predictors with the aim of accurately predicting the response for future observations prediction or better understanding the relationship between the response and the predictors inference
many classical statistical learning methods such as linear regression and logistic regression chapter as logistic regression well as more modern approaches such as gam boosting and support vector machines operate in the supervised learning domain
the vast majority of this book is devoted to this setting
in contrast unsupervised learning describes the somewhat more challenging situation in which for every observation we observe vector of measurements xi but no associated response yi
it is not possible to fit linear regression model since there is no response variable to predict
in this setting we are in some sense working blind the situation is referred to as unsupervised because we lack response variable that can supervise our analysis
what sort of statistical analysis is possible
we can seek to understand the relationships between the variables or between the observations
one statistical learning tool that we may use in this setting is cluster analysis or clustering
the goal of cluster analysis is cluster analysis to ascertain on the basis of xn whether the observations fall into relatively distinct groups
for example in market segmentation study we might observe multiple characteristics variables for potential customers
what is statistical learning
clustering data set involving three groups
each group is shown using different colored symbol
left the three groups are well separated
in this setting clustering approach should successfully identify the three groups
right there is some overlap among the groups
now the clustering task is more challenging such as zip code family income and shopping habits
we might believe that the customers fall into different groups such as big spenders versus low spenders
if the information about each customer's spending patterns were available then supervised analysis would be possible
however this information is not available that is we do not know whether each potential customer is big spender or not
in this setting we can try to cluster the customers on the basis of the variables measured in order to identify distinct groups of potential customers
identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest such as spending habits
figure provides simple illustration of the clustering problem
we have plotted observations with measurements on two variables and
each observation corresponds to one of three distinct groups
for illustrative purposes we have plotted the members of each group using different colors and symbols
however in practice the group memberships are unknown and the goal is to determine the group to which each observation belongs
in the left hand panel of figure this is relatively easy task because the groups are well separated
in contrast the right hand panel illustrates more challenging problem in which there is some overlap between the groups
clustering method could not be expected to assign all of the overlapping points to their correct group blue green or orange
in the examples shown in figure there are only two variables and so one can simply visually inspect the scatterplots of the observations in
statistical learning order to identify clusters
however in practice we often encounter data sets that contain many more than two variables
in this case we cannot easily plot the observations
for instance if there are variables in our data set then distinct scatterplots can be made and visual inspection is simply not viable way to identify clusters
for this reason automated clustering methods are important
we discuss clustering and other unsupervised learning approaches in chapter
many problems fall naturally into the supervised or unsupervised learning paradigms
however sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear cut
for instance suppose that we have set of observations
for of the observations where we have both predictor measurements and response measurement
for the remaining observations we have predictor measurements but no response measurement
such scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect
we refer to this setting as semi supervised learning problem
in this setting we wish to use stasemi supervised tistical learning method that can incorporate the observations for which learning response measurements are available as well as the observations for which they are not
although this is an interesting topic it is beyond the scope of this book
regression versus classification problems variables can be characterized as either quantitative or qualitative also quantitative known as categorical
quantitative variables take on numerical values
exqualitative amples include person's age height or income the value of house and categorical the price of stock
in contrast qualitative variables take on values in one of different classes or categories
examples of qualitative variables include class person's gender male or female the brand of product purchased brand or whether person defaults on debt yes or no or cancer diagnosis acute myelogenous leukemia acute lymphoblastic leukemia or no leukemia
we tend to refer to problems with quantitative response as regression problems while those involving qualitative response are ofregression ten referred to as classification problems
however the distinction is not classification always that crisp
least squares linear regression chapter is used with quantitative response whereas logistic regression chapter is typically used with qualitative two class or binary response
as such it is often binary used as classification method
but since it estimates class probabilities it can be thought of as regression method as well
some statistical methods such as nearest neighbors chapters and and boosting chapter can be used in the case of either quantitative or qualitative responses
we tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative we might use linear regression when quantitative and logistic regression when qualitative
assessing model accuracy whether the predictors are qualitative or quantitative is generally considered less important
most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type provided that any qualitative predictors are properly coded before the analysis is performed
this is discussed in chapter
assessing model accuracy one of the key aims of this book is to introduce the reader to wide range of statistical learning methods that extend far beyond the standard linear regression approach
why is it necessary to introduce so many different statistical learning approaches rather than just single best method
there is no free lunch in statistics no one method dominates all others over all possible data sets
on particular data set one specific method may work best but some other method may work better on similar but different data set
hence it is an important task to decide for any given set of data which method produces the best results
selecting the best approach can be one of the most challenging parts of performing statistical learning in practice
in this section we discuss some of the most important concepts that arise in selecting statistical learning procedure for specific data set
as the book progresses we will explain how the concepts presented here can be applied in practice
measuring the quality of fit in order to evaluate the performance of statistical learning method on given data set we need some way to measure how well its predictions actually match the observed data
that is we need to quantify the extent to which the predicted response value for given observation is close to the true response value for that observation
in the regression setting the most commonly used measure is the mean squared error mse given by mean squared error se yi xi where xi is the prediction that gives for the ith observation
the mse will be small if the predicted responses are very close to the true responses and will be large if for some of the observations the predicted and true responses differ substantially
the mse in is computed using the training data that was used to fit the model and so should more accurately be referred to as the training mse
but in general we do not really care how well the method works on training mse
statistical learning the training data
rather we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data
why is this what we care about
suppose that we are interested in test data developing an algorithm to predict stock's price based on previous stock returns
we can train the method using stock returns from the past six months
but we don't really care how well our method predicts last week's stock price
we instead care about how well it will predict tomorrow's price or next month's price
on similar note suppose that we have clinical measurements weight blood pressure height age family history of disease for number of patients as well as information about whether each patient has diabetes
we can use these patients to train statistical learning method to predict risk of diabetes based on clinical measurements
in practice we want this method to accurately predict diabetes risk for future patients based on their clinical measurements
we are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model since we already know which of those patients have diabetes
to state it more mathematically suppose that we fit our statistical learning method on our training observations xn yn and we obtain the estimate
we can then compute xn
if these are approximately equal to yn then the training mse given by is small
however we are really not interested in whether xi yi instead we want to know whether is approximately equal to where is previously unseen test observation not used to train the statistical learning method
we want to choose the method that gives the lowest test mse as opposed to the lowest training mse
in other words test mse if we had large number of test observations we could compute ave the average squared prediction error for these test observations
we'd like to select the model for which the average of this quantity the test mse is as small as possible
how can we go about trying to select method that minimizes the test mse
in some settings we may have test data set available that is we may have access to set of observations that were not used to train the statistical learning method
we can then simply evaluate on the test observations and select the learning method for which the test mse is smallest
but what if no test observations are available
in that case one might imagine simply selecting statistical learning method that minimizes the training mse
this seems like it might be sensible approach since the training mse and the test mse appear to be closely related
unfortunately there is fundamental problem with this strategy there is no guarantee that the method with the lowest training mse will also have the lowest test mse
roughly speaking the problem is that many
left data simulated from shown in black
three estimates of are shown the linear regression line orange curve and two smoothing spline fits blue and green curves
right training mse grey curve test mse red curve and minimum possible test mse over all methods dashed line
squares represent the training and test mses for the three fits shown in the left hand panel statistical methods specifically estimate coefficients so as to minimize the training set mse
for these methods the training set mse can be quite small but the test mse is often much larger
figure illustrates this phenomenon on simple example
in the lefthand panel of figure we have generated observations from with the true given by the black curve
the orange blue and green curves illustrate three possible estimates for obtained using methods with increasing levels of flexibility
the orange line is the linear regression fit which is relatively inflexible
the blue and green curves were produced using smoothing splines discussed in chapter with different levels of smoothness
it is smoothing spline clear that as the level of flexibility increases the curves fit the observed data more closely
the green curve is the most flexible and matches the data very well however we observe that it fits the true shown in black poorly because it is too wiggly
by adjusting the level of flexibility of the smoothing spline fit we can produce many different fits to this data
we now move on to the right hand panel of figure
the grey curve displays the average training mse as function of flexibility or more formally the degrees of freedom for number of smoothing splines
the dedegrees of freedom grees of freedom is quantity that summarizes the flexibility of curve it is discussed more fully in chapter
the orange blue and green squares
statistical learning indicate the mses associated with the corresponding curves in the lefthand panel
more restricted and hence smoother curve has fewer degrees of freedom than wiggly curve note that in figure linear regression is at the most restrictive end with two degrees of freedom
the training mse declines monotonically as flexibility increases
in this example the true is non linear and so the orange linear fit is not flexible enough to estimate well
the green curve has the lowest training mse of all three methods since it corresponds to the most flexible of the three curves fit in the left hand panel
in this example we know the true function and so we can also compute the test mse over very large test set as function of flexibility
of course in general is unknown so this will not be possible
the test mse is displayed using the red curve in the right hand panel of figure
as with the training mse the test mse initially declines as the level of flexibility increases
however at some point the test mse levels off and then starts to increase again
consequently the orange and green curves both have high test mse
the blue curve minimizes the test mse which should not be surprising given that visually it appears to estimate the best in the left hand panel of figure
the horizontal dashed line indicates var the irreducible error in which corresponds to the lowest achievable test mse among all possible methods
hence the smoothing spline represented by the blue curve is close to optimal
in the right hand panel of figure as the flexibility of the statistical learning method increases we observe monotone decrease in the training mse and shape in the test mse
this is fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used
as model flexibility increases training mse will decrease but the test mse may not
when given method yields small training mse but large test mse we are said to be overfitting the data
this happens because our statistical learning procedure is working too hard to find patterns in the training data and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function
when we overfit the training data the test mse will be very large because the supposed patterns that the method found in the training data simply don't exist in the test data
note that regardless of whether or not overfitting has occurred we almost always expect the training mse to be smaller than the test mse because most statistical learning methods either directly or indirectly seek to minimize the training mse
overfitting refers specifically to the case in which less flexible model would have yielded smaller test mse
figure provides another example in which the true is approximately linear
again we observe that the training mse decreases monotonically as the model flexibility increases and that there is shape in the test mse
however because the truth is close to linear the test mse
details are as in figure using different true that is much closer to linear
in this setting linear regression provides very good fit to the data
details are as in figure using different that is far from linear
in this setting linear regression provides very poor fit to the data
statistical learning only decreases slightly before increasing again so that the orange least squares fit is substantially better than the highly flexible green curve
finally figure displays an example in which is highly non linear
the training and test mse curves still exhibit the same general patterns but now there is rapid decrease in both curves before the test mse starts to increase slowly
in practice one can usually compute the training mse with relative ease but estimating test mse is considerably more difficult because usually no test data are available
as the previous three examples illustrate the flexibility level corresponding to the model with the minimal test mse can vary considerably among data sets
throughout this book we discuss variety of approaches that can be used in practice to estimate this minimum point
one important method is cross validation chapter which is cross validation method for estimating test mse using the training data
the bias variance trade off the shape observed in the test mse curves figures turns out to be the result of two competing properties of statistical learning methods
though the mathematical proof is beyond the scope of this book it is possible to show that the expected test mse for given value can always be decomposed into the sum of three fundamental quantities the variance of the squared bias of and the variance of the error variance terms
that is bias var bias var
here the notation defines the expected test mse and refers expected test mse to the average test mse that we would obtain if we repeatedly estimated using large number of training sets and tested each at
the overall expected test mse can be computed by averaging over all possible values of in the test set
equation tells us that in order to minimize the expected test error we need to select statistical learning method that simultaneously achieves low variance and low bias
note that variance is inherently nonnegative quantity and squared bias is also nonnegative
hence we see that the expected test mse can never lie below var the irreducible error from
what do we mean by the variance and bias of statistical learning method
variance refers to the amount by which would change if we estimated it using different training data set
since the training data are used to fit the statistical learning method different training data sets will result in different
but ideally the estimate for should not vary
assessing model accuracy too much between training sets
however if method has high variance then small changes in the training data can result in large changes in
in general more flexible statistical methods have higher variance
consider the green and orange curves in figure
the flexible green curve is following the observations very closely
it has high variance because changing any one of these data points may cause the estimate to change considerably
in contrast the orange least squares line is relatively inflexible and has low variance because moving any single observation will likely cause only small shift in the position of the line
on the other hand bias refers to the error that is introduced by approximating real life problem which may be extremely complicated by much simpler model
for example linear regression assumes that there is linear relationship between and xp
it is unlikely that any real life problem truly has such simple linear relationship and so performing linear regression will undoubtedly result in some bias in the estimate of
in figure the true is substantially non linear so no matter how many training observations we are given it will not be possible to produce an accurate estimate using linear regression
in other words linear regression results in high bias in this example
however in figure the true is very close to linear and so given enough data it should be possible for linear regression to produce an accurate estimate
generally more flexible methods result in less bias
as general rule as we use more flexible methods the variance will increase and the bias will decrease
the relative rate of change of these two quantities determines whether the test mse increases or decreases
as we increase the flexibility of class of methods the bias tends to initially decrease faster than the variance increases
consequently the expected test mse declines
however at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance
when this happens the test mse increases
note that we observed this pattern of decreasing test mse followed by increasing test mse in the right hand panels of figures
the three plots in figure illustrate equation for the examples in figures
in each case the blue solid curve represents the squared bias for different levels of flexibility while the orange curve corresponds to the variance
the horizontal dashed line represents var the irreducible error
finally the red curve corresponding to the test set mse is the sum of these three quantities
in all three cases the variance increases and the bias decreases as the method's flexibility increases
however the flexibility level corresponding to the optimal test mse differs considerably among the three data sets because the squared bias and variance change at different rates in each of the data sets
in the left hand panel of figure the bias initially decreases rapidly resulting in an initial sharp decrease in the expected test mse
on the other hand in the center panel of figure the true is close to linear so there is only small decrease in bias as flex
squared bias blue curve variance orange curve var dashed line and test mse red curve for the three data sets in figures
the vertical dashed line indicates the flexibility level corresponding to the smallest test mse ibility increases and the test mse only declines slightly before increasing rapidly as the variance increases
finally in the right hand panel of figure as flexibility increases there is dramatic decline in bias because the true is very non linear
there is also very little increase in variance as flexibility increases
consequently the test mse declines substantially before experiencing small increase as model flexibility increases
the relationship between bias variance and test set mse given in equation and displayed in figure is referred to as the bias variance trade off
good test set performance of statistical learning method rebias variance quires low variance as well as low squared bias
this is referred to as trade off trade off because it is easy to obtain method with extremely low bias but high variance for instance by drawing curve that passes through every single training observation or method with very low variance but high bias by fitting horizontal line to the data
the challenge lies in finding method for which both the variance and the squared bias are low
this trade off is one of the most important recurring themes in this book
in real life situation in which is unobserved it is generally not possible to explicitly compute the test mse bias or variance for statistical learning method
nevertheless one should always keep the bias variance trade off in mind
in this book we explore methods that are extremely flexible and hence can essentially eliminate bias
however this does not guarantee that they will outperform much simpler method such as linear regression
to take an extreme example suppose that the true is linear
in this situation linear regression will have no bias making it very hard for more flexible method to compete
in contrast if the true is highly non linear and we have an ample number of training observations then we may do better using highly flexible approach as in figure
assessing model accuracy chapter we discuss cross validation which is way to estimate the test mse using the training data
the classification setting thus far our discussion of model accuracy has been focused on the regression setting
but many of the concepts that we have encountered such as the bias variance trade off transfer over to the classification setting with only some modifications due to the fact that yi is no longer numerical
suppose that we seek to estimate on the basis of training observations xn yn where now yn are qualitative
the most common approach for quantifying the accuracy of our estimate is the training error rate the proportion of mistakes that are made if we apply error rate our estimate to the training observations yi
here is the predicted class label for the ith observation using
and yi is an indicator variable that equals if yi and zero if yi indicator variable if yi then the ith observation was classified correctly by our classification method otherwise it was misclassified
hence equation computes the fraction of incorrect classifications
equation is referred to as the training error rate because it is comtraining error puted based on the data that was used to train our classifier
as in the regression setting we are most interested in the error rates that result from applying our classifier to test observations that were not used in training
the test error rate associated with set of test observations of the form test error is given by ave where is the predicted class label that results from applying the classifier to the test observation with predictor
good classifier is one for which the test error is smallest
it is possible to show though the proof is outside of the scope of this book that the test error rate given in is minimized on average by very simple classifier that assigns each observation to the most likely class given its predictor values
in other words we should simply assign test observation with predictor vector to the class for which pr is largest
note that is conditional probability it is the probability conditional that given the observed predictor vector
this very simple clas probability sifier is called the bayes classifier
in two class problem where there are bayes classifier
simulated data set consisting of observations in each of two groups indicated in blue and in orange
the purple dashed line represents the bayes decision boundary
the orange background grid indicates the region in which test observation will be assigned to the orange class and the blue background grid indicates the region in which test observation will be assigned to the blue class only two possible response values say class or class the bayes classifier corresponds to predicting class one if pr and class two otherwise
figure provides an example using simulated data set in twodimensional space consisting of predictors and
the orange and blue circles correspond to training observations that belong to two different classes
for each value of and there is different probability of the response being orange or blue
since this is simulated data we know how the data were generated and we can calculate the conditional probabilities for each value of and
the orange shaded region reflects the set of points for which pr orange is greater than while the blue shaded region indicates the set of points for which the probability is below
the purple dashed line represents the points where the probability is exactly
this is called the bayes decision boundary
the bayes bayes decision classifier's prediction is determined by the bayes decision boundary an boundary observation that falls on the orange side of the boundary will be assigned to the orange class and similarly an observation on the blue side of the boundary will be assigned to the blue class
the bayes classifier produces the lowest possible test error rate called the bayes error rate
since the bayes classifier will always choose the class bayes error rate for which is largest the error rate at will be maxj pr
assessing model accuracy
in general the overall bayes error rate is given by max pr where the expectation averages the probability over all possible values of
for our simulated data the bayes error rate is
it is greater than zero because the classes overlap in the true population so maxj pr for some values of
the bayes error rate is analogous to the irreducible error discussed earlier
in theory we would always like to predict qualitative responses using the bayes classifier
but for real data we do not know the conditional distribution of given and so computing the bayes classifier is impossible
therefore the bayes classifier serves as an unattainable gold standard against which to compare other methods
many approaches attempt to estimate the conditional distribution of given and then classify given observation to the class with highest estimated probability
one such method is the nearest neighbors knn classifier
given positive nearest integer and test observation the knn classifier first identifies the neighbors points in the training data that are closest to represented by
it then estimates the conditional probability for class as the fraction of points in whose response values equal pr yi
finally knn applies bayes rule and classifies the test observation to the class with the largest probability
figure provides an illustrative example of the knn approach
in the left panel we have plotted small training data set consisting of six blue and six orange observations
our goal is to make prediction for the point labeled by the black cross
suppose that we choose
then knn will first identify the three observations that are closest to the cross
this neighborhood is shown as circle
it consists of two blue points and one orange point resulting in estimated probabilities of for the blue class and for the orange class
hence knn will predict that the black cross belongs to the blue class
in the right panel of figure we have applied the knn approach with at all of the possible values for and and have drawn in the corresponding knn decision boundary
despite the fact that it is very simple approach knn can often produce classifiers that are surprisingly close to the optimal bayes classifier
figure displays the knn decision boundary using when applied to the larger simulated data set from figure
notice that even though the true distribution is not known by the knn classifier the knn decision boundary is very close to that of the bayes classifier
the test error rate using knn is which is close to the bayes error rate of
the knn approach using is illustrated in simple situation with six blue observations and six orange observations
left test observation at which predicted class label is desired is shown as black cross
the closest points to the test observation are identified and it is predicted that the test observation belongs to the most commonly occurring class in this case blue
right the knn decision boundary for this example is shown in black
the blue grid indicates the region in which test observation will be assigned to the blue class and the orange grid indicates the region in which it will be assigned to the orange class
the choice of has drastic effect on the knn classifier obtained
figure displays two knn fits to the simulated data from figure using and
when the decision boundary is overly flexible and finds patterns in the data that don't correspond to the bayes decision boundary
this corresponds to classifier that has low bias but very high variance
as grows the method becomes less flexible and produces decision boundary that is close to linear
this corresponds to low variance but high bias classifier
on this simulated data set neither nor give good predictions they have test error rates of and respectively
just as in the regression setting there is not strong relationship between the training error rate and the test error rate
with the knn training error rate is but the test error rate may be quite high
in general as we use more flexible classification methods the training error rate will decline but the test error rate may not
in figure we have plotted the knn test and training errors as function of
as increases the method becomes more flexible
as in the regression setting the training error rate consistently declines as the flexibility increases
however the test error exhibits characteristic shape declining at first with minimum at approximately before increasing again when the method becomes excessively flexible and overfits
the black curve indicates the knn decision boundary on the data from figure using
the bayes decision boundary is shown as purple dashed line
the knn and bayes decision boundaries are very similar
comparison of the knn decision boundaries solid black curves obtained using and on the data from figure
with the decision boundary is overly flexible while with it is not sufficiently flexible
the bayes decision boundary is shown as purple dashed line
the knn training error rate blue observations and test error rate orange observations on the data from figure as the level of flexibility assessed using increases or equivalently as the number of neighbors decreases
the black dashed line indicates the bayes error rate
the jumpiness of the curves is due to the small size of the training data set
in both the regression and classification settings choosing the correct level of flexibility is critical to the success of any statistical learning method
the bias variance tradeoff and the resulting shape in the test error can make this difficult task
in chapter we return to this topic and discuss various methods for estimating test error rates and thereby choosing the optimal level of flexibility for given statistical learning method
lab introduction to in this lab we will introduce some simple commands
the best way to learn new language is to try out the commands
can be downloaded from http cran project org basic commands uses functions to perform operations
to run function called funcname function we type funcname input input where the inputs or arguments input argument
lab introduction to and input tell how to run the function
function can have any number of inputs
for example to create vector of numbers we use the function for concatenate
any numbers inside the parentheses are joined toc gether
the following command instructs to join together the numbers and and to save them as vector named
when we type it vector gives us back the vector note that the is not part of the command rather it is printed by to indicate that it is ready for another command to be entered
we can also save things using rather than hitting the up arrow multiple times will display the previous commands which can then be edited
this is useful since one often wishes to repeat similar command
in addition typing
funcname will always cause to open new help file window with additional information about the function funcname
we can tell to add sets of numbers together
it will then add the first number from to the first number from and so on
however and should be the same length
we can check their length using the length length function length length the ls function allows us to look at list of all of the objects such ls as data and functions that we have saved so far
the rm function can be rm used to delete any that we don't want ls rm ls character it's also possible to remove all objects at once rm list ls the matrix function can be used to create matrix of numbers
before matrix we use the matrix function we can learn more about it
statistical learning
matrix the help file reveals that the matrix function takes number of inputs but for now we focus on the first three the data the entries in the matrix the number of rows and the number of columns
first we create simple matrix matrix data nrow ncol note that we could just as well omit typing data nrow and ncol in the matrix command above that is we could just type matrix and this would have the same effect
however it can sometimes be useful to specify the names of the arguments passed in since otherwise will assume that the function arguments are passed into the function in the same order that is given in the function's help file
as this example illustrates by default creates matrices by successively filling in columns
alternatively the byrow true option can be used to populate the matrix in order of the rows matrix byrow true notice that in the above command we did not assign the matrix to value such as
in this case the matrix is printed to the screen but is not saved for future calculations
the sqrt function returns the square root of each sqrt element of vector or matrix
the command raises each element of to the power any powers are possible including fractional or negative powers sqrt the rnorm function generates vector of random normal variables rnorm with first argument the sample size
each time we call this function we will get different answer
here we create two correlated sets of numbers and and use the cor function to compute the correlation between cor them
lab introduction to rnorm rnorm mean sd cor by default rnorm creates standard normal random variables with mean of and standard deviation of
however the mean and standard deviation can be altered using the mean and sd arguments as illustrated above
sometimes we want our code to reproduce the exact same set of random numbers we can use the set seed function to do this
the set seed set seed function takes an arbitrary integer argument set seed rnorm
we use set seed throughout the labs whenever we perform calculations involving random quantities
in general this should allow the user to reproduce our results
however it should be noted that as new versions of become available it is possible that some small discrepancies may form between the book and the output from
the mean and var functions can be used to compute the mean and mean variance of vector of numbers
applying sqrt to the output of var var will give the standard deviation
or we can simply use the sd function sd set seed rnorm mean var sqrt var sd graphics the plot function is the primary way to plot data in
for instance plot plot produces scatterplot of the numbers in versus the numbers in
there are many additional options that can be passed in to the plot function
for example passing in the argument xlab will result in label on the axis
to find out more information about the plot function type
plot rnorm rnorm plot plot xlab this is the axis ylab this is the axis main plot of vs
statistical learning we will often want to save the output of an plot
the command that we use to do this will depend on the file type that we would like to create
for instance to create pdf we use the pdf function and to create jpeg pdf we use the jpeg function jpeg pdf figure pdf plot col green dev off null device the function dev off indicates to that we are done creating the plot dev off alternatively we can simply copy the plot window and paste it into an appropriate file type such as word document
the function seq can be used to create sequence of numbers
for seq instance seq makes vector of integers between and
there are many other options for instance seq length makes sequence of numbers that are equally spaced between and
typing is shorthand for seq for integer arguments seq seq pi pi length we will now create some more sophisticated plots
the contour funccontour tion produces contour plot in order to represent three dimensional data contour plot it is like topographical map
it takes three arguments vector of the values the first dimension vector of the values the second dimension and matrix whose elements correspond to the value the third dimension for each pair of coordinates
as with the plot function there are many other inputs that can be used to fine tune the output of the contour function
to learn more about these take look at the help file by typing
contour outer function cos contour contour nlevels add fa contour fa nlevels the image function works the same way as contour except that it image produces color coded plot whose colors depend on the value
this is
lab introduction to known as heatmap and is sometimes used to plot temperature in weather heatmap forecasts
alternatively persp can be used to produce three dimensional persp plot
the arguments theta and phi control the angles at which the plot is viewed image fa persp fa persp fa theta persp fa theta phi persp fa theta phi persp fa theta phi indexing data we often wish to examine part of set of data
suppose that our data is stored in the matrix
matrix then typing will select the element corresponding to the second row and the third column
the first number after the open bracket symbol always refers to the row and the second number always refers to the column
we can also select multiple rows and columns at time by providing vectors as the indices

statistical learning the last two examples include either no index for the columns or no index for the rows
these indicate that should include all columns or all rows respectively
treats single row or column of matrix as vector
the use of negative sign in the index tells to keep all rows or columns except those indicated in the index
the dim function outputs the number of rows followed by the number of dim columns of given matrix dim loading data for most analyses the first step involves importing data set into
the read table function is one of the primary ways to do this
the help file read table contains details about how to use this function
we can use the function write table to export data write table before attempting to load data set we must make sure that knows to search for the data in the proper directory
for example on windows system one could select the directory using the change dir option under the file menu
however the details of how to do this depend on the operating system
windows mac unix that is being used and so we do not give further details here
we begin by loading in the auto data set
this data is part of the islr library we discuss libraries in chapter but to illustrate the read table function we load it now from text file
the following command will load the auto data file into and store it as an object called auto in format referred to as data frame
the text file data frame can be obtained from this book's website
once the data has been loaded the fix function can be used to view it in spreadsheet like window
however the window must be closed before further commands can be entered
auto read table auto data fix auto
lab introduction to note that auto data is simply text file which you could alternatively open on your computer using standard text editor
it is often good idea to view data set using text editor or other software such as excel before loading it into
this particular data set has not been loaded correctly because has assumed that the variable names are part of the data and so has included them in the first row
the data set also includes number of missing observations indicated by question mark
missing values are common occurrence in real data sets
using the option header or header true in the read table function tells that the first line of the file contains the variable names and using the option na strings tells that any time it sees particular character or set of characters such as question mark it should be treated as missing element of the data matrix
auto read table auto data header na strings
fix auto excel is common format data storage program
an easy way to load such data into is to save it as csv comma separated value file and then use the read csv function to load it in
auto read csv auto csv header na strings
fix auto dim auto auto the dim function tells us that the data has observations or rows and dim variables or columns
there are various ways to deal with the missing data
in this case only of the rows contain missing observations and so we choose to use the na omit function to simply remove these rows na omit auto na omit auto dim auto once the data are loaded correctly we can use names to check the names variable names names auto mpg cylinders horsepowe weight year origin name additional graphical and numerical summaries we can use the plot function to produce scatterplots of the quantitative scatterplot variables
however simply typing the variable names will produce an error message because does not know to look in the auto data set for those variables
statistical learning plot cylinders mpg error in plot cylinders mpg object cylinders not found to refer to variable we must type the data set and the variable name joined with symbol
alternatively we can use the attach function in attach order to tell to make the variables in this data frame available by name plot auto cylinders auto mpg attach auto plot cylinders mpg the cylinders variable is stored as numeric vector so has treated it as quantitative
however since there are only small number of possible values for cylinders one may prefer to treat it as qualitative variable
the as factor function converts quantitative variables into qualitative as factor variables cylinders as factor cylinders if the variable plotted on the axis is categorial then boxplots will auboxplot tomatically be produced by the plot function
as usual number of options can be specified in order to customize the plots plot cylinders mpg plot cylinders mpg col red plot cylinders mpg col red varwidth plot cylinders mpg col red varwidth horizontal plot cylinders mpg col red varwidth xlab cylinders ylab mpg the hist function can be used to plot histogram
note that col hist has the same effect as col red histogram hist mpg hist mpg col hist mpg col breaks the pairs function creates scatterplot matrix scatterplot for every scatterplot matrix pair of variables for any given data set
we can also produce scatterplots for just subset of the variables pairs auto pairs mpg horsepower weight acceleration auto in conjunction with the plot function identify provides useful identify interactive method for identifying the value for particular variable for points on plot
we pass in three arguments to identify the axis variable the axis variable and the variable whose values we would like to see printed for each point
then clicking on given point in the plot will cause to print the value of the variable of interest
right clicking on the plot will exit the identify function control click on mac
the numbers printed under the identify function correspond to the rows for the selected points
exercises plot horsepower mpg identify horsepower mpg name the summary function produces numerical summary of each variable in summary particular data set summary auto mpg cylinders displacement min min min st qu st qu st qu median median median mean mean mean rd qu rd qu rd qu max max max horsepower weight acceleration min min min st qu st qu st qu median median median mean mean mean rd qu rd qu rd qu max max max year origin name min min amc matador st qu st qu ford pinto median median toyota corolla mean mean amc gremlin rd qu rd qu amc hornet max max chevrolet chevette other for qualitative variables such as name will list the number of observations that fall in each category
we can also produce summary of just single variable summary mpg min st qu
median mean rd qu
once we have finished using we type in order to shut it down or quit
when exiting we have the option to save the current workspace so workspace that all objects such as data sets that we have created in this session will be available next time
before exiting we may want to save record of all of the commands that we typed in the most recent session this can be accomplished using the savehistory function
next time we enter savehistory we can load that history using the loadhistory function loadhistory exercises
statistical learning conceptual
for each of parts through indicate whether or ii is correct and explain your answer
in general do we expect the performance of flexible statistical learning method to perform better or worse than an inflexible method when the sample size is extremely large and the number of predictors is small
the number of predictors is extremely large and the number of observations is small
the relationship between the predictors and response is highly non linear
the variance of the error terms
var is extremely high
explain whether each scenario is classification or regression problem and indicate whether we are most interested in inference or prediction
finally provide and we collect set of data on the top firms in the us
for each firm we record profit number of employees industry and the ceo salary
we are interested in understanding which factors affect ceo salary we are considering launching new product and wish to know whether it will be success or failure
we collect data on similar products that were previously launched
for each product we have recorded whether it was success or failure price charged for the product marketing budget competition price and ten other variables we are interesting in predicting the change in the us dollar in relation to the weekly changes in the world stock markets
hence we collect weekly data for all of
for each week we record the change in the dollar the change in the us market the change in the british market and the change in the german market
we now revisit the bias variance decomposition provide sketch of typical squared bias variance training error test error and bayes or irreducible error curves on single plot as we go from less flexible statistical learning methods towards more flexible approaches
the axis should represent the amount of flexibility in the method and the axis should
exercises represent the values for each curve
there should be five curves
make sure to label each one explain why each of the five curves has the shape displayed in part
you will now think of some real life applications for statistical learning describe three real life applications in which classification might be useful
describe the response as well as the predictors
is the goal of each application inference or prediction
explain your answer describe three real life applications in which regression might be useful
describe the response as well as the predictors
is the goal of each application inference or prediction
explain your answer describe three real life applications in which cluster analysis might be useful
what are the advantages and disadvantages of very flexible versus less flexible approach for regression or classification
under what circumstances might more flexible approach be preferred to less flexible approach
when might less flexible approach be preferred
describe the differences between parametric and non parametric statistical learning approach
what are the advantages of parametric approach to regression or classification as opposed to nonparametric approach
what are its disadvantages
the table below provides training data set containing observations predictors and qualitative response variable
red red red green green red suppose we wish to use this data set to make prediction for when using nearest neighbors compute the euclidean distance between each observation and the test point what is our prediction with
statistical learning what is our prediction with
if the bayes decision boundary in this problem is highly nonlinear then would we expect the best value for to be large or small
this exercise relates to the college data set which can be found in the file college csv
it contains number of variables for different universities and colleges in the us
call the loaded data college
make sure that you have the directory set to the correct location for the data look at the data using the fix function
you should notice that the first column is just the name of each university
we don't really want to treat this as data
however it may be handy to have these names for later
try the following commands
exercises rownames college college fix college you should see that there is now row names column with the name of each university recorded
this means that has given each row name corresponding to the appropriate university
will not try to perform calculations on the row names
however we still need to eliminate the first column in the data where the names are stored
try college college fix college now you should see that the first data column is private
note that another column labeled row names now appears before the private column
however this is not data column but rather the name that is giving to each row
use the summary function to produce numerical summary of the variables in the data set ii
use the pairs function to produce scatterplot matrix of the first ten columns or variables of the data
recall that you can reference the first columns of matrix using iii
use the plot function to produce side by side boxplots of outstate versus private iv
create new qualitative variable called elite by binning the top perc variable
we are going to divide universities into two groups based on whether or not the proportion of students coming from the top of their high school classes exceeds
elite rep no nrow college elite college top er yes elite as factor elite college data frame college elite use the summary function to see how many elite universities there are
now use the plot function to produce side by side boxplots of outstate versus elite
use the hist function to produce some histograms with differing numbers of bins for few of the quantitative variables
you may find the command par mfrow useful it will divide the print window into four regions so that four plots can be made simultaneously
modifying the arguments to this function will divide the screen in other ways vi
continue exploring the data and provide brief summary of what you discover
statistical learning
this exercise involves the auto data set studied in the lab which of the predictors are quantitative and which are qualitative
what is the range of each quantitative predictor
you can answer this using the range function range what is the mean and standard deviation of each quantitative predictor
now remove the th through th observations
what is the range mean and standard deviation of each predictor in the subset of the data that remains
using the full data set investigate the predictors graphically using scatterplots or other tools of your choice
create some plots highlighting the relationships among the predictors
comment on your findings suppose that we wish to predict gas mileage mpg on the basis of the other variables
do your plots suggest that any of the other variables might be useful in predicting mpg
justify your answer
this exercise involves the boston housing data set to begin load in the boston data set
the boston data set is part of the mass library in library mass now the data set is contained in the object boston print boston read about the data set
boston how many rows are in this data set
how many columns
what do the rows and columns represent
make some pairwise scatterplots of the predictors columns in this data set
describe your findings are any of the predictors associated with per capita crime rate
if so explain the relationship do any of the suburbs of boston appear to have particularly high crime rates
tax rates
pupil teacher ratios
comment on the range of each predictor how many of the suburbs in this data set bound the charles river
exercises what is the median pupil teacher ratio among the towns in this data set
which suburb of boston has lowest median value of owneroccupied homes
what are the values of the other predictors for that suburb and how do those values compare to the overall ranges for those predictors
comment on your findings in this data set how many of the suburbs average more than seven rooms per dwelling
more than eight rooms per dwelling
comment on the suburbs that average more than eight rooms per dwelling
statistical learning
this is page printer opaque this linear regression this chapter is about linear regression very simple approach for supervised learning
in particular linear regression is useful tool for predicting quantitative response
linear regression has been around for long time and is the topic of innumerable textbooks
though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters of this book linear regression is still useful and widely used statistical learning method
moreover it serves as good jumping off point for newer approaches as we will see in later chapters many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression
consequently the importance of having good understanding of linear regression before studying more complex learning methods cannot be overstated
in this chapter we review some of the key ideas underlying the linear regression model as well as the least squares approach that is most commonly used to fit this model
recall the advertising data from chapter
figure displays sales in thousands of units for particular product as function of advertising budgets in thousands of dollars for tv radio and newspaper media
suppose that in our role as statistical consultants we are asked to suggest on the basis of this data marketing plan for next year that will result in high product sales
what information would be useful in order to provide such recommendation
here are few important questions that we might seek to address
is there relationship between advertising budget and sales
our first goal should be to determine whether the data provide evi
linear regression dence of an association between advertising expenditure and sales
if the evidence is weak then one might argue that no money should be spent on advertising
how strong is the relationship between advertising budget and sales
assuming that there is relationship between advertising and sales we would like to know the strength of this relationship
in other words given certain advertising budget can we predict sales with high level of accuracy
this would be strong relationship
or is prediction of sales based on advertising expenditure only slightly better than random guess
this would be weak relationship
which media contribute to sales
do all three media tv radio and newspaper contribute to sales or do just one or two of the media contribute
to answer this question we must find way to separate out the individual effects of each medium when we have spent money on all three media
how accurately can we estimate the effect of each medium on sales
for every dollar spent on advertising in particular medium by what amount will sales increase
how accurately can we predict this amount of increase
how accurately can we predict future sales
for any given level of television radio or newspaper advertising what is our prediction for sales and what is the accuracy of this prediction
is the relationship linear
if there is approximately straight line relationship between advertising expenditure in the various media and sales then linear regression is an appropriate tool
if not then it may still be possible to transform the predictor or the response so that linear regression can be used
is there synergy among the advertising media
perhaps spending on television advertising and on radio advertising results in more sales than allocating to either television or radio individually
in marketing this is known as synergy effect while in statistics it is called an interaction effect synergy it turns out that linear regression can be used to answer each of these interaction questions
we will first discuss all of these questions in general context and then return to them in this specific context in section
simple linear regression simple linear regression lives up to its name it is very straightforward simple linear regression
simple linear regression approach for predicting quantitative response on the basis of single predictor variable
it assumes that there is approximately linear relationship between and
mathematically we can write this linear relationship as
you might read as is approximately modeled as
we will sometimes describe by saying that we are regressing on or onto
for example may represent tv advertising and may represent sales
then we can regress sales onto tv by fitting the model sales tv
in equation and are two unknown constants that represent the intercept and slope terms in the linear model
together and are intercept known as the model coefficients or parameters
once we have used our slope training data to produce estimates and for the model coefficients we coefficient can predict future sales on the basis of particular value of tv advertising parameter by computing where indicates prediction of on the basis of
here we use hat symbol to denote the estimated value for an unknown parameter or coefficient or to denote the predicted value of the response
estimating the coefficients in practice and are unknown
so before we can use to make predictions we must use data to estimate the coefficients
let xn yn represent observation pairs each of which consists of measurement of and measurement of
in the advertising example this data set consists of the tv advertising budget and product sales in different markets
recall that the data are displayed in figure
our goal is to obtain coefficient estimates and such that the linear model fits the available data well that is so that yi xi for
in other words we want to find an intercept and slope such that the resulting line is as close as possible to the data points
there are number of ways of measuring closeness
however by far the most common approach involves minimizing the least squares criterion least squares and we take that approach in this chapter
alternative approaches will be considered in chapter
let xi be the prediction for based on the ith value of
then ei yi represents the ith residual this is the difference residual
linear regression between the ith observed response value and the ith response value that is predicted by our linear model
we define the residual sum of squares rss residual sum of as squares rss or equivalently as rss yn xn
the least squares approach chooses and to minimize the rss
using some calculus one can show that the minimizers are pn yi pn xi where ni yi and ni xi are the sample means
in other words defines the least squares coefficient estimates for simple linear regression
for the advertising data the least squares fit for the regression of sales onto tv is shown
the fit is found by minimizing the sum of squared errors
each grey line segment represents an error and the fit makes compromise by averaging their squares
in this case linear fit captures the essence of the relationship although it is somewhat deficient in the left of the plot
figure displays the simple linear regression fit to the advertising data where and
in other words according to this
contour and three dimensional plots of the rss on the advertising data using sales as the response and tv as the predictor
the red dots correspond to the least squares estimates and given by approximation an additional spent on tv advertising is associated with selling approximately additional units of the product
in figure we have computed rss for number of values of and using the advertising data with sales as the response and tv as the predictor
in each plot the red dot represents the pair of least squares estimates given by
these values clearly minimize the rss
assessing the accuracy of the coefficient estimates recall from that we assume that the true relationship between and takes the form for some unknown function where is mean zero random error term
if is to be approximated by linear function then we can write this relationship as
here is the intercept term that is the expected value of when and is the slope the average increase in associated with one unit increase in
the error term is catch all for what we miss with this simple model the true relationship is probably not linear there may be other variables that cause variation in and there may be measurement error
we typically assume that the error term is independent of
the model given by defines the population regression line which population is the best linear approximation to the true relationship between and regression line
simulated data set
left the red line represents the true relationship which is known as the population regression line
the blue line is the least squares line it is the least squares estimate for based on the observed data shown in black
right the population regression line is again shown in red and the least squares line in dark blue
in light blue least squares lines are shown each computed on the basis of separate random set of observations
each least squares line is different but on average the least squares lines are quite close to the population regression line
the least squares regression coefficient estimates characterize the least squares line
the left hand panel of figure displays these least squares line two lines in simple simulated example
we created random xs and generated corresponding from the model where was generated from normal distribution with mean zero
the red line in the left hand panel of figure displays the true relationship while the blue line is the least squares estimate based on the observed data
the true relationship is generally not known for real data but the least squares line can always be computed using the coefficient estimates given in
in other words in real applications we have access to set of observations from which we can compute the least squares line however the population regression line is unobserved
in the right hand panel of figure we have generated different data sets from the model given by and plotted the corresponding least squares lines
notice that different data sets generated from the same true the assumption of linearity is often useful working model
however despite what many textbooks might tell us we seldom believe that the true relationship is linear
simple linear regression model result in slightly different least squares lines but the unobserved population regression line does not change
at first glance the difference between the population regression line and the least squares line may seem subtle and confusing
we only have one data set and so what does it mean that two different lines describe the relationship between the predictor and the response
fundamentally the concept of these two lines is natural extension of the standard statistical approach of using information from sample to estimate characteristics of large population
for example suppose that we are interested in knowing the population mean of some random variable
unfortunately is unknown but we do have access to observations from which we can pncan use to estimate
reasonable write as yn and which we estimate is where yi is the sample mean
the sample mean and the population mean are different but in general the sample mean will provide good estimate of the population mean
in the same way the unknown coefficients and in linear regression define the population regression line
we seek to estimate these unknown coefficients using and given in
these coefficient estimates define the least squares line
the analogy between linear regression and estimation of the mean of random variable is an apt one based on the concept of bias
if we use the bias sample mean to estimate this estimate is unbiased in the sense that unbiased on average we expect to equal
what exactly does this mean
it means that on the basis of one particular set of observations yn might overestimate and on the basis of another set of observations might underestimate
but if we could average huge number of estimates of obtained from huge number of sets of observations then this average would exactly equal
hence an unbiased estimator does not systematically overor under estimate the true parameter
the property of unbiasedness holds for the least squares coefficient estimates given by as well if we estimate and on the basis of particular data set then our estimates won't be exactly equal to and
but if we could average the estimates obtained over huge number of data sets then the average of these estimates would be spot on
in fact we can see from the righthand panel of figure that the average of many least squares lines each estimated from separate data set is pretty close to the true population regression line
we continue the analogy with the estimation of the population mean of random variable
natural question is as follows how accurate is the sample mean as an estimate of
we have established that the average of over many data sets will be very close to but that single estimate may be substantial underestimate or overestimate of
how far off will that single estimate of be
in general we answer this question by computing the standard error of written as se
we have standard error
linear regression the well known formula var se where is the standard deviation of each of the realizations yi of roughly speaking the standard error tells us the average amount that this estimate differs from the actual value of
equation also tells us how this deviation shrinks with the more observations we have the smaller the standard error of
in similar vein we can wonder how close and are to the true values and
to compute the standard errors associated with and we use the following formulas se pn se xi xi where var
for these formulas to be strictly valid we need to assume that the errors oi for each observation are uncorrelated with common variance
this is clearly not true in figure but the formula still turns out to be good approximation
notice in the formula that se is smaller when the xi are more spread out intuitively we have more leverage to estimate slope when this is the case
we also see that se would be the same as se if were zero in which case would be equal to
in general is not known but can be estimated from the data
this estimate is pknown as the residual standard error and is given by the formula residual standard rse rss
strictly speaking when is estimated from the error data we should write se to indicate that an estimate has been made but for simplicity of notation we will drop this extra hat
standard errors can be used to compute confidence intervals
confidence interval confidence interval is defined as range of values such that with probability the range will contain the true unknown value of the parameter
the range is defined in terms of lower and upper limits computed from the sample of data
for linear regression the confidence interval for approximately takes the form se
that is there is approximately chance that the interval se se will contain the true value of similarly confidence interval for approximately takes the form se
this formula holds provided that the observations are uncorrelated
approximately for several reasons
equation relies on the assumption that the errors are gaussian
also the factor of in front of the se term will vary slightly
simple linear regression in the case of the advertising data the confidence interval for is and the confidence interval for is
therefore we can conclude that in the absence of any advertising sales will on average fall somewhere between and units
furthermore for each increase in television advertising there will be an average increase in sales of between and units
standard errors can also be used to perform hypothesis tests on the hypothesis test coefficients
the most common hypothesis test involves testing the null hypothesis of null hypothesis there is no relationship between and versus the alternative hypothesis alternative hypothesis ha there is some relationship between and
mathematically this corresponds to testing versus ha since if then the model reduces to and is not associated with
to test the null hypothesis we need to determine whether our estimate for is sufficiently far from zero that we can be confident that is non zero
how far is far enough
this of course depends on the accuracy of that is it depends on se
if se is small then even relatively small values of may provide strong evidence that and hence that there is relationship between and
in contrast if se is large then must be large in absolute value in order for us to reject the null hypothesis
in practice we compute statistic statistic given by se which measures the number of standard deviations that is away from
if there really is no relationship between and then we expect that will have distribution with degrees of freedom
the tdistribution has bell shape and for values of greater than approximately it is quite similar to the normal distribution
consequently it is simple matter to compute the probability of observing any value equal to or larger assuming
we call this probability the value
roughly value depending on the number of observations in the linear regression
to be precise rather than the number should contain the quantile of distribution with degrees of freedom
details of how to compute the confidence interval precisely in will be provided later in this chapter
linear regression speaking we interpret the value as follows small value indicates that it is unlikely to observe such substantial association between the predictor and the response due to chance in the absence of any real association between the predictor and the response
hence if we see small value then we can infer that there is an association between the predictor and the response
we reject the null hypothesis that is we declare relationship to exist between and if the value is small enough
typical value cutoffs for rejecting the null hypothesis are or
when these correspond to statistics of around and respectively
coefficient std
error statistic value intercept tv table
for the advertising data coefficients of the least squares model for the regression of number of units sold on tv advertising budget
an increase of in the tv advertising budget is associated with an increase in sales by around units
recall that the sales variable is in thousands of units and the tv variable is in thousands of dollars
table provides details of the least squares model for the regression of number of units sold on tv advertising budget for the advertising data
notice that the coefficients for and are very large relative to their standard errors so the statistics are also large the probabilities of seeing such values if is true are virtually zero
hence we can conclude that and assessing the accuracy of the model once we have rejected the null hypothesis in favor of the alternative hypothesis it is natural to want to quantify the extent to which the model fits the data
the quality of linear regression fit is typically assessed using two related quantities the residual standard error rse and the statistic
table displays the rse the statistic and the statistic to be described in section for the linear regression of number of units sold on tv advertising budget
in table small value for the intercept indicates that we can reject the null hypothesis that and small value for tv indicates that we can reject the null hypothesis that
rejecting the latter null hypothesis allows us to conclude that there is relationship between tv and sales
rejecting the former allows us to conclude that in the absence of tv expenditure sales are non zero
simple linear regression quantity value residual standard error statistic table
for the advertising data more information about the least squares model for the regression of number of units sold on tv advertising budget
residual standard error recall from the model that associated with each observation is an error term
due to the presence of these error terms even if we knew the true regression line even if and were known we would not be able to perfectly predict from
the rse is an estimate of the standard deviation of
roughly speaking it is the average amount that the response will deviate from the true regression line
it is computed using the formula rse rss yi
note that rss was defined in section and is given by the formula rss yi
in the case of the advertising data we see from the linear regression output in table that the rse is
in other words actual sales in each market deviate from the true regression line by approximately units on average
another way to think about this is that even if the model were correct and the true values of the unknown coefficients and were known exactly any prediction of sales on the basis of tv advertising would still be off by about units on average
of course whether or not units is an acceptable prediction error depends on the problem context
in the advertising data set the mean value of sales over all markets is approximately units and so the percentage error is
the rse is considered measure of the lack of fit of the model to the data
if the predictions obtained using the model are very close to the true outcome values that is if yi for then will be small and we can conclude that the model fits the data very well
on the other hand if is very far from yi for one or more observations then the rse may be quite large indicating that the model doesn't fit the data well
statistic the rse provides an absolute measure of lack of fit of the model to the data
but since it is measured in the units of it is not always clear what
linear regression constitutes good rse
the statistic provides an alternative measure of fit
it takes the form of proportion the proportion of variance explained and so it always takes on value between and and is independent of the scale of
to calculate we use the formula tss rss rss tss tss where tss yi is the total sum of squares and rss is defined total sum of in
tss measures the total variance in the response and can be squares thought of as the amount of variability inherent in the response before the regression is performed
in contrast rss measures the amount of variability that is left unexplained after performing the regression
hence tss rss measures the amount of variability in the response that is explained or removed by performing the regression and measures the proportion of variability in that can be explained using
an statistic that is close to indicates that large proportion of the variability in the response has been explained by the regression
number near indicates that the regression did not explain much of the variability in the response this might occur because the linear model is wrong or the inherent error is high or both
in table the was and so just under two thirds of the variability in sales is explained by linear regression on tv
the statistic has an interpretational advantage over the rse since unlike the rse it always lies between and
however it can still be challenging to determine what is good value and in general this will depend on the application
for instance in certain problems in physics we may know that the data truly comes from linear model with small residual error
in this case we would expect to see an value that is extremely close to and substantially smaller value might indicate serious problem with the experiment in which the data were generated
on the other hand in typical applications in biology psychology marketing and other domains the linear model is at best an extremely rough approximation to the data and residual errors due to other unmeasured factors are often very large
in this setting we would expect only very small proportion of the variance in the response to be explained by the predictor and an value well below might be more realistic
the statistic is measure of the linear relationship between and
recall that correlation defined as correlation pn xi yi cor ppn ppn xi yi
multiple linear regression is also measure of the linear relationship between and this suggests that we might be able to use cor instead of in order to assess the fit of the linear model
in fact it can be shown that in the simple linear regression setting
in other words the squared correlation and the statistic are identical
however in the next section we will discuss the multiple linear regression problem in which we use several predictors simultaneously to predict the response
the concept of correlation between the predictors and the response does not extend automatically to this setting since correlation quantifies the association between single pair of variables rather than between larger number of variables
we will see that fills this role
multiple linear regression simple linear regression is useful approach for predicting response on the basis of single predictor variable
however in practice we often have more than one predictor
for example in the advertising data we have examined the relationship between sales and tv advertising
we also have data for the amount of money spent advertising on the radio and in newspapers and we may want to know whether either of these two media is associated with sales
how can we extend our analysis of the advertising data in order to accommodate these two additional predictors
one option is to run three separate simple linear regressions each of which uses different advertising medium as predictor
for instance we can fit simple linear regression to predict sales on the basis of the amount spent on radio advertisements
results are shown in table top table
we find that increase in spending on radio advertising is associated with an increase in sales by around units
table bottom table contains the least squares coefficients for simple linear regression of sales onto newspaper advertising budget
increase in newspaper advertising budget is associated with an increase in sales by approximately units
however the approach of fitting separate simple linear regression model for each predictor is not entirely satisfactory
first of all it is unclear how to make single prediction of sales given levels of the three advertising media budgets since each of the budgets is associated with separate regression equation
second each of the three regression equations ignores the other two media in forming estimates for the regression coefficients
we will see shortly that if the media budgets are correlated with each other in the we note that in fact the right hand side of is the sample correlation thus however we omit the hat for ease of it would be more correct to write cor notation
linear regression simple regression of sales on radio coefficient std
error statistic value intercept radio simple regression of sales on newspaper coefficient std
error statistic value intercept newspaper table
more simple linear regression models for the advertising data
coefficients of the simple linear regression model for number of units sold on top radio advertising budget and bottom newspaper advertising budget
increase in spending on radio advertising is associated with an average increase in sales by around units while the same increase in spending on newspaper advertising is associated with an average increase in sales by around units
note that the sales variable is in thousands of units and the radio and newspaper variables are in thousands of dollars markets that constitute our data set then this can lead to very misleading estimates of the individual media effects on sales
instead of fitting separate simple linear regression model for each predictor better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors
we can do this by giving each predictor separate slope coefficient in single model
in general suppose that we have distinct predictors
then the multiple linear regression model takes the form xp where xj represents the jth predictor and quantifies the association between that variable and the response
we interpret as the average effect on of one unit increase in xj holding all other predictors fixed
in the advertising example becomes sales tv radio newspaper
estimating the regression coefficients as was the case in the simple linear regression setting the regression coefficients in are unknown and must be estimated
given estimates we can make predictions using the formula xp
in three dimensional setting with two predictors and one response the least squares regression line becomes plane
the plane is chosen to minimize the sum of the squared vertical distances between each observation shown in red and the plane
the parameters are estimated using the same least squares approach that we saw in the context of simple linear regression
we choose to minimize the sum of squared residuals rss yi yi xi xi xip
the values that minimize are the multiple least squares regression coefficient estimates
unlike the simple linear regression estimates given in the multiple regression coefficient estimates have somewhat complicated forms that are most easily represented using matrix algebra
for this reason we do not provide them here
any statistical software package can be used to compute these coefficient estimates and later in this chapter we will show how this can be done in
figure illustrates an example of the least squares fit to toy data set with predictors
table displays the multiple regression coefficient estimates when tv radio and newspaper advertising budgets are used to predict product sales
linear regression coefficient std
error statistic value intercept tv radio newspaper table
for the advertising data least squares coefficient estimates of the multiple linear regression of number of units sold on radio tv and newspaper advertising budgets
tv radio newspaper sales tv radio newspaper sales table
correlation matrix for tv radio newspaper and sales for the advertising data using the advertising data
we interpret these results as follows for given amount of tv and newspaper advertising spending an additional on radio advertising leads to an increase in sales by approximately units
comparing these coefficient estimates to those displayed in tables and we notice that the multiple regression coefficient estimates for tv and radio are pretty similar to the simple linear regression coefficient estimates
however while the newspaper regression coefficient estimate in table was significantly non zero the coefficient estimate for newspaper in the multiple regression model is close to zero and the corresponding value is no longer significant with value around
this illustrates that the simple and multiple regression coefficients can be quite different
this difference stems from the fact that in the simple regression case the slope term represents the average effect of increase in newspaper advertising ignoring other predictors such as tv and radio
in contrast in the multiple regression setting the coefficient for newspaper represents the average effect of increasing newspaper spending by while holding tv and radio fixed
does it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite
in fact it does
consider the correlation matrix for the three predictor variables and response variable displayed in table
notice that the correlation between radio and newspaper is
this reveals tendency to spend more on newspaper advertising in markets where more is spent on radio advertising
now suppose that the multiple regression is correct and newspaper advertising has no direct impact on sales but radio advertising does increase sales
then in markets where we spend more on radio our sales will tend to be higher and as our correlation matrix
multiple linear regression shows we also tend to spend more on newspaper advertising in those same markets
hence in simple linear regression which only examines sales versus newspaper we will observe that higher values of newspaper tend to be associated with higher values of sales even though newspaper advertising does not actually affect sales
so newspaper sales are surrogate for radio advertising newspaper gets credit for the effect of radio ad sales
this slightly counterintuitive result is very common in many real life situations
consider an absurd example to illustrate the point
running regression of shark attacks versus ice cream sales for data collected at given beach community over period of time would show positive relationship similar to that seen between sales and newspaper
of course no one yet has suggested that ice creams should be banned at beaches to reduce shark attacks
in reality higher temperatures cause more people to visit the beach which in turn results in more ice cream sales and more shark attacks
multiple regression of attacks versus ice cream sales and temperature reveals that as intuition implies the former predictor is no longer significant after adjusting for temperature
some important questions when we perform multiple linear regression we usually are interested in answering few important questions
is at least one of the predictors xp useful in predicting the response
do all the predictors help to explain or is only subset of the predictors useful
how well does the model fit the data
given set of predictor values what response value should we predict and how accurate is our prediction
we now address each of these questions in turn
one is there relationship between the response and predictors
recall that in the simple linear regression setting in order to determine whether there is relationship between the response and the predictor we can simply check whether
in the multiple regression setting with predictors we need to ask whether all of the regression coefficients are zero whether
as in the simple linear regression setting we use hypothesis test to answer this question
we test the null hypothesis
linear regression quantity value residual standard error statistic table
more information about the least squares model for the regression of number of units sold on tv newspaper and radio advertising budgets in the advertising data
other information about this model was displayed in table versus the alternative ha at least one is non zero
this hypothesis test is performed by computing the statistic statistic tss rss rss where as with simple linear regression tss yi and rss yi
if the linear model assumptions are correct one can show that rss and that provided is true tss rss
hence when there is no relationship between the response and predictors one would expect the statistic to take on value close to
on the other hand if ha is true then tss rss so we expect to be greater than
the statistic for the multiple linear regression model obtained by regressing sales onto radio tv and newspaper is shown in table
in this example the statistic is
since this is far larger than it provides compelling evidence against the null hypothesis
in other words the large statistic suggests that at least one of the advertising media must be related to sales
however what if the statistic had been closer to
how large does the statistic need to be before we can reject and conclude that there is relationship
it turns out that the answer depends on the values of and
when is large an statistic that is just little larger than might still provide evidence against
in contrast larger statistic is needed to reject if is small
when is true and the errors oi have normal distribution the statistic follows an distribution for any given value of and any statistical software even if the errors are not normally distributed the statistic approximately follows an distribution provided that the sample size is large
multiple linear regression package can be used to compute the value associated with the statistic using this distribution
based on this value we can determine whether or not to reject
for the advertising data the value associated with the statistic in table is essentially zero so we have extremely strong evidence that at least one of the media is associated with increased sales
in we are testing that all the coefficients are zero
sometimes we want to test that particular subset of of the coefficients are zero
this corresponds to null hypothesis
where for convenience we have put the variables chosen for omission at the end of the list
in this case we fit second model that uses all the variables except those last
suppose that the residual sum of squares for that model is rss
then the appropriate statistic is rss rss
rss notice that in table for each individual predictor statistic and value were reported
these provide information about whether each individual predictor is related to the response after adjusting for the other predictors
it turns out that each of these are exactly equivalent to the ftest that omits that single variable from the model leaving all the others in in
so it reports the partial effect of adding that variable to the model
for instance as we discussed earlier these values indicate that tv and radio are related to sales but that there is no evidence that newspaper is associated with sales in the presence of these two
given these individual values for each variable why do we need to look at the overall statistic
after all it seems likely that if any one of the values for the individual variables is very small then at least one of the predictors is related to the response
however this logic is flawed especially when the number of predictors is large
for instance consider an example in which and
is true so no variable is truly associated with the response
in this situation about of the values associated with each variable of the type shown in table will be below by chance
in other words we expect to see approximately small values even in the absence of any true association between the predictors and the response
in fact we are almost guaranteed that we will observe at least one value below by chance
hence if we use the individual statistics and associated values in order to decide whether or not there is any association between the variables and the response there is very high chance that we will incorrectly conclude that there is relationship
however the statistic the square of each statistic is the corresponding statistic
linear regression does not suffer from this problem because it adjusts for the number of predictors
hence if is true there is only chance that the fstatistic will result in value below regardless of the number of predictors or the number of observations
the approach of using an statistic to test for any association between the predictors and the response works when is relatively small and certainly small compared to
however sometimes we have very large number of variables
if then there are more coefficients to estimate than observations from which to estimate them
in this case we cannot even fit the multiple linear regression model using least squares so the fstatistic cannot be used and neither can most of the other concepts that we have seen so far in this chapter
when is large some of the approaches discussed in the next section such as forward selection can be used
this high dimensional setting is discussed in greater detail in chapter high dimensional two deciding on important variables as discussed in the previous section the first step in multiple regression analysis is to compute the statistic and to examine the associated pvalue
if we conclude on the basis of that value that at least one of the predictors is related to the response then it is natural to wonder which are the guilty ones
we could look at the individual values as in table but as discussed if is large we are likely to make some false discoveries
it is possible that all of the predictors are associated with the response but it is more often the case that the response is only related to subset of the predictors
the task of determining which predictors are associated with the response in order to fit single model involving only those predictors is referred to as variable selection
the variable selection problem is studied variable selection extensively in chapter and so here we will provide only brief outline of some classical approaches
ideally we would like to perform variable selection by trying out lot of different models each containing different subset of the predictors
for instance if then we can consider four models model containing no variables model containing only model containing only and model containing both and
we can then select the best model out of all of the models that we have considered
how do we determine which model is best
various statistics can be used to judge the quality of model
these include mallow's cp akaike informamallow's cp tion criterion aic bayesian information criterion bic and adjusted akaike information
these are discussed in more detail in chapter
we can also deter criterion mine which model is best by plotting various model outputs such as the bayesian residuals in order to search for patterns information unfortunately there are total of models that contain subsets of criterion variables
this means that even for moderate trying out every possible adjusted subset of the predictors is infeasible
for instance we saw that if
multiple linear regression then there are models to consider
but if then we must consider models
this is not practical
therefore unless is very small we cannot consider all models and instead we need an automated and efficient approach to choose smaller set of models to consider
we begin with the null model model that forward selection contains an intercept but no predictors
we then fit simple linear null model regressions and add to the null model the variable that results in the lowest rss
we then add to that model the variable that results in the lowest rss for the new two variable model
this approach is continued until some stopping rule is satisfied
we start with all variables in the model and backward selection remove the variable with the largest value that is the variable that is the least statistically significant
the new variable model is fit and the variable with the largest value is removed
this procedure continues until stopping rule is reached
for instance we may stop when all remaining variables have value below some threshold
this is combination of forward and backward semixed selection lection
we start with no variables in the model and as with forward selection we add the variable that provides the best fit
we continue to add variables one by one
of course as we noted with the advertising example the values for variables can become larger as new predictors are added to the model
hence if at any point the value for one of the variables in the model rises above certain threshold then we remove that variable from the model
we continue to perform these forward and backward steps until all variables in the model have sufficiently low value and all variables outside the model would have large value if added to the model
backward selection cannot be used if while forward selection can always be used
forward selection is greedy approach and might include variables early that later become redundant
mixed selection can remedy this
three model fit two of the most common numerical measures of model fit are the rse and the fraction of variance explained
these quantities are computed and interpreted in the same fashion as for simple linear regression
recall that in simple regression is the square of the correlation of the response and the variable
in multiple linear regression it turns out that it equals cor the square of the correlation between the response and
linear regression the fitted linear model in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models
an value close to indicates that the model explains large portion of the variance in the response variable
as an example we saw in table that for the advertising data the model that uses all three advertising media to predict sales has an of
on the other hand the model that uses only tv and radio to predict sales has an value of
in other words there is small increase in if we include newspaper advertising in the model that already contains tv and radio advertising even though we saw earlier that the value for newspaper advertising in table is not significant
it turns out that will always increase when more variables are added to the model even if those variables are only weakly associated with the response
this is due to the fact that adding another variable to the least squares equations must allow us to fit the training data though not necessarily the testing data more accurately
thus the statistic which is also computed on the training data must increase
the fact that adding newspaper advertising to the model containing only tv and radio advertising leads to just tiny increase in provides additional evidence that newspaper can be dropped from the model
essentially newspaper provides no real improvement in the model fit to the training samples and its inclusion will likely lead to poor results on independent test samples due to overfitting
in contrast the model containing only tv as predictor had an of table
adding radio to the model leads to substantial improvement in
this implies that model that uses tv and radio expenditures to predict sales is substantially better than one that uses only tv advertising
we could further quantify this improvement by looking at the value for the radio coefficient in model that contains only tv and radio as predictors
the model that contains only tv and radio as predictors has an rse of and the model that also contains newspaper as predictor has an rse of table
in contrast the model that contains only tv has an rse of table
this corroborates our previous conclusion that model that uses tv and radio expenditures to predict sales is much more accurate on the training data than one that only uses tv spending
furthermore given that tv and radio expenditures are used as predictors there is no point in also using newspaper spending as predictor in the model
the observant reader may wonder how rse can increase when newspaper is added to the model given that rss must decrease
in general rse is defined as rse rss
for the advertising data linear regression fit to sales using tv and radio as predictors
from the pattern of the residuals we can see that there is pronounced non linear relationship in the data which simplifies to for simple linear regression
thus models with more variables can have higher rse if the decrease in rss is small relative to the increase in
in addition to looking at the rse and statistics just discussed it can be useful to plot the data
graphical summaries can reveal problems with model that are not visible from numerical statistics
for example figure displays three dimensional plot of tv and radio versus sales
we see that some observations lie above and some observations lie below the least squares regression plane
notice that there is clear pattern of negative residuals followed by positive residuals followed by negative residuals
in particular the linear model seems to overestimate sales for instances in which most of the advertising money was spent exclusively on either tv or radio
it underestimates sales for instances where the budget was split between the two media
this pronounced non linear pattern cannot be modeled accurately using linear regression
it suggests synergy or interaction effect between the advertising media whereby combining the media together results in bigger boost to sales than using any single medium
in section we will discuss extending the linear model to accommodate such synergistic effects through the use of interaction terms
linear regression four predictions once we have fit the multiple regression model it is straightforward to apply in order to predict the response on the basis of set of values for the predictors xp
however there are three sorts of uncertainty associated with this prediction
the coefficient estimates are estimates for
that is the least squares plane xp is only an estimate for the true population regression plane xp
the inaccuracy in the coefficient estimates is related to the reducible error from chapter
we can compute confidence interval in order to determine how close will be to
of course in practice assuming linear model for is almost always an approximation of reality so there is an additional source of potentially reducible error which we call model bias
so when we use linear model we are in fact estimating the best linear approximation to the true surface
however here we will ignore this discrepancy and operate as if the linear model were correct
even if we knew that is even if we knew the true values for the response value cannot be predicted perfectly because of the random error in the model
in chapter we referred to this as the irreducible error
how much will vary from
we use prediction intervals to answer this question
prediction intervals are always wider than confidence intervals because they incorporate both the error in the estimate for the reducible error and the uncertainty as to how much an individual point will differ from the population regression plane the irreducible error
we use confidence interval to quantify the uncertainty surrounding confidence interval the average sales over large number of cities
for example given that is spent on tv advertising and is spent on radio advertising in each city the confidence interval is
we interpret this to mean that of intervals of this form will contain the true value of on the other hand prediction interval can be used prediction interval in other words if we collect large number of data sets like the advertising data set and we construct confidence interval for the average sales on the basis of each data set given in tv and in radio advertising then of these confidence intervals will contain the true value of average sales
other considerations in the regression model to quantify the uncertainty surrounding sales for particular city
given that is spent on tv advertising and is spent on radio advertising in that city the prediction interval is
we interpret this to mean that of intervals of this form will contain the true value of for this city
note that both intervals are centered at but that the prediction interval is substantially wider than the confidence interval reflecting the increased uncertainty about sales for given city in comparison to the average sales over many locations
other considerations in the regression model qualitative predictors in our discussion so far we have assumed that all variables in our linear regression model are quantitative
but in practice this is not necessarily the case often some predictors are qualitative
for example the credit data set displayed in figure records balance average credit card debt for number of individuals as well as several quantitative predictors age cards number of credit cards education years of education income in thousands of dollars limit credit limit and rating credit rating
each panel of figure is scatterplot for pair of variables whose identities are given by the corresponding row and column labels
for example the scatterplot directly to the right of the word balance depicts balance versus age while the plot directly to the right of age corresponds to age versus cards
in addition to these quantitative variables we also have four qualitative variables gender student student status status marital status and ethnicity caucasian african american or asian
predictors with only two levels suppose that we wish to investigate differences in credit card balance between males and females ignoring the other variables for the moment
if qualitative predictor also known as factor only has two levels or possifactor ble values then incorporating it into regression model is very simple
we level simply create an indicator or dummy variable that takes on two possible dummy variable numerical values
for example based on the gender variable we can create new variable that takes the form if ith person is female xi if ith person is male
the credit data set contains information about balance age cards education income limit and rating for number of potential customers and use this variable as predictor in the regression equation
this results in the model if ith person is female yi xi oi if ith person is male
now can be interpreted as the average credit card balance among males as the average credit card balance among females and as the average difference in credit card balance between females and males
table displays the coefficient estimates and other information associated with the model
the average credit card debt for males is estimated to be whereas females are estimated to carry in additional debt for total of
however we notice that the value for the dummy variable is very high
this indicates
other considerations in the regression model coefficient std
error statistic value intercept gender female table
least squares coefficient estimates associated with the regression of balance onto gender in the credit data set
the linear model is given in
that is gender is encoded as dummy variable as in that there is no statistical evidence of difference in average credit card balance between the genders
the decision to code females as and males as in is arbitrary and has no effect on the regression fit but does alter the interpretation of the coefficients
if we had coded males as and females as then the estimates for and would have been and respectively leading once again to prediction of credit card debt of for males and prediction of for females
alternatively instead of coding scheme we could create dummy variable if ith person is female xi if ith person is male and use this variable in the regression equation
this results in the model if ith person is female yi xi oi
if ith person is male now can be interpreted as the overall average credit card balance ignoring the gender effect and is the amount that females are above the average and males are below the average
in this example the estimate for would be halfway between the male and female averages of and
the estimate for would be which is half of the average difference between females and males
it is important to note that the final predictions for the credit balances of males and females will be identical regardless of the coding scheme used
the only difference is in the way that the coefficients are interpreted
qualitative predictors with more than two levels when qualitative predictor has more than two levels single dummy variable cannot represent all possible values
in this situation we can create additional dummy variables
for example for the ethnicity variable we create two dummy variables
the first could be if ith person is asian xi if ith person is not asian
linear regression coefficient std
error statistic value intercept ethnicity asian ethnicity caucasian table
least squares coefficient estimates associated with the regression of balance onto ethnicity in the credit data set
the linear model is given in
that is ethnicity is encoded via two dummy variables and and the second could be if ith person is caucasian xi if ith person is not caucasian
then both of these variables can be used in the regression equation in order to obtain the model oi if ith person is asian yi xi xi oi oi if ith person is caucasian if ith person is african american
now can be interpreted as the average credit card balance for african americans can be interpreted as the difference in the average balance between the asian and african american categories and can be interpreted as the difference in the average balance between the caucasian and african american categories
there will always be one fewer dummy variable than the number of levels
the level with no dummy variable african american in this example is known as the baseline baseline from table we see that the estimated balance for the baseline african american is
it is estimated that the asian category will have less debt than the african american category and that the caucasian category will have less debt than the african american category
however the values associated with the coefficient estimates for the two dummy variables are very large suggesting no statistical evidence of real difference in credit card balance between the ethnicities
once again the level selected as the baseline category is arbitrary and the final predictions for each group will be the same regardless of this choice
however the coefficients and their values do depend on the choice of dummy variable coding
rather than rely on the individual coefficients we can use an test to test this does not depend on the coding
this test has value of indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity
using this dummy variable approach presents no difficulties when incorporating both quantitative and qualitative predictors
for example to regress balance on both quantitative variable such as income and qualitative variable such as student we must simply create dummy variable
other considerations in the regression model for student and then fit multiple regression model using income and the dummy variable as predictors for credit card balance
there are many different ways of coding qualitative variables besides the dummy variable approach taken here
all of these approaches lead to equivalent model fits but the coefficients are different and have different interpretations and are designed to measure particular contrasts
this topic contrast is beyond the scope of the book and so we will not pursue it further
extensions of the linear model the standard linear regression model provides interpretable results and works quite well on many real world problems
however it makes several highly restrictive assumptions that are often violated in practice
two of the most important assumptions state that the relationship between the predictors and response are additive and linear
the additive assumption additive means that the effect of changes in predictor xj on the response is linear independent of the values of the other predictors
the linear assumption states that the change in the response due to one unit change in xj is constant regardless of the value of xj
in this book we examine number of sophisticated methods that relax these two assumptions
here we briefly examine some common classical approaches for extending the linear model
removing the additive assumption in our previous analysis of the advertising data we concluded that both tv and radio seem to be associated with sales
the linear models that formed the basis for this conclusion assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media
for example the linear model states that the average effect on sales of one unit increase in tv is always regardless of the amount spent on radio
however this simple model may be incorrect
suppose that spending money on radio advertising actually increases the effectiveness of tv advertising so that the slope term for tv should increase as radio increases
in this situation given fixed budget of spending half on radio and half on tv may increase sales more than allocating the entire amount to either tv or to radio
in marketing this is known as synergy effect and in statistics it is referred to as an interaction effect
figure suggests that such an effect may be present in the advertising data
notice that when levels of either tv or radio are low then the true sales are lower than predicted by the linear model
but when advertising is split between the two media then the model tends to underestimate sales
consider the standard linear regression model with two variables
linear regression according to this model if we increase by one unit then will increase by an average of units
notice that the presence of does not alter this statement that is regardless of the value of one unit increase in will lead to unit increase in
one way of extending this model to allow for interaction effects is to include third predictor called an interaction term which is constructed by computing the product of and
this results in the model
how does inclusion of this interaction term relax the additive assumption
notice that can be rewritten as where
since changes with the effect of on is no longer constant adjusting will change the impact of on
for example suppose that we are interested in studying the productivity of factory
we wish to predict the number of units produced on the basis of the number of production lines and the total number of workers
it seems likely that the effect of increasing the number of production lines will depend on the number of workers since if no workers are available to operate the lines then increasing the number of lines will not increase production
this suggests that it would be appropriate to include an interaction term between lines and workers in linear model to predict units
suppose that when we fit the model we obtain units lines workers lines workers workers lines workers in other words adding an additional line will increase the number of units produced by workers
hence the more workers we have the stronger will be the effect of lines
we now return to the advertising example
linear model that uses radio tv and an interaction between the two to predict sales takes the form sales tv radio radio tv radio tv radio
we can interpret as the increase in the effectiveness of tv advertising for one unit increase in radio advertising or vice versa
the coefficients that result from fitting the model are given in table
the results in table strongly suggest that the model that includes the interaction term is superior to the model that contains only main effects main effect
other considerations in the regression model coefficient std
error statistic value intercept tv radio tv radio table
for the advertising data least squares coefficient estimates associated with the regression of sales onto tv and radio with an interaction term as in
the value for the interaction term tv radio is extremely low indicating that there is strong evidence for ha
in other words it is clear that the true relationship is not additive
the for the model is compared to only for the model that predicts sales using tv and radio without an interaction term
this means that of the variability in sales that remains after fitting the additive model has been explained by the interaction term
the coefficient estimates in table suggest that an increase in tv advertising of is associated with increased sales of radio radio units
and an increase in radio advertising of will be associated with an increase in sales of tv tv units
in this example the values associated with tv radio and the interaction term all are statistically significant table and so it is obvious that all three variables should be included in the model
however it is sometimes the case that an interaction term has very small value but the associated main effects in this case tv and radio do not
the hierarchical principle states that if we include an interaction in model we hierarchical should also include the main effects even if the values associated with principle their coefficients are not significant
in other words if the interaction between and seems important then we should include both and in the model even if their coefficient estimates have large values
the rationale for this principle is that if is related to the response then whether or not the coefficients of or are exactly zero is of little interest
also is typically correlated with and and so leaving them out tends to alter the meaning of the interaction
in the previous example we considered an interaction between tv and radio both of which are quantitative variables
however the concept of interactions applies just as well to qualitative variables or to combination of quantitative and qualitative variables
in fact an interaction between qualitative variable and quantitative variable has particularly nice interpretation
consider the credit data set from section and suppose that we wish to predict balance using the income quantitative and student qualitative variables
in the absence of an interaction term the model
for the credit data the least squares lines are shown for prediction of balance from income for students and non students
left the model was fit
there is no interaction between income and student
right the model was fit
there is an interaction term between income and student takes the form if ith person is student balancei incomei if ith person is not student if ith person is student incomei if ith person is not student
notice that this amounts to fitting two parallel lines to the data one for students and one for non students
the lines for students and non students have different intercepts versus but the same slope
this is illustrated in the left hand panel of figure
the fact that the lines are parallel means that the average effect on balance of one unit increase in income does not depend on whether or not the individual is student
this represents potentially serious limitation of the model since in fact change in income may have very different effect on the credit card balance of student versus non student
this limitation can be addressed by adding an interaction variable created by multiplying income with the dummy variable for student
our model now becomes incomei if student balancei incomei if not student incomei if student incomei if not student once again we have two different regression lines for the students and the non students
but now those regression lines have different intercepts
the auto data set
for number of cars mpg and horsepower are shown
the linear regression fit is shown in orange
the linear regression fit for model that includes horsepower is shown as blue curve
the linear regression fit for model that includes all polynomials of horsepower up to fifth degree is shown in green
versus as well as different slopes versus
this allows for the possibility that changes in income may affect the credit card balances of students and non students differently
the right hand panel of figure shows the estimated relationships between income and balance for students and non students in the model
we note that the slope for students is lower than the slope for non students
this suggests that increases in income are associated with smaller increases in credit card balance among students as compared to non students
non linear relationships as discussed previously the linear regression model assumes linear relationship between the response and predictors
but in some cases the true relationship between the response and the predictors may be nonlinear
here we present very simple way to directly extend the linear model to accommodate non linear relationships using polynomial regression
in polynomial later chapters we will present more complex approaches for performing regression non linear fits in more general settings
consider figure in which the mpg gas mileage in miles per gallon versus horsepower is shown for number of cars in the auto data set
the orange line represents the linear regression fit
there is pronounced rela
linear regression coefficient std
error statistic value intercept horsepower horsepower table
for the auto data set least squares coefficient estimates associated with the regression of mpg onto horsepower and horsepower tionship between mpg and horsepower but it seems clear that this relationship is in fact non linear the data suggest curved relationship
simple approach for incorporating non linear associations in linear model is to include transformed versions of the predictors in the model
for example the points in figure seem to have quadratic shape suggesting that quadratic model of the form mpg horsepower horsepower may provide better fit
equation involves predicting mpg using non linear function of horsepower
but it is still linear model
that is is simply multiple linear regression model with horsepower and horsepower
so we can use standard linear regression software to estimate and in order to produce non linear fit
the blue curve in figure shows the resulting quadratic fit to the data
the quadratic fit appears to be substantially better than the fit obtained when just the linear term is included
the of the quadratic fit is compared to for the linear fit and the value in table for the quadratic term is highly significant
if including horsepower led to such big improvement in the model why not include horsepower horsepower or even horsepower
the green curve in figure displays the fit that results from including all polynomials up to fifth degree in the model
the resulting fit seems unnecessarily wiggly that is it is unclear that including the additional terms really has led to better fit to the data
the approach that we have just described for extending the linear model to accommodate non linear relationships is known as polynomial regression since we have included polynomial functions of the predictors in the regression model
we further explore this approach and other non linear extensions of the linear model in chapter
potential problems when we fit linear regression model to particular data set many problems may occur
most common among these are the following
non linearity of the response predictor relationships
correlation of error terms
plots of residuals versus predicted or fitted values for the auto data set
in each plot the red line is smooth fit to the residuals intended to make it easier to identify trend
left linear regression of mpg on horsepower
strong pattern in the residuals indicates non linearity in the data
right linear regression of mpg on horsepower and horsepower
there is little pattern in the residuals
non constant variance of error terms
high leverage points
in practice identifying and overcoming these problems is as much an art as science
many pages in countless books have been written on this topic
since the linear regression model is not our primary focus here we will provide only brief summary of some key points
non linearity of the data the linear regression model assumes that there is straight line relationship between the predictors and the response
if the true relationship is far from linear then virtually all of the conclusions that we draw from the fit are suspect
in addition the prediction accuracy of the model can be significantly reduced
residual plots are useful graphical tool for identifying non linearity residual plot given simple linear regression model we can plot the residuals ei yi versus the predictor xi
in the case of multiple regression model since there are multiple predictors we instead plot the residuals versus the predicted or fitted values
ideally the residual plot will show no fitted
linear regression discernible pattern
the presence of pattern may indicate problem with some aspect of the linear model
the left panel of figure displays residual plot from the linear regression of mpg onto horsepower on the auto data set that was illustrated in figure
the red line is smooth fit to the residuals which is displayed in order to make it easier to identify any trends
the residuals exhibit clear shape which provides strong indication of non linearity in the data
in contrast the right hand panel of figure displays the residual plot that results from the model which contains quadratic term
there appears to be little pattern in the residuals suggesting that the quadratic term improves the fit to the data
if the residual plot indicates that there are non linear associations in the data then simple approach is to use non linear transformations of the predictors such as log and in the regression model
in the later chapters of this book we will discuss other more advanced non linear approaches for addressing this issue
correlation of error terms an important assumption of the linear regression model is that the error terms on are uncorrelated
what does this mean
for instance if the errors are uncorrelated then the fact that oi is positive provides little or no information about the sign of oi
the standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms
if in fact there is correlation among the error terms then the estimated standard errors will tend to underestimate the true standard errors
as result confidence and prediction intervals will be narrower than they should be
for example confidence interval may in reality have much lower probability than of containing the true value of the parameter
in addition pvalues associated with the model will be lower than they should be this could cause us to erroneously conclude that parameter is statistically significant
in short if the error terms are correlated we may have an unwarranted sense of confidence in our model
as an extreme example suppose we accidentally doubled our data leading to observations and error terms identical in pairs
if we ignored this our standard error calculations would be as if we had sample of size when in fact we have only samples
our estimated parameters would be the same for the samples as for thevn samples but the confidence intervals would be narrower by factor of
why might correlations among the error terms occur
such correlations frequently occur in the context of time series data which consists of obtime series servations for which measurements are obtained at discrete points in time
in many cases observations that are obtained at adjacent time points will have positively correlated errors
in order to determine if this is the case
plots of residuals from simulated time series data sets generated with differing levels of correlation between error terms for adjacent time points for given data set we can plot the residuals from our model as function of time
if the errors are uncorrelated then there should be no discernible pattern
on the other hand if the error terms are positively correlated then we may see tracking in the residuals that is adjacent residuals tracking may have similar values
figure provides an illustration
in the top panel we see the residuals from linear regression fit to data generated with uncorrelated errors
there is no evidence of time related trend in the residuals
in contrast the residuals in the bottom panel are from data set in which adjacent errors had correlation of
now there is clear pattern in the residuals adjacent residuals tend to take on similar values
finally the center panel illustrates more moderate case in which the residuals had correlation of
there is still evidence of tracking but the pattern is less clear
many methods have been developed to properly take account of correlations in the error terms in time series data
correlation among the error terms can also occur outside of time series data
for instance consider study in which individuals heights are predicted from their weights
residual plots
in each plot the red line is smooth fit to the residuals intended to make it easier to identify trend
the blue lines track the outer quantiles of the residuals and emphasize patterns
left the funnel shape indicates heteroscedasticity
right the predictor has been log transformed and there is now no evidence of heteroscedasticity assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family or eat the same diet or have been exposed to the same environmental factors
in general the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods and good experimental design is crucial in order to mitigate the risk of such correlations
non constant variance of error terms another important assumption of the linear regression model is that the error terms have constant variance var oi
the standard errors confidence intervals and hypothesis tests associated with the linear model rely upon this assumption
unfortunately it is often the case that the variances of the error terms are non constant
for instance the variances of the error terms may increase with the value of the response
one can identify non constant variances in the errors or heteroscedasticity from the presence of funnel shape in the heteroscedasticity residual plot
an example is shown in the left hand panel of figure in which the magnitude of the residuals tends to increase with the fitted values
when faced with this problem one possible solution isvto transform the response using concave function such as log or
such transformation results in greater amount of shrinkage of the larger responses leading to reduction in heteroscedasticity
the right hand panel of figure displays the residual plot after transforming the response
left the least squares regression line is shown in red and the regression line after removing the outlier is shown in blue
center the residual plot clearly identifies the outlier
right the outlier has studentized residual of typically we expect values between and using log
the residuals now appear to have constant variance though there is some evidence of slight non linear relationship in the data
sometimes we have good idea of the variance of each response
for example the ith response could be an average of ni raw observations
if each of these raw observations is uncorrelated with variance then their average has variance ni
in this case simple remedy is to fit our model by weighted least squares with weights proportional to the inverse weighted least variances wi ni in this case
most linear regression software allows squares for observation weights
outliers an outlier is point for which yi is far from the value predicted by the outlier model
outliers can arise for variety of reasons such as incorrect recording of an observation during data collection
the red point observation in the left hand panel of figure illustrates typical outlier
the red solid line is the least squares regression fit while the blue dashed line is the least squares fit after removal of the outlier
in this case removing the outlier has little effect on the least squares line it leads to almost no change in the slope and miniscule reduction in the intercept
it is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit
however even if an outlier does not have much effect on the least squares fit it can cause other problems
for instance in this example the rse is when the outlier is included in the regression but it is only when the outlier is removed
since the rse is used to compute all confidence intervals and values such dramatic increase caused by single data point can have implications for the interpretation of the fit
similarly inclusion of the outlier causes the to decline from to
left observation is high leverage point while is not
the red line is the fit to all the data and the blue line is the fit with observation removed
center the red observation is not unusual in terms of its value or its value but still falls outside the bulk of the data and hence has high leverage
right observation has high leverage and high residual
residual plots can be used to identify outliers
in this example the outlier is clearly visible in the residual plot illustrated in the center panel of figure
but in practice it can be difficult to decide how large residual needs to be before we consider the point to be an outlier
to address this problem instead of plotting the residuals we can plot the studentized residuals computed by dividing each residual ei by its estimated standard studentized error
observations whose studentized residuals are greater than in abso residual lute value are possible outliers
in the right hand panel of figure the outlier's studentized residual exceeds while all other observations have studentized residuals between and
if we believe that an outlier has occurred due to an error in data collection or recording then one solution is to simply remove the observation
however care should be taken since an outlier may instead indicate deficiency with the model such as missing predictor
high leverage points we just saw that outliers are observations for which the response yi is unusual given the predictor xi
in contrast observations with high leverage high leverage have an unusual value for xi
for example observation in the left hand panel of figure has high leverage in that the predictor value for this observation is large relative to the other observations
note that the data displayed in figure are the same as the data displayed in figure but with the addition of single high leverage observation
the red solid line is the least squares fit to the data while the blue dashed line is the fit produced when observation is removed
comparing the left hand panels of figures and we observe that removing the high leverage observation has much more substantial impact on the least squares line than removing the outlier
in fact high leverage observations tend to have
other considerations in the regression model sizable impact on the estimated regression line
it is cause for concern if the least squares line is heavily affected by just couple of observations because any problems with these points may invalidate the entire fit
for this reason it is important to identify high leverage observations
in simple linear regression high leverage observations are fairly easy to identify since we can simply look for observations for which the predictor value is outside of the normal range of the observations
but in multiple linear regression with many predictors it is possible to have an observation that is well within the range of each individual predictor's values but that is unusual in terms of the full set of predictors
an example is shown in the center panel of figure for data set with two predictors and
most of the observations predictor values fall within the blue dashed ellipse but the red observation is well outside of this range
but neither its value for nor its value for is unusual
so if we examine just or just we will fail to notice this high leverage point
this problem is more pronounced in multiple regression settings with more than two predictors because then there is no simple way to plot all dimensions of the data simultaneously
in order to quantify an observation's leverage we compute the leverage statistic
large value of this statistic indicates an observation with high leverage statistic leverage
for simple linear regression xi hi pn
xi it is clear from this equation that hi increases with the distance of xi from
there is simple extension of hi to the case of multiple predictors though we do not provide the formula here
the leverage statistic hi is always between and and the average leverage for all the observations is always equal to
so if given observation has leverage statistic that greatly exceeds then we may suspect that the corresponding point has high leverage
the right hand panel of figure provides plot of the studentized residuals versus hi for the data in the left hand panel of figure
observation stands out as having very high leverage statistic as well as high studentized residual
in other words it is an outlier as well as high leverage observation
this is particularly dangerous combination
this plot also reveals the reason that observation had relatively little effect on the least squares fit in figure it has low leverage
collinearity collinearity refers to the situation in which two or more predictor variables collinearity are closely related to one another
the concept of collinearity is illustrated in figure using the credit data set
in the left hand panel of figure the two predictors limit and age appear to have no obvious rela
scatterplots of the observations from the credit data set
left plot of age versus limit
these two variables are not collinear
right plot of rating versus limit
there is high collinearity
contour plots for the rss values as function of the parameters for various regressions involving the credit data set
in each plot the black dots represent the coefficient values corresponding to the minimum rss
left contour plot of rss for the regression of balance onto age and limit
the minimum value is well defined
right contour plot of rss for the regression of balance onto rating and limit
because of the collinearity there are many pairs limit rating with similar value for rss
other considerations in the regression model tionship
in contrast in the right hand panel of figure the predictors limit and rating are very highly correlated with each other and we say that they are collinear
the presence of collinearity can pose problems in the regression context since it can be difficult to separate out the individual effects of collinear variables on the response
in other words since limit and rating tend to increase or decrease together it can be difficult to determine how each one separately is associated with the response balance
figure illustrates some of the difficulties that can result from collinearity
the left hand panel of figure is contour plot of the rss associated with different possible coefficient estimates for the regression of balance on limit and age
each ellipse represents set of coefficients that correspond to the same rss with ellipses nearest to the center taking on the lowest values of rss
the black dots and associated dashed lines represent the coefficient estimates that result in the smallest possible rss in other words these are the least squares estimates
the axes for limit and age have been scaled so that the plot includes possible coefficient estimates that are up to standard errors on either side of the least squares estimates
thus the plot includes all plausible values for the coefficients
for example we see that the true limit coefficient is almost certainly somewhere between and
in contrast the right hand panel of figure displays contour plots of the rss associated with possible coefficient estimates for the regression of balance onto limit and rating which we know to be highly collinear
now the contours run along narrow valley there is broad range of values for the coefficient estimates that result in equal values for rss
hence small change in the data could cause the pair of coefficient values that yield the smallest rss that is the least squares estimates to move anywhere along this valley
this results in great deal of uncertainty in the coefficient estimates
notice that the scale for the limit coefficient now runs from roughly to this is an eight fold increase over the plausible range of the limit coefficient in the regression with age
interestingly even though the limit and rating coefficients now have much more individual uncertainty they will almost certainly lie somewhere in this contour valley
for example we would not expect the true value of the limit and rating coefficients to be and respectively even though such value is plausible for each coefficient individually
since collinearity reduces the accuracy of the estimates of the regression coefficients it causes the standard error for to grow
recall that the statistic for each predictor is calculated by dividing by its standard error
consequently collinearity results in decline in the statistic
as result in the presence of collinearity we may fail to reject
this means that the power of the hypothesis test the probability of correctly power detecting non zero coefficient is reduced by collinearity
linear regression coefficient std
error statistic value intercept model age limit intercept model rating limit table
the results for two multiple regression models involving the credit data set are shown
model is regression of balance on age and limit and model regression of balance on rating and limit
the standard error of limit increases fold in the second regression due to collinearity
table compares the coefficient estimates obtained from two separate multiple regression models
the first is regression of balance on age and limit and the second is regression of balance on rating and limit
in the first regression both age and limit are highly significant with very small pvalues
in the second the collinearity between limit and rating has caused the standard error for the limit coefficient estimate to increase by factor of and the value to increase to
in other words the importance of the limit variable has been masked due to the presence of collinearity
to avoid such situation it is desirable to identify and address potential collinearity problems while fitting the model
simple way to detect collinearity is to look at the correlation matrix of the predictors
an element of this matrix that is large in absolute value indicates pair of highly correlated variables and therefore collinearity problem in the data
unfortunately not all collinearity problems can be detected by inspection of the correlation matrix it is possible for collinearity to exist between three or more variables even if no pair of variables has particularly high correlation
we call this situation multicollinearity multicollinearity instead of inspecting the correlation matrix better way to assess multicollinearity is to compute the variance inflation factor vif
the vif is variance inflation the ratio of the variance of when fitting the full model divided by the factor variance of if fit on its own
the smallest possible value for vif is which indicates the complete absence of collinearity
typically in practice there is small amount of collinearity among the predictors
as rule of thumb vif value that exceeds or indicates problematic amount of collinearity
the vif for each variable can be computed using the formula if rx where rx is the from regression of xj onto all of the other predictors
if rxj is close to one then collinearity is present and so the vif will be large
the marketing plan in the credit data regression of balance on age rating and limit indicates that the predictors have vif values of and
as we suspected there is considerable collinearity in the data
when faced with the problem of collinearity there are two simple solutions
the first is to drop one of the problematic variables from the regression
this can usually be done without much compromise to the regression fit since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables
for instance if we regress balance onto age and limit without the rating predictor then the resulting vif values are close to the minimum possible value of and the drops from to
so dropping rating from the set of predictors has effectively solved the collinearity problem without compromising the fit
the second solution is to combine the collinear variables together into single predictor
for instance we might take the average of standardized versions of limit and rating in order to create new variable that measures credit worthiness
the marketing plan we now briefly return to the seven questions about the advertising data that we set out to answer at the beginning of this chapter
is there relationship between advertising sales and budget
this question can be answered by fitting multiple regression model of sales onto tv radio and newspaper as in and testing the hypothesis tv radio newspaper
in section we showed that the statistic can be used to determine whether or not we should reject this null hypothesis
in this case the value corresponding to the statistic in table is very low indicating clear evidence of relationship between advertising and sales
how strong is the relationship
we discussed two measures of model accuracy in section
first the rse estimates the standard deviation of the response from the population regression line
for the advertising data the rse is units while the mean value for the response is indicating percentage error of roughly
second the statistic records the percentage of variability in the response that is explained by the predictors
the predictors explain almost of the variance in sales
the rse and statistics are displayed in table
which media contribute to sales
to answer this question we can examine the values associated with
linear regression each predictor's statistic section
in the multiple linear regression displayed in table the values for tv and radio are low but the value for newspaper is not
this suggests that only tv and radio are related to sales
in chapter we explore this question in greater detail
how large is the effect of each medium on sales
we saw in section that the standard error of can be used to construct confidence intervals for
for the advertising data the confidence intervals are as follows for tv for radio and for newspaper
the confidence intervals for tv and radio are narrow and far from zero providing evidence that these media are related to sales
but the interval for newspaper includes zero indicating that the variable is not statistically significant given the values of tv and radio
we saw in section that collinearity can result in very wide standard errors
could collinearity be the reason that the confidence interval associated with newspaper is so wide
the vif scores are and for tv radio and newspaper suggesting no evidence of collinearity
in order to assess the association of each medium individually on sales we can perform three separate simple linear regressions
results are shown in tables and
there is evidence of an extremely strong association between tv and sales and between radio and sales
there is evidence of mild association between newspaper and sales when the values of tv and radio are ignored
how accurately can we predict future sales
the response can be predicted using
the accuracy associated with this estimate depends on whether we wish to predict an individual response or the average response section
if the former we use prediction interval and if the latter we use confidence interval
prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with the irreducible error
is the relationship linear
in section we saw that residual plots can be used in order to identify non linearity
if the relationships are linear then the residual plots should display no pattern
in the case of the advertising data we observe non linear effect in figure though this effect could also be observed in residual plot
in section we discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non linear relationships
comparison of linear regression with nearest neighbors
is there synergy among the advertising media
the standard linear regression model assumes an additive relationship between the predictors and the response
an additive model is easy to interpret because the effect of each predictor on the response is unrelated to the values of the other predictors
however the additive assumption may be unrealistic for certain data sets
in section we showed how to include an interaction term in the regression model in order to accommodate non additive relationships
small value associated with the interaction term indicates the presence of such relationships
figure suggested that the advertising data may not be additive
including an interaction term in the model results in substantial increase in from around to almost
comparison of linear regression with nearest neighbors as discussed in chapter linear regression is an example of parametric approach because it assumes linear functional form for
parametric methods have several advantages
they are often easy to fit because one need estimate only small number of coefficients
in the case of linear regression the coefficients have simple interpretations and tests of statistical significance can be easily performed
but parametric methods do have disadvantage by construction they make strong assumptions about the form of
if the specified functional form is far from the truth and prediction accuracy is our goal then the parametric method will perform poorly
for instance if we assume linear relationship between and but the true relationship is far from linear then the resulting model will provide poor fit to the data and any conclusions drawn from it will be suspect
in contrast non parametric methods do not explicitly assume parametric form for and thereby provide an alternative and more flexible approach for performing regression
we discuss various non parametric methods in this book
here we consider one of the simplest and best known non parametric methods nearest neighbors regression knn regression
nearest the knn regression method is closely related to the knn classifier dis neighbors cussed in chapter
given value for and prediction point knn regression regression first identifies the training observations that are closest to represented by
it then estimates using the average of all the training responses in
in other words yi
xi
plots of using knn regression on two dimensional data set with observations orange dots
left results in rough step function fit
right produces much smoother fit
figure illustrates two knn fits on data set with predictors
the fit with is shown in the left hand panel while the right hand panel corresponds to
we see that when the knn fit perfectly interpolates the training observations and consequently takes the form of step function
when the knn fit still is step function but averaging over observations results in much smaller regions of constant prediction and consequently smoother fit
in general the optimal value for will depend on the bias variance tradeoff which we introduced in chapter
small value for provides the most flexible fit which will have low bias but high variance
this variance is due to the fact that the prediction in given region is entirely dependent on just one observation
in contrast larger values of provide smoother and less variable fit the prediction in region is an average of several points and so changing one observation has smaller effect
however the smoothing may cause bias by masking some of the structure in
in chapter we introduce several approaches for estimating test error rates
these methods can be used to identify the optimal value of in knn regression
in what setting will parametric approach such as least squares linear regression outperform non parametric approach such as knn regression
the answer is simple the parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of
figure provides an example with data generated from one dimensional linear regression model
the black solid lines represent while the blue curves correspond to the knn fits using and
in this case the predictions are far too variable while the smoother fit is much closer to
however since the true relationship is linear it is hard for non parametric approach to compete with linear regression non parametric approach incurs cost in variance
plots of using knn regression on one dimensional data set with observations
the true relationship is given by the black solid line
left the blue curve corresponds to and interpolates passes directly through the training data
right the blue curve corresponds to and represents smoother fit
the same data set shown in figure is investigated further
left the blue dashed line is the least squares fit to the data
since is in fact linear displayed as the black line the least squares regression line provides very good estimate of
right the dashed horizontal line represents the least squares test set mse while the green solid line corresponds to the mse for knn as function of on the log scale
linear regression achieves lower test mse than does knn regression since is in fact linear
for knn regression the best results occur with very large value of corresponding to small value of
linear regression that is not offset by reduction in bias
the blue dashed line in the lefthand panel of figure represents the linear regression fit to the same data
it is almost perfect
the right hand panel of figure reveals that linear regression outperforms knn for this data
the green solid line plotted as function of represents the test set mean squared error mse for knn
the knn errors are well above the black dashed line which is the test mse for linear regression
when the value of is large then knn performs only little worse than least squares regression in terms of mse
it performs far worse when is small
in practice the true relationship between and is rarely exactly linear
figure examines the relative performances of least squares regression and knn under increasing levels of non linearity in the relationship between and
in the top row the true relationship is nearly linear
in this case we see that the test mse for linear regression is still superior to that of knn for low values of
however for knn outperforms linear regression
the second row illustrates more substantial deviation from linearity
in this situation knn substantially outperforms linear regression for all values of
note that as the extent of non linearity increases there is little change in the test set mse for the non parametric knn method but there is large increase in the test set mse of linear regression
figures and display situations in which knn performs slightly worse than linear regression when the relationship is linear but much better than linear regression for non linear situations
in real life situation in which the true relationship is unknown one might draw the conclusion that knn should be favored over linear regression because it will at worst be slightly inferior than linear regression if the true relationship is linear and may give substantially better results if the true relationship is non linear
but in reality even when the true relationship is highly non linear knn may still provide inferior results to linear regression
in particular both figures and illustrate settings with predictor
but in higher dimensions knn often performs worse than linear regression
figure considers the same strongly non linear situation as in the second row of figure except that we have added additional noise predictors that are not associated with the response
when or knn outperforms linear regression
but for the results are mixed and for linear regression is superior to knn
in fact the increase in dimension has only caused small deterioration in the linear regression test set mse but it has caused more than ten fold increase in the mse for knn
this decrease in performance as the dimension increases is common problem for knn and results from the fact that in higher dimensions there is effectively reduction in sample size
in this data set there are training observations when this provides enough information to accurately estimate
however spreading observations over dimensions results in phenomenon in which given observation has no
top left in setting with slightly non linear relationship between and solid black line the knn fits with blue and red are displayed
top right for the slightly non linear data the test set mse for least squares regression horizontal black and kn with various values of green are displayed
bottom left and bottom right as in the top panel but with strongly non linear relationship between and
test mse for linear regression black dashed lines and knn green curves as the number of variables increases
the true function is non linear in the first variable as in the lower panel in figure and does not depend on the additional variables
the performance of linear regression deteriorates slowly in the presence of these additional noise variables whereas knn's performance degrades much more quickly as increases nearby neighbors this is the so called curse of dimensionality
that is curse of the observations that are nearest to given test observation may be dimensionality very far away from in dimensional space when is large leading to very poor prediction of and hence poor knn fit
as general rule parametric methods will tend to outperform non parametric approaches when there is small number of observations per predictor
even in problems in which the dimension is small we might prefer linear regression to knn from an interpretability standpoint
if the test mse of knn is only slightly lower than that of linear regression we might be willing to forego little bit of prediction accuracy for the sake of simple model that can be described in terms of just few coefficients and for which values are available
lab linear regression libraries the library function is used to load libraries or groups of functions and library data sets that are not included in the base distribution
basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution but more exotic functions require additional libraries
here we load the mass package which is very large collection of data sets and functions
we also load the islr package which includes the functions and data sets associated with this book library mass library islr
lab linear regression if you receive an error message when loading any of these libraries it likely indicates that the corresponding library has not yet been installed on your system
some libraries such as mass come with and do not need to be separately installed on your computer
however other packages such as islr must be downloaded the first time they are used
this can be done directly from within
for example on windows system select the install package option under the packages tab
after you select any mirror site list of available packages will appear
simply select the package you wish to install and will automatically download the package
alternatively this can be done at the command line via install packages islr
this installation only needs to be done the first time you use package
however the library function must be called each time you wish to use given package
simple linear regression the mass library contains the boston data set which records medv median house value for neighborhoods around boston
we will seek to predict medv using predictors such as rm average number of rooms per house age average age of houses and lstat percent of households with low socioeconomic status fix boston names boston crim zn indus chas nox rm age dis rad tax ptratio black lstat medv to find out more about the data set we can type
we will start by using the lm function to fit simple linear regression lm model with medv as the response and lstat as the predictor
the basic syntax is lm data where is the response is the predictor and data is the data set in which these two variables are kept lm fit lm medv lstat error in eval expr envir enclos object medv not found the command causes an error because does not know where to find the variables medv and lstat
the next line tells that the variables are in boston
if we attach boston the first line works fine because now recognizes the variables lm fit lm medv lstat data boston attach boston lm fit lm medv lstat if we type lm fit some basic information about the model is output
for more detailed information we use summary lm fit
this gives us pvalues and standard errors for the coefficients as well as the statistic and statistic for the model
linear regression lm fit call lm formula medv lstat coefficients intercept lstat summary lm fit call lm formula medv lstat residuals min median max coefficients estimate std
error value pr intercept lstat signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value we can use the names function in order to find out what other pieces of names information are stored in lm fit
although we can extract these quantities by name lm fit coefficients it is safer to use the extractor functions like coef to access them coef names lm fit residuals effects rank fitted values assign qr df residual xlevels call terms model coef lm fit intercept lstat in order to obtain confidence interval for the coefficient estimates we can use the confint command confint confint lm fit intercept lstat the predict function can be used to produce confidence intervals and predict prediction intervals for the prediction of medv for given value of lstat predict lm fit data frame lstat
lab linear regression interval confidence fit lwr upr predict lm fit data frame lstat interval prediction fit lwr upr for instance the confidence interval associated with lstat value of is and the prediction interval is
as expected the confidence and prediction intervals are centered around the same point predicted value of for medv when lstat equals but the latter are substantially wider
we will now plot medv and lstat along with the least squares regression line using the plot and abline functions abline plot lstat medv abline lm fit there is some evidence for non linearity in the relationship between lstat and medv
we will explore this issue later in this lab
the abline function can be used to draw any line not just the least squares regression line
to draw line with intercept and slope we type abline
below we experiment with some additional settings for plotting lines and points
the lwd command causes the width of the regression line to be increased by factor of this works for the plot and lines functions also
we can also use the pch option to create different plotting symbols abline lm fit lwd abline lm fit lwd col red plot lstat medv col red plot lstat medv pch plot lstat medv pch plot pch next we examine some diagnostic plots several of which were discussed in section
four diagnostic plots are automatically produced by applying the plot function directly to the output from lm
in general this command will produce one plot at time and hitting enter will generate the next plot
however it is often convenient to view all four plots together
we can achieve this by using the par function which tells to split the par display screen into separate panels so that multiple plots can be viewed simultaneously
for example par mfrow divides the plotting region into grid of panels par mfrow plot lm fit
linear regression alternatively we can compute the residuals from linear regression fit using the residuals function
the function rstudent will return the residuals studentized residuals and we can use this function to plot the residuals rstudent against the fitted values plot predict lm fit residuals lm fit plot predict lm fit rstudent lm fit on the basis of the residual plots there is some evidence of non linearity
leverage statistics can be computed for any number of predictors using the hatvalues function hatvalues plot hatvalues lm fit which max hatvalues lm fit the which max function identifies the index of the largest element of which max vector
in this case it tells us which observation has the largest leverage statistic
multiple linear regression in order to fit multiple linear regression model using least squares we again use the lm function
the syntax lm is used to fit model with three predictors and
the summary function now outputs the regression coefficients for all the predictors lm fit lm medv lstat age data boston summary lm fit call lm formula medv lstat age data boston residuals min median max coefficients estimate std
error value pr intercept lstat age signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value the boston data set contains thirteen variables and so it would be cumbersome to have to type all of these in order to perform regression using all of the predictors
instead we can use the following short hand
lab linear regression lm fit lm medv data boston summary lm fit call lm formula medv data boston residuals min median max coefficients estimate std
error value pr intercept crim zn indus chas nox rm age dis rad tax ptratio black lstat signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value we can access the individual components of summary object by name type
summary lm to see what is available
hence summary lm fit sq gives us the and summary lm fit sigma gives us the rse
the vif vif function part of the car package can be used to compute variance inflation factors
most vif's are low to moderate for this data
the car package is not part of the base installation so it must be downloaded the first time you use it via the install packages option in library car vif lm fit crim zn indus chas nox rm age dis rad tax ptratio black lstat what if we would like to perform regression using all of the variables but one
for example in the above regression output age has high value
so we may wish to run regression excluding this predictor
the following syntax results in regression using all predictors except age
linear regression lm fit lm medv age data boston summary lm fit
alternatively the update function can be used update lm fit update lm fit age interaction terms it is easy to include interaction terms in linear model using the lm function
the syntax lstat black tells to include an interaction term between lstat and black
the syntax lstat age simultaneously includes lstat age and the interaction term lstat age as predictors it is shorthand for lstat age lstat age summary lm medv lstat age data boston call lm formula medv lstat age data boston residuals min median max coefficients estimate std
error value pr intercept lstat age lstat age signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value non linear transformations of the predictors the lm function can also accommodate non linear transformations of the predictors
for instance given predictor we can create predictor using
the function is needed since the has special meaning in formula wrapping as we do allows the standard usage in which is to raise to the power
we now perform regression of medv onto lstat and lstat lm fit lm medv lstat lstat summary lm fit
lab linear regression call lm formula medv lstat lstat residuals min median max coefficients estimate std
error value pr intercept lstat lstat signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value the near zero value associated with the quadratic term suggests that it leads to an improved model
we use the anova function to further anova quantify the extent to which the quadratic fit is superior to the linear fit lm fit lm medv lstat anova lm fit lm fit analysis of variance table model medv lstat model medv lstat lstat res
df rss df sum of sq pr signif codes
here model represents the linear submodel containing only one predictor lstat while model corresponds to the larger quadratic model that has two predictors lstat and lstat
the anova function performs hypothesis test comparing the two models
the null hypothesis is that the two models fit the data equally well and the alternative hypothesis is that the full model is superior
here the statistic is and the associated value is virtually zero
this provides very clear evidence that the model containing the predictors lstat and lstat is far superior to the model that only contains the predictor lstat
this is not surprising since earlier we saw evidence for non linearity in the relationship between medv and lstat
if we type par mfrow plot lm fit then we see that when the lstat term is included in the model there is little discernible pattern in the residuals
linear regression in order to create cubic fit we can include predictor of the form
however this approach can start to get cumbersome for higherorder polynomials
better approach involves using the poly function poly to create the polynomial within lm
for example the following command produces fifth order polynomial fit lm fit lm medv poly lstat summary lm fit call lm formula medv poly lstat residuals min median max coefficients estimate std
error value pr intercept poly lstat poly lstat poly lstat poly lstat poly lstat signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value this suggests that including additional polynomial terms up to fifth order leads to an improvement in the model fit
however further investigation of the data reveals that no polynomial terms beyond fifth order have significant values in regression fit
of course we are in no way restricted to using polynomial transformations of the predictors
here we try log transformation summary lm medv log rm data boston
qualitative predictors we will now examine the carseats data which is part of the islr library
we will attempt to predict sales child car seat sales in locations based on number of predictors fix carseats names carseats sales compprice income advertisin populatio price shelveloc age education urban us
lab linear regression the carseats data includes qualitative predictors such as shelveloc an indicator of the quality of the shelving location that is the space within store in which the car seat is displayed at each location
the predictor shelveloc takes on three possible values bad medium and good
given qualitative variable such as shelveloc generates dummy variables automatically
below we fit multiple regression model that includes some interaction terms lm fit lm sales
income advertisi price age data carseats summary lm fit call lm formula sales
income advertisi price age data carseats residuals min median max coefficients estimate std
error value pr intercept compprice income advertisi population price shelvelocgood shelvelocmedium age education urbanyes usyes income advertisin price age signif codes
residual standard error on degrees of freedom multiple squared adjusted squared statistic on and df value the contrasts function returns the coding that uses for the dummy contrasts variables attach carseats contrasts shelveloc good medium bad good medium use
contrasts to learn about other contrasts and how to set them
linear regression has created shelvelocgood dummy variable that takes on value of if the shelving location is good and otherwise
it has also created shelvelocmedium dummy variable that equals if the shelving location is medium and otherwise
bad shelving location corresponds to zero for each of the two dummy variables
the fact that the coefficient for shelvelocgood in the regression output is positive indicates that good shelving location is associated with high sales relative to bad location
and shelvelocmedium has smaller positive coefficient indicating that medium shelving location leads to higher sales than bad shelving location but lower sales than good shelving location
writing functions as we have seen comes with many useful functions and still more functions are available by way of libraries
however we will often be interested in performing an operation for which no function is available
in this setting we may want to write our own function
for instance below we provide simple function that reads in the islr and mass libraries called loadlibraries
before we have created the function returns an error if we try to call it
loadlibraries error object loadlibraries not found error could not find function we now create the function
note that the symbols are printed by and should not be typed in
the symbol informs that multiple commands are about to be input
hitting enter after typing will cause to print the symbol
we can then input as many commands as we wish hitting enter after each one
finally the symbol informs that no further commands will be entered
function library islr library mass print the libraries have been loaded
now if we type in loadlibraries will tell us what is in the function
loadlibraries function library islr library mass print the libraries have been loaded
if we call the function the libraries are loaded in and the print statement is output
exercises the libraries have been loaded
exercises conceptual
describe the null hypotheses to which the values given in table correspond
explain what conclusions you can draw based on these values
your explanation should be phrased in terms of sales tv radio and newspaper rather than in terms of the coefficients of the linear model
carefully explain the differences between the knn classifier and knn regression methods
suppose we have data set with five predictors gpa iq gender for female and for male interaction between gpa and iq and interaction between gpa and gender
the response is starting salary after graduation in thousands of dollars
suppose we use least squares to fit the model and get which answer is correct and why
for fixed value of iq and gpa males earn more on average than females ii
for fixed value of iq and gpa females earn more on average than males iii
for fixed value of iq and gpa males earn more on average than females provided that the gpa is high enough iv
for fixed value of iq and gpa females earn more on average than males provided that the gpa is high enough predict the salary of female with iq of and gpa of true or false since the coefficient for the gpa iq interaction term is very small there is very little evidence of an interaction effect
justify your answer
collect set of data observations containing single predictor and quantitative response
then fit linear regression model to the data as well as separate cubic regression
suppose that the true relationship between and is linear

consider the training residual sum of
linear regression squares rss for the linear regression and also the training rss for the cubic regression
would we expect one to be lower than the other would we expect them to be the same or is there not enough information to tell
justify your answer answer using test rather than training rss suppose that the true relationship between and is not linear but we don't know how far it is from linear
consider the training rss for the linear regression and also the training rss for the cubic regression
would we expect one to be lower than the other would we expect them to be the same or is there not enough information to tell
justify your answer answer using test rather than training rss
consider the fitted values that result from performing linear regression without an intercept
in this setting the ith fitted value takes the form xi where
xi yi
show that we can write ai yi what is ai
note we interpret this result by saying that the fitted values from linear regression are linear combinations of the response values
using argue that in the case of simple linear regression the least squares line always passes through the point
it is claimed in the text that in the case of simple linear regression of onto the statistic is equal to the square of the correlation between and
prove that this is the case
for simplicity you may assume that
this question involves the use of simple linear regression on the auto data set
exercises use the lm function to perform simple linear regression with mpg as the response and horsepower as the predictor
use the summary function to print the results
comment on the output
for example
is there relationship between the predictor and the response
how strong is the relationship between the predictor and the response
is the relationship between the predictor and the response positive or negative
what is the predicted mpg associated with horsepower of
what are the associated confidence and prediction intervals
plot the response and the predictor
use the abline function to display the least squares regression line use the plot function to produce diagnostic plots of the least squares regression fit
comment on any problems you see with the fit
this question involves the use of multiple linear regression on the auto data set produce scatterplot matrix which includes all of the variables in the data set compute the matrix of correlations between the variables using the function cor
you will need to exclude the name variable cor which is qualitative use the lm function to perform multiple linear regression with mpg as the response and all other variables except name as the predictors
use the summary function to print the results
comment on the output
for instance
is there relationship between the predictors and the response
which predictors appear to have statistically significant relationship to the response
what does the coefficient for the year variable suggest
use the plot function to produce diagnostic plots of the linear regression fit
comment on any problems you see with the fit
do the residual plots suggest any unusually large outliers
does the leverage plot identify any observations with unusually high leverage
linear regression use the and symbols to fit linear regression models with interaction effects
do any interactions appear to be statistically significant
different transformations of the variables such as try few log
comment on your findings
this question should be answered using the carseats data set fit multiple regression model to predict sales using population urban and us provide an interpretation of each coefficient in the model
be careful some of the variables in the model are qualitative
write out the model in equation form being careful to handle the qualitative variables properly for which of the predictors can you reject the null hypothesis
on the basis of your response to the previous question fit smaller model that only uses the predictors for which there is evidence of association with the outcome how well do the models in and fit the data
using the model from obtain confidence intervals for the coefficient is there evidence of outliers or high leverage observations in the model from
in this problem we will investigate the statistic for the null hypothesis in simple linear regression without an intercept
to begin we generate predictor and response as follows set seed rnorm rnorm perform simple linear regression of onto without an intercept
report the coefficient estimate the standard error of this coefficient estimate and the statistic and value associated with the null hypothesis
comment on these results
you can perform regression without an intercept using the command lm now perform simple linear regression of onto without an intercept and report the coefficient estimate its standard error and the corresponding statistic and values associated with the null hypothesis
comment on these results
exercises what is the relationship between the results obtained in and
for the regression of onto without an intercept the tstatistic for takes the form se where is given by and where sp xi yp se these formulas are slightly different from those given in sections and since here we are performing regression without an intercept
show algebraically and confirm numerically in that the statistic can be written as pn xi yi pn pn yi xi yi using the results from argue that the statistic for the regression of onto is the same as the statistic for the regression of onto in show that when regression is performed with an intercept the statistic for is the same for the regression of onto as it is for the regression of onto
this problem involves simple linear regression without an intercept recall that the coefficient estimate for the linear regression of onto without an intercept is given by
under what circumstance is the coefficient estimate for the regression of onto the same as the coefficient estimate for the regression of onto
generate an example in with observations in which the coefficient estimate for the regression of onto is different from the coefficient estimate for the regression of onto generate an example in with observations in which the coefficient estimate for the regression of onto is the same as the coefficient estimate for the regression of onto
in this exercise you will create some simulated data and will fit simple linear regression models to it using the rnorm function create vector containing observations drawn from distribution
this represents feature
linear regression using the rnorm function create vector eps containing observations drawn from distribution
this represents the noise vector using and eps generate vector according to the model
what is the length of the vector
what are the values of and in this linear model
create scatterplot displaying the relationship between and
comment on what you observe fit least squares linear model to predict using
comment on the model obtained
how do and compare to and
display the least squares line on the scatterplot obtained in
draw the population regression line on the plot in different color
use the legend command to create an appropriate legend now fit polynomial regression model that predicts using and
is there evidence that the quadratic term improves the model fit
explain your answer repeat after modifying the data generation process in such way that there is less noise in the data
the model should remain the same
you can do this by decreasing the variance of the normal distribution used to generate the error term in
describe your results repeat after modifying the data generation process in such way that there is more noise in the data
the model should remain the same
you can do this by increasing the variance of the normal distribution used to generate the error term in
describe your results what are the confidence intervals for and based on the original data set the noisier data set and the less noisy data set
comment on your results
this problem focuses on the collinearity problem perform the following commands in set seed runif rnorm rnorm
exercises the last line corresponds to creating linear model in which is function of and
write out the form of the linear model
what are the regression coefficients
what is the correlation between and
create scatterplot displaying the relationship between the variables using this data fit least squares regression to predict using and
describe the results obtained
what are and
how do these relate to the true and
can you reject the null hypothesis
how about the null hypothesis
now fit least squares regression to predict using only
comment on your results
can you reject the null hypothesis
now fit least squares regression to predict using only
comment on your results
can you reject the null hypothesis
do the results obtained in contradict each other
explain your answer now suppose we obtain one additional observation which was unfortunately mismeasured re fit the linear models from using this new data
what effect does this new observation have on the each of the models
in each model is this observation an outlier
high leverage point
explain your answers
this problem involves the boston data set which we saw in the lab for this chapter
we will now try to predict per capita crime rate using the other variables in this data set
in other words per capita crime rate is the response and the other variables are the predictors for each predictor fit simple linear regression model to predict the response
describe your results
in which of the models is there statistically significant association between the predictor and the response
create some plots to back up your assertions fit multiple regression model to predict the response using all of the predictors
describe your results
for which predictors can we reject the null hypothesis
how do your results from compare to your results from
create plot displaying the univariate regression coefficients
linear regression from on the axis and the multiple regression coefficients from on the axis
that is each predictor is displayed as single point in the plot
its coefficient in simple linear regression model is shown on the axis and its coefficient estimate in the multiple linear regression model is shown on the axis is there evidence of non linear association between any of the predictors and the response
to answer this question for each predictor fit model of the form
this is page printer opaque this classification the linear regression model discussed in chapter assumes that the response variable is quantitative
but in many situations the response variable is instead qualitative
for example eye color is qualitative taking qualitative on values blue brown or green
often qualitative variables are referred to as categorical we will use these terms interchangeably
in this chapter we study approaches for predicting qualitative responses process that is known as classification
predicting qualitative response for an obserclassification vation can be referred to as classifying that observation since it involves assigning the observation to category or class
on the other hand often the methods used for classification first predict the probability of each of the categories of qualitative variable as the basis for making the classification
in this sense they also behave like regression methods
there are many possible classification techniques or classifiers that one classifier might use to predict qualitative response
we touched on some of these in sections and
in this chapter we discuss three of the most widely used classifiers logistic regression linear discriminant analysis and logistic regression nearest neighbors
we discuss more computer intensive methods in later linear discriminant chapters such as generalized additive models chapter trees random analysis forests and boosting chapter and support vector machines chap nearest ter neighbors
classification an overview of classification classification problems occur often perhaps even more so than regression problems
some examples include
person arrives at the emergency room with set of symptoms that could possibly be attributed to one of three medical conditions
which of the three conditions does the individual have
an online banking service must be able to determine whether or not transaction being performed on the site is fraudulent on the basis of the user's ip address past transaction history and so forth
on the basis of dna sequence data for number of patients with and without given disease biologist would like to figure out which dna mutations are deleterious disease causing and which are not
just as in the regression setting in the classification setting we have set of training observations xn yn that we can use to build classifier
we want our classifier to perform well not only on the training data but also on test observations that were not used to train the classifier
in this chapter we will illustrate the concept of classification using the simulated default data set
we are interested in predicting whether an individual will default on his or her credit card payment on the basis of annual income and monthly credit card balance
the data set is displayed in figure
we have plotted annual income and monthly credit card balance for subset of individuals
the left hand panel of figure displays individuals who defaulted in given month in orange and those who did not in blue
the overall default rate is about so we have plotted only fraction of the individuals who did not default
it appears that individuals who defaulted tended to have higher credit card balances than those who did not
in the right panel of figure two pairs of boxplots are shown
the first shows the distribution of balance split by the binary default variable the second is similar plot for income
in this chapter we learn how to build model to predict default for any given value of balance and income
since is not quantitative the simple linear regression model of chapter is not appropriate
it is worth noting that figure displays very pronounced relationship between the predictor balance and the response default
in most real applications the relationship between the predictor and the response will not be nearly so strong
however for the sake of illustrating the classification procedures discussed in this chapter we use an example in which the relationship between the predictor and the response is somewhat exaggerated
why not linear regression
the default data set
left the annual incomes and monthly credit card balances of number of individuals
the individuals who defaulted on their credit card payments are shown in orange and those who did not are shown in blue
center boxplots of balance as function of default status
right boxplots of income as function of default status
why not linear regression
we have stated that linear regression is not appropriate in the case of qualitative response
why not
suppose that we are trying to predict the medical condition of patient in the emergency room on the basis of her symptoms
in this simplified example there are three possible diagnoses stroke drug overdose and epileptic seizure
we could consider encoding these values as quantitative response variable as follows if stroke if drug overdose if epileptic seizure
using this coding least squares could be used to fit linear regression model to predict on the basis of set of predictors xp
unfortunately this coding implies an ordering on the outcomes putting drug overdose in between stroke and epileptic seizure and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure
in practice there is no particular reason that this needs to be the case
for instance one could choose an
classification equally reasonable coding if epileptic seizure if stroke if drug overdose which would imply totally different relationship among the three conditions
each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations
if the response variable's values did take on natural ordering such as mild moderate and severe and we felt the gap between mild and moderate was similar to the gap between moderate and severe then coding would be reasonable
unfortunately in general there is no natural way to convert qualitative response variable with more than two levels into quantitative response that is ready for linear regression
for binary two level qualitative response the situation is better
for binary instance perhaps there are only two possibilities for the patient's medical condition stroke and drug overdose
we could then potentially use the dummy variable approach from section to code the response as follows if stroke if drug overdose
we could then fit linear regression to this binary response and predict drug overdose if and stroke otherwise
in the binary case it is not hard to show that even if we flip the above coding linear regression will produce the same final predictions
for binary response with coding as above regression by least squares does make sense it can be shown that the obtained using linear regression is in fact an estimate of pr drug overdose in this special case
however if we use linear regression some of our estimates might be outside the interval see figure making them hard to interpret as probabilities
nevertheless the predictions provide an ordering and can be interpreted as crude probability estimates
curiously it turns out that the classifications that we get if we use linear regression to predict binary response will be the same as for the linear discriminant analysis lda procedure we discuss in section
however the dummy variable approach cannot be easily extended to accommodate qualitative responses with more than two levels
for these reasons it is preferable to use classification method that is truly suited for qualitative response values such as the ones presented next
classification using the default data
left estimated probability of default using linear regression
some estimated probabilities are negative
the orange ticks indicate the values coded for default no or yes
right predicted probabilities of default using logistic regression
all probabilities lie between and
logistic regression consider again the default data set where the response default falls into one of two categories yes or no
rather than modeling this response directly logistic regression models the probability that belongs to particular category
for the default data logistic regression models the probability of default
for example the probability of default given balance can be written as pr default yes balance
the values of pr default yes balance which we abbreviate balance will range between and
then for any given value of balance prediction can be made for default
for example one might predict default yes for any individual for whom balance
alternatively if company wishes to be conservative in predicting individuals who are at risk for default then they may choose to use lower threshold such as balance
the logistic model how should we model the relationship between pr and
for convenience we are using the generic coding for the response
in section we talked of using linear regression model to represent these probabilities
classification if we use this approach to predict default yes using balance then we obtain the model shown in the left hand panel of figure
here we see the problem with this approach for balances close to zero we predict negative probability of default if we were to predict for very large balances we would get values bigger than
these predictions are not sensible since of course the true probability of default regardless of credit card balance must fall between and
this problem is not unique to the credit default data
any time straight line is fit to binary response that is coded as or in principle we can always predict for some values of and for others unless the range of is limited
to avoid this problem we must model using function that gives outputs between and for all values of
many functions meet this description
in logistic regression we use the logistic function logistic function
to fit the model we use method called maximum likelihood which maximum we discuss in the next section
the right hand panel of figure illustrates likelihood the fit of the logistic regression model to the default data
notice that for low balances we now predict the probability of default as close to but never below zero
likewise for high balances we predict default probability close to but never above one
the logistic function will always produce an shaped curve of this form and so regardless of the value of we will obtain sensible prediction
we also see that the logistic model is better able to capture the range of probabilities than is the linear regression model in the left hand plot
the average fitted probability in both cases is averaged over the training data which is the same as the overall proportion of defaulters in the data set
after bit of manipulation of we find that
the quantity is called the odds and can take on any value odds between and
values of the odds close to and indicate very low and very high probabilities of default respectively
for example on average in people with an odds of will default since implies an odds of
likewise on average out of every people with an odds of will default since implies an odds of
odds are traditionally used instead of probabilities in horse racing since they relate more naturally to the correct betting strategy
by taking the logarithm of both sides of we arrive at log

logistic regression the left hand side is called the log odds or logit
we see that the logistic log odds regression model has logit that is linear in logit recall from chapter that in linear regression model gives the average change in associated with one unit increase in
in contrast in logistic regression model increasing by one unit changes the log odds by or equivalently it multiplies the odds by
however because the relationship between and in is not straight line does not correspond to the change in associated with one unit increase in
the amount that changes due to one unit change in will depend on the current value of
but regardless of the value of if is positive then increasing will be associated with increasing and if is negative then increasing will be associated with decreasing
the fact that there is not straight line relationship between and and the fact that the rate of change in per unit change in depends on the current value of can also be seen by inspection of the right hand panel of figure
estimating the regression coefficients the coefficients and in are unknown and must be estimated based on the available training data
in chapter we used the least squares approach to estimate the unknown linear regression coefficients
although we could use non linear least squares to fit the model the more general method of maximum likelihood is preferred since it has better statistical properties
the basic intuition behind using maximum likelihood to fit logistic regression model is as follows we seek estimates for and such that the predicted probability xi of default for each individual using corresponds as closely as possible to the individual's observed default status
in other words we try to find and such that plugging these estimates into the model for given in yields number close to one for all individuals who defaulted and number close to zero for all individuals who did not
this intuition can be formalized using mathematical equation called likelihood function likelihood function xi xi
yi yi the estimates and are chosen to maximize this likelihood function
maximum likelihood is very general approach that is used to fit many of the non linear models that we examine throughout this book
in the linear regression setting the least squares approach is in fact special case of maximum likelihood
the mathematical details of maximum likelihood are beyond the scope of this book
however in general logistic regression and other models can be easily fit using statistical software package such
classification coefficient std
error statistic value intercept balance table
for the default data estimated coefficients of the logistic regression model that predicts the probability of default using balance
one unit increase in balance is associated with an increase in the log odds of default by units as and so we do not need to concern ourselves with the details of the maximum likelihood fitting procedure
table shows the coefficient estimates and related information that result from fitting logistic regression model on the default data in order to predict the probability of default yes using balance
we see that this indicates that an increase in balance is associated with an increase in the probability of default
to be precise one unit increase in balance is associated with an increase in the log odds of default by units
many aspects of the logistic regression output shown in table are similar to the linear regression output of chapter
for example we can measure the accuracy of the coefficient estimates by computing their standard errors
the statistic in table plays the same role as the statistic in the linear regression output for example in table on page
for instance the statistic associated with is equal to se and so large absolute value of the statistic indicates evidence against the null hypothesis
this null hypothesis implies that in other words that the probability of default does not depend on balance
since the value associated with balance in table is tiny we can reject
in other words we conclude that there is indeed an association between balance and probability of default
the estimated intercept in table is typically not of interest its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data
making predictions once the coefficients have been estimated it is simple matter to compute the probability of default for any given credit card balance
for example using the coefficient estimates given in table we predict that the default probability for an individual with balance of is which is below
in contrast the predicted probability of default for an individual with balance of is much higher and equals or
logistic regression coefficient std
error statistic value intercept student yes table
for the default data estimated coefficients of the logistic regression model that predicts the probability of default using student status
student status is encoded as dummy variable with value of for student and value of for non student and represented by the variable student yes in the table
one can use qualitative predictors with the logistic regression model using the dummy variable approach from section
as an example the default data set contains the qualitative variable student
to fit the model we simply create dummy variable that takes on value of for students and for non students
the logistic regression model that results from predicting probability of default from student status can be seen in table
the coefficient associated with the dummy variable is positive and the associated value is statistically significant
this indicates that students tend to have higher default probabilities than non students default yes student yes pr default yes student no pr multiple logistic regression we now consider the problem of predicting binary response using multiple predictors
by analogy with the extension from simple to multiple linear regression in chapter we can generalize as follows log where xp are predictors
equation can be rewritten as xp
xp just as in section we use the maximum likelihood method to estimate
table shows the coefficient estimates for logistic regression model that uses balance income in thousands of dollars and student status to predict probability of default
there is surprising result here
the pvalues associated with balance and the dummy variable for student status are very small indicating that each of these variables is associated with
classification coefficient std
error statistic value intercept balance income student yes table
for the default data estimated coefficients of the logistic regression model that predicts the probability of default using balance income and student status
student status is encoded as dummy variable student yes with value of for student and value of for non student
in fitting this model income was measured in thousands of dollars the probability of default
however the coefficient for the dummy variable is negative indicating that students are less likely to default than nonstudents
in contrast the coefficient for the dummy variable is positive in table
how is it possible for student status to be associated with an increase in probability of default in table and decrease in probability of default in table
the left hand panel of figure provides graphical illustration of this apparent paradox
the orange and blue solid lines show the average default rates for students and non students respectively as function of credit card balance
the negative coefficient for student in the multiple logistic regression indicates that for fixed value of balance and income student is less likely to default than non student
indeed we observe from the left hand panel of figure that the student default rate is at or below that of the non student default rate for every value of balance
but the horizontal broken lines near the base of the plot which show the default rates for students and non students averaged over all values of balance and income suggest the opposite effect the overall student default rate is higher than the non student default rate
consequently there is positive coefficient for student in the single variable logistic regression output shown in table
the right hand panel of figure provides an explanation for this discrepancy
the variables student and balance are correlated
students tend to hold higher levels of debt which is in turn associated with higher probability of default
in other words students are more likely to have large credit card balances which as we know from the left hand panel of figure tend to be associated with high default rates
thus even though an individual student with given credit card balance will tend to have lower probability of default than non student with the same credit card balance the fact that students on the whole tend to have higher credit card balances means that overall students tend to default at higher rate than non students
this is an important distinction for credit card company that is trying to determine to whom they should offer credit
student is riskier than non student if no information about the student's credit card
confounding in the default data
left default rates are shown for students orange and non students blue
the solid lines display default rate as function of balance while the horizontal broken lines display the overall default rates
right boxplots of balance for students orange and non students blue are shown balance is available
however that student is less risky than non student with the same credit card balance
this simple example illustrates the dangers and subtleties associated with performing regressions involving only single predictor when other predictors may also be relevant
as in the linear regression setting the results obtained using one predictor may be quite different from those obtained using multiple predictors especially when there is correlation among the predictors
in general the phenomenon seen in figure is known as confounding confounding by substituting estimates for the regression coefficients from table into we can make predictions
for example student with credit card balance of and an income of has an estimated probability of default of
non student with the same balance and income has an estimated probability of default of
here we multiply the income coefficient estimate from table by rather than by because in that table the model was fit with income measured in units of
classification logistic regression for response classes we sometimes wish to classify response variable that has more than two classes
for example in section we had three categories of medical condition in the emergency room stroke drug overdose epileptic seizure
in this setting we wish to model both pr stroke and pr drug overdose with the remaining pr epileptic seizure pr stroke pr drug overdose
the two class logistic regression models discussed in the previous sections have multiple class extensions but in practice they tend not to be used all that often
one of the reasons is that the method we discuss in the next section discriminant analysis is popular for multiple class classification
so we do not go into the details of multiple class logistic regression here but simply note that such an approach is possible and that software for it is available in
linear discriminant analysis logistic regression involves directly modeling pr using the logistic function given by for the case of two response classes
in statistical jargon we model the conditional distribution of the response given the predictor
we now consider an alternative and less direct approach to estimating these probabilities
in this alternative approach we model the distribution of the predictors separately in each of the response classes given and then use bayes theorem to flip these around into estimates for pr
when these distributions are assumed to be normal it turns out that the model is very similar in form to logistic regression
why do we need another method when we have logistic regression
linear discriminant analysis does not suffer from this problem
using bayes theorem for classification suppose that we wish to classify an observation into one of classes where
in other words the qualitative response variable can take on
linear discriminant analysis possible distinct and unordered values
let represent the overall or prior prior probability that randomly chosen observation comes from the kth class this is the probability that given observation is associated with the kth category of the response variable
let fk pr denote the density function of for an observation that comes from the kth class density function in other words fk is relatively large if there is high probability that an observation in the kth class has and fk is small if it is very unlikely that an observation in the kth class has
then bayes theorem states that bayes theorem fk pr pk
fl in accordance with our earlier notation we will use the abbreviation pk pr
this suggests that instead of directly computing pk as in section we can simply plug in estimates of and fk into
in general estimating is easy if we have random sample of from the population we simply compute the fraction of the training observations that belong to the kth class
however estimating fk tends to be more challenging unless we assume some simple forms for these densities
we refer to pk as the posterior probability that an observation posterior belongs to the kth class
that is it is the probability that the observation belongs to the kth class given the predictor value for that observation
we know from chapter that the bayes classifier which classifies an observation to the class for which pk is largest has the lowest possible error rate out of all classifiers
this is of course only true if the terms in are all correctly specified
therefore if we can find way to estimate fk then we can develop classifier that approximates the bayes classifier
such an approach is the topic of the following sections
linear discriminant analysis for for now assume that that is we have only one predictor
we would like to obtain an estimate for fk that we can plug into in order to estimate pk
we will then classify an observation to the class for which pk is greatest
in order to estimate fk we will first make some assumptions about its form
suppose we assume that fk is normal or gaussian
in the one dimensional normal setting the normal density takes the form gaussian fk exp k where k and are the mean and variance parameters for the kth class
for now let us further assume that
that is there is shared
classification variance term across all classes which for simplicity we can denote by
plugging into we find that exp k pk pk
exp l note that in denotes the prior probability that an observation belongs to the kth class not to be confused with the mathematical constant
the bayes classifier involves assigning an observation to the class for which is largest
taking the log of and rearranging the terms it is not hard to show that this is equivalent to assigning the observation to the class for which k log is largest
for instance if and then the bayes classifier assigns an observation to class if and to class otherwise
in this case the bayes decision boundary corresponds to the point where
an example is shown in the left hand panel of figure
the two normal density functions that are displayed and represent two distinct classes
the mean and variance parameters for the two density functions are and
the two densities overlap and so given that there is some uncertainty about the class to which the observation belongs
if we assume that an observation is equally likely to come from either class that is then by inspection of we see that the bayes classifier assigns the observation to class if and class otherwise
note that in this case we can compute the bayes classifier because we know that is drawn from gaussian distribution within each class and we know all of the parameters involved
in real life situation we are not able to calculate the bayes classifier
in practice even if we are quite certain of our assumption that is drawn from gaussian distribution within each class we still have to estimate the parameters k and
the linear discriminant analysis lda method approximates the bayes classifier by plugging estilinear discriminant mates for k and into
in particular the following estimates analysis are used xi nk yi xi yi
left two one dimensional normal density functions are shown
the dashed vertical line represents the bayes decision boundary
right observations were drawn from each of the two classes and are shown as histograms
the bayes decision boundary is again shown as dashed vertical line
the solid vertical line represents the lda decision boundary estimated from the training data where is the total number of training observations and nk is the number of training observations in the kth class
the estimate for k is simply the average of all the training observations from the kth class while can be seen as weighted average of the sample variances for each of the classes
sometimes we have knowledge of the class membership probabilities which can be used directly
in the absence of any additional information lda estimates using the proportion of the training observations that belong to the kth class
in other words nk
the lda classifier plugs the estimates given in and into and assigns an observation to the class for which log is largest
the word linear in the classifier's name stems from the fact that the discriminant functions in are linear functions of as discriminant opposed to more complex function of function the right hand panel of figure displays histogram of random sample of observations from each class
to implement lda we began by estimating k and using and
we then computed the decision boundary shown as black solid line that results from assigning an observation to the class for which is largest
all points to the left of this line will be assigned to the green class while points to the right of this line are assigned to the purple class
in this case since
two multivariate gaussian density functions are shown with
left the two predictors are uncorrelated
right the two variables have correlation of we have
as result the decision boundary corresponds to the midpoint between the sample means for the two classes
the figure indicates that the lda decision boundary is slightly to the left of the optimal bayes decision boundary which instead equals
how well does the lda classifier perform on this data
since this is simulated data we can generate large number of test observations in order to compute the bayes error rate and the lda test error rate
these are and respectively
in other words the lda classifier's error rate is only above the smallest possible error rate
this indicates that lda is performing pretty well on this data set
to reiterate the lda classifier results from assuming that the observations within each class come from normal distribution with class specific mean vector and common variance and plugging estimates for these parameters into the bayes classifier
in section we will consider less stringent set of assumptions by allowing the observations in the kth class to have class specific variance
linear discriminant analysis for we now extend the lda classifier to the case of multiple predictors
to do this we will assume that xp is drawn from multivariate gaussian or multivariate normal distribution with class specific multivariate mean vector and common covariance matrix
we begin with brief review gaussian of such distribution
linear discriminant analysis the multivariate gaussian distribution assumes that each individual predictor follows one dimensional normal distribution as in with some correlation between each pair of predictors
two examples of multivariate gaussian distributions with are shown in figure
the height of the surface at any particular point represents the probability that both and fall in small region around that point
in either panel if the surface is cut along the axis or along the axis the resulting cross section will have the shape of one dimensional normal distribution
the left hand panel of figure illustrates an example in which var var and cor this surface has characteristic bell shape
however the bell shape will be distorted if the predictors are correlated or have unequal variances as is illustrated in the right hand panel of figure
in this situation the base of the bell will have an elliptical rather than circular shape
to indicate that dimensional random variable has multivariate gaussian distribution we write
here is the mean of vector with components and cov is the covariance matrix of
formally the multivariate gaussian density is defined as exp
in the case of predictors the lda classifier assumes that the observations in the kth class are drawn from multivariate gaussian distribution k where k is class specific mean vector and is covariance matrix that is common to all classes
plugging the density function for the kth class fk into and performing little bit of algebra reveals that the bayes classifier assigns an observation to the class for which xt k tk k log is largest
this is the vector matrix version of
an example is shown in the left hand panel of figure
three equallysized gaussian classes are shown with class specific mean vectors and common covariance matrix
the three ellipses represent regions that contain of the probability for each of the three classes
the dashed lines are the bayes decision boundaries
in other words they represent the set of values for which xt k tk k xt l tl l for
the log term from has disappeared because each of the three classes has the same number of training observations
is the same for each class
note that there are three lines representing the
an example with three classes
the observations from each class are drawn from multivariate gaussian distribution with with class specific mean vector and common covariance matrix
left ellipses that contain of the probability for each of the three classes are shown
the dashed lines are the bayes decision boundaries
right observations were generated from each class and the corresponding lda decision boundaries are indicated using solid black lines
the bayes decision boundaries are once again shown as dashed lines
bayes decision boundaries because there are three pairs of classes among the three classes
that is one bayes decision boundary separates class from class one separates class from class and one separates class from class
these three bayes decision boundaries divide the predictor space into three regions
the bayes classifier will classify an observation according to the region in which it is located
once again we need to estimate the unknown parameters k and the formulas are similar to those used in the onedimensional case given in
to assign new observation lda plugs these estimates into and classifies to the class for which is largest
note that in is linear function of that is the lda decision rule depends on only through linear combination of its elements
once again this is the reason for the word linear in lda
in the right hand panel of figure observations drawn from each of the three classes are displayed and the resulting lda decision boundaries are shown as solid black lines
overall the lda decision boundaries are pretty close the the bayes decision boundaries shown again as dashed lines
the test error rates for the bayes and lda classifiers are and respectively
this indicates that lda is performing well on this data
we can perform lda on the default data in order to predict whether or not an individual will default on the basis of credit card balance and student status
the lda model fit to the training samples results
linear discriminant analysis in training error rate of
this sounds like low error rate but two caveats must be noted
in other words we might expect this classifier to perform worse if we use it to predict whether or not new set of individuals will default
the reason is that we specifically adjust the parameters of our model to do well on the training data
the higher the ratio of parameters to number of samples the more we expect this overfitting to play role
for overfitting these data we don't expect this to be problem since and
in other words the trivial null classifier will achieve an error rate that null is only bit higher than the lda training set error rate
in practice binary classifier such as this one can make two types of errors it can incorrectly assign an individual who defaults to the no default category or it can incorrectly assign an individual who does not default to the default category
it is often of interest to determine which of these two types of errors are being made
confusion matrix shown for the default confusion matrix data in table is convenient way to display this information
the table reveals that lda predicted that total of people would default
of these people actually defaulted and did not
hence only out of of the individuals who did not default were incorrectly labeled
this looks like pretty low error rate
however of the individuals who defaulted or were missed by lda
so while the overall error rate is low the error rate among individuals who defaulted is very high
from the perspective of credit card company that is trying to identify high risk individuals an error rate of among individuals who default may well be unacceptable
class specific performance is also important in medicine and biology where the terms sensitivity and specificity characterize the performance of sensitivity classifier or screening test
in this case the sensitivity is the percentage of specificity true defaulters that are identified low in this case
the specificity is the percentage of non defaulters that are correctly identified here
why does lda do such poor job of classifying the customers who default
in other words why does it have such low sensitivity
as we have seen lda is trying to approximate the bayes classifier which has the lowest total error rate out of all classifiers if the gaussian model is correct
that is the bayes classifier will yield the smallest possible total number
classification true default status no yes total predicted no default status yes total table
confusion matrix compares the lda predictions to the true default statuses for the training observations in the default data set
elements on the diagonal of the matrix represent individuals whose default statuses were correctly predicted while off diagonal elements represent individuals that were misclassified
lda made incorrect predictions for individuals who did not default and for individuals who did default of misclassified observations irrespective of which class the errors come from
that is some misclassifications will result from incorrectly assigning customer who does not default to the default class and others will result from incorrectly assigning customer who defaults to the non default class
in contrast credit card company might particularly wish to avoid incorrectly classifying an individual who will default whereas incorrectly classifying an individual who will not default though still to be avoided is less problematic
we will now see that it is possible to modify lda in order to develop classifier that better meets the credit card company's needs
the bayes classifier works by assigning an observation to the class for which the posterior probability pk is greatest
in the two class case this amounts to assigning an observation to the default class if pr default yes
thus the bayes classifier and by extension lda uses threshold of for the posterior probability of default in order to assign an observation to the default class
however if we are concerned about incorrectly predicting the default status for individuals who default then we can consider lowering this threshold
for instance we might label any customer with posterior probability of default above to the default class
in other words instead of assigning an observation to the default class if holds we could instead assign an observation to this class if default yes
the error rates that result from taking this approach are shown in table
now lda predicts that individuals will default
of the individuals who default lda correctly predicts all but or
this is vast improvement over the error rate of that resulted from using the threshold of
however this improvement comes at cost now individuals who do not default are incorrectly classified
as result the
linear discriminant analysis true default status no yes total predicted no default status yes total table
confusion matrix compares the lda predictions to the true default statuses for the training observations in the default data set using modified threshold value that predicts default for any individuals whose posterior default probability exceeds
for the default data set error rates are shown as function of the threshold value for the posterior probability that is used to perform the assignment
the black solid line displays the overall error rate
the blue dashed line represents the fraction of defaulting customers that are incorrectly classified and the orange dotted line indicates the fraction of errors among the non defaulting customers overall error rate has increased slightly to
but credit card company may consider this slight increase in the total error rate to be small price to pay for more accurate identification of individuals who do indeed default
figure illustrates the trade off that results from modifying the threshold value for the posterior probability of default
various error rates are shown as function of the threshold value
using threshold of as in minimizes the overall error rate shown as black solid line
this is to be expected since the bayes classifier uses threshold of and is known to have the lowest overall error rate
but when threshold of is used the error rate among the individuals who default is quite high blue dashed line
as the threshold is reduced the error rate among individuals who default decreases steadily but the error rate among the individuals who do not default increases
how can we decide which threshold value is
classification best
such decision must be based on domain knowledge such as detailed information about the costs associated with default
the roc curve is popular graphic for simultaneously displaying the roc curve two types of errors for all possible thresholds
the name roc is historic and comes from communications theory
it is an acronym for receiver operating characteristics
roc curve for the lda classifier on the default data
it traces out two types of error as we vary the threshold value for the posterior probability of default
the actual thresholds are not shown
the true positive rate is the sensitivity the fraction of defaulters that are correctly identified using given threshold value
the false positive rate is specificity the fraction of non defaulters that we classify incorrectly as defaulters using that same threshold value
the ideal roc curve hugs the top left corner indicating high true positive rate and low false positive rate
the dotted line represents the no information classifier this is what we would expect if student status and credit card balance are not associated with probability of default classifier on the training data
the overall performance of classifier summarized over all possible thresholds is given by the area under the roc curve auc
an ideal roc curve will hug the top left corner so the larger area under the the auc the better the classifier
for this data the auc is which is roc curve close to the maximum of one so would be considered very good
we expect classifier that performs no better than chance to have an auc of when evaluated on an independent test set not used in model training
roc curves are useful for comparing different classifiers since they take
linear discriminant analysis predicted class or null or non null total true or null true neg
tn false pos
fp class or non null false neg
fn true pos
tp total table
possible results when applying classifier or diagnostic test to population
name definition synonyms false pos
rate fp type error specificity true pos
rate tp type ii error power sensitivity recall pos
value tp precision false discovery proportion neg
value tn table
important measures for classification and diagnostic testing derived from quantities in table into account all possible thresholds
it turns out that the roc curve for the logistic regression model of section fit to these data is virtually indistinguishable from this one for the lda model so we do not display it here
as we have seen above varying the classifier threshold changes its true positive and false positive rate
these are also called the sensitivity and one sensitivity minus the specificity of our classifier
since there is an almost bewildering specificity array of terms used in this context we now give summary
table shows the possible results when applying classifier or diagnostic test to population
to make the connection with the epidemiology literature we think of as the disease that we are trying to detect and as the non disease state
to make the connection to the classical hypothesis testing literature we think of as the null hypothesis and as the alternative non null hypothesis
in the context of the default data indicates an individual who defaults and indicates one who does not
table lists many of the popular performance measures that are used in this context
the denominators for the false positive and true positive rates are the actual population counts in each class
in contrast the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class
quadratic discriminant analysis as we have discussed lda assumes that the observations within each class are drawn from multivariate gaussian distribution with classspecific mean vector and covariance matrix that is common to all classes
quadratic discriminant analysis qda provides an alternative quadratic discriminant analysis
classification approach
like lda the qda classifier results from assuming that the observations from each class are drawn from gaussian distribution and plugging estimates for the parameters into bayes theorem in order to perform prediction
however unlike lda qda assumes that each class has its own covariance matrix
that is it assumes that an observation from the kth class is of the form k where is covariance matrix for the kth class
under this assumption the bayes classifier assigns an observation to the class for which k k log xt k k k log is largest
so the qda classifier involves plugging estimates for k and into and then assigning an observation to the class for which this quantity is largest
unlike in the quantity appears as quadratic function in
this is where qda gets its name
why does it matter whether or not we assume that the classes share common covariance matrix
in other words why would one prefer lda to qda or vice versa
the answer lies in the bias variance trade off
when there are predictors then estimating covariance matrix requires estimating parameters
qda estimates separate covariance matrix for each class for total of kp parameters
with predictors this is some multiple of which is lot of parameters
by instead assuming that the classes share common covariance matrix the lda model becomes linear in which means there are kp linear coefficients to estimate
consequently lda is much less flexible classifier than qda and so has substantially lower variance
this can potentially lead to improved prediction performance
but there is trade off if lda's assumption that the classes share common covariance matrix is badly off then lda can suffer from high bias
roughly speaking lda tends to be better bet than qda if there are relatively few training observations and so reducing variance is crucial
in contrast qda is recommended if the training set is very large so that the variance of the classifier is not major concern or if the assumption of common covariance matrix for the classes is clearly untenable
figure illustrates the performances of lda and qda in two scenarios
in the left hand panel the two gaussian classes have common correlation of between and
as result the bayes decision boundary is linear and is accurately approximated by the lda decision boundary
the qda decision boundary is inferior because it suffers from higher variance without corresponding decrease in bias
in contrast the right hand panel displays situation in which the orange class has correlation of between the variables and the blue class has correlation of
left the bayes purple dashed lda black dotted and qda green solid decision boundaries for two class problem with
the shading indicates the qda decision rule
since the bayes decision boundary is linear it is more accurately approximated by lda than by qda
right details are as given in the left hand panel except that
since the bayes decision boundary is non linear it is more accurately approximated by qda than by lda the bayes decision boundary is quadratic and so qda more accurately approximates this boundary than does lda
comparison of classification methods in this chapter we have considered three different classification approaches logistic regression lda and qda
in chapter we also discussed the knearest neighbors knn method
we now consider the types of scenarios in which one approach might dominate the others
though their motivations differ the logistic regression and lda methods are closely connected
consider the two class setting with predictor and let and be the probabilities that the observation belongs to class and class respectively
in the lda framework we can see from and and bit of simple algebra that the log odds is given by log log where and are functions of and
from we know that in logistic regression log

classification both and are linear functions of
hence both logistic regression and lda produce linear decision boundaries
the only difference between the two approaches lies in the fact that and are estimated using maximum likelihood whereas and are computed using the estimated mean and variance from normal distribution
this same connection between lda and logistic regression also holds for multidimensional data with
since logistic regression and lda differ only in their fitting procedures one might expect the two approaches to give similar results
this is often but not always the case
lda assumes that the observations are drawn from gaussian distribution with common covariance matrix in each class and so can provide some improvements over logistic regression when this assumption approximately holds
conversely logistic regression can outperform lda if these gaussian assumptions are not met
recall from chapter that knn takes completely different approach from the classifiers seen in this chapter
in order to make prediction for an observation the training observations that are closest to are identified
then is assigned to the class to which the plurality of these observations belong
hence knn is completely non parametric approach no assumptions are made about the shape of the decision boundary
therefore we can expect this approach to dominate lda and logistic regression when the decision boundary is highly non linear
on the other hand knn does not tell us which predictors are important we don't get table of coefficients as in table
finally qda serves as compromise between the non parametric knn method and the linear lda and logistic regression approaches
since qda assumes quadratic decision boundary it can accurately model wider range of problems than can the linear methods
though not as flexible as knn qda can perform better in the presence of limited number of training observations because it does make some assumptions about the form of the decision boundary
to illustrate the performances of these four classification approaches we generated data from six different scenarios
in three of the scenarios the bayes decision boundary is linear and in the remaining scenarios it is non linear
for each scenario we produced random training data sets
on each of these training sets we fit each method to the data and computed the resulting test error rate on large test set
results for the linear scenarios are shown in figure and the results for the non linear scenarios are in figure
the knn method requires selection of the number of neighbors
we performed knn with two values of and value of that was chosen automatically using an approach called cross validation which we discuss further in chapter
in each of the six scenarios there were predictors
the scenarios were as follows
boxplots of the test error rates for each of the linear scenarios described in the main text
scenario there were training observations in each of two classes
the observations within each class were uncorrelated random normal variables with different mean in each class
the left hand panel of figure shows that lda performed well in this setting as one would expect since this is the model assumed by lda
knn performed poorly because it paid price in terms of variance that was not offset by reduction in bias
qda also performed worse than lda since it fit more flexible classifier than necessary
since logistic regression assumes linear decision boundary its results were only slightly inferior to those of lda
scenario details are as in scenario except that within each class the two predictors had correlation of
the center panel of figure indicates little change in the relative performances of the methods as compared to the previous scenario
scenario we generated and from the distribution with distribution observations per class
the distribution has similar shape to the normal distribution but it has tendency to yield more extreme points that is more points that are far from the mean
in this setting the decision boundary was still linear and so fit into the logistic regression framework
the set up violated the assumptions of lda since the observations were not drawn from normal distribution
the right hand panel of figure shows that logistic regression outperformed lda though both methods were superior to the other approaches
in particular the qda results deteriorated considerably as consequence of non normality
scenario the data were generated from normal distribution with correlation of between the predictors in the first class and correlation of between the predictors in the second class
boxplots of the test error rates for each of the non linear scenarios described in the main text
this setup corresponded to the qda assumption and resulted in quadratic decision boundaries
the left hand panel of figure shows that qda outperformed all of the other approaches
scenario within each class the observations were generated from normal distribution with uncorrelated predictors
however the responses were sampled from the logistic function using and as predictors
consequently there is quadratic decision boundary
the center panel of figure indicates that qda once again performed best followed closely by knn cv
the linear methods had poor performance
scenario details are as in the previous scenario but the responses were sampled from more complicated non linear function
as result even the quadratic decision boundaries of qda could not adequately model the data
the right hand panel of figure shows that qda gave slightly better results than the linear methods while the much more flexible knn cv method gave the best results
but knn with gave the worst results out of all methods
this highlights the fact that even when the data exhibits complex nonlinear relationship non parametric method such as knn can still give poor results if the level of smoothness is not chosen correctly
these six examples illustrate that no one method will dominate the others in every situation
when the true decision boundaries are linear then the lda and logistic regression approaches will tend to perform well
when the boundaries are moderately non linear qda may give better results
finally for much more complicated decision boundaries non parametric approach such as knn can be superior
but the level of smoothness for non parametric approach must be chosen carefully
in the next chapter we
lab logistic regression lda qda and knn examine number of approaches for choosing the correct level of smoothness and in general for selecting the best overall method
finally recall from chapter that in the regression setting we can accommodate non linear relationship between the predictors and the response by performing regression using transformations of the predictors
similar approach could be taken in the classification setting
for instance we could create more flexible version of logistic regression by including and even as predictors
this may or may not improve logistic regression's performance depending on whether the increase in variance due to the added flexibility is offset by sufficiently large reduction in bias
we could do the same for lda
if we added all possible quadratic terms and cross products to lda the form of the model would be the same as the qda model although the parameter estimates would be different
this device allows us to move somewhere between an lda and qda model
lab logistic regression lda qda and knn the stock market data we will begin by examining some numerical and graphical summaries of the smarket data which is part of the islr library
this data set consists of percentage returns for the stock index over days from the beginning of until the end of
for each date we have recorded the percentage returns for each of the five previous trading days lag through lag
we have also recorded volume the number of shares traded on the previous day in billions today the percentage return on the date in question and direction whether the market was up or down on this date library islr names smarket year lag lag lag lag lag volume today direction dim smarket summary smarket year lag lag min min min st qu st qu st qu median median median mean mean mean rd qu rd qu rd qu max max max lag lag lag min min min st qu st qu st qu
classification median median median mean mean mean rd qu rd qu rd qu max max max volume today direction min min down st qu st qu up median median mean mean rd qu rd qu max max pairs smarket the cor function produces matrix that contains all of the pairwise correlations among the predictors in data set
the first command below gives an error message because the direction variable is qualitative cor smarket error in cor smarket must be numeric cor smarket year lag lag lag lag lag year lag lag lag lag lag volume today volume today year lag lag lag lag lag volume today as one would expect the correlations between the lag variables and today's returns are close to zero
in other words there appears to be little correlation between today's returns and previous days returns
the only substantial correlation is between year and volume
by plotting the data we see that volume is increasing over time
in other words the average number of shares traded daily increased from to attach smarket plot volume logistic regression next we will fit logistic regression model in order to predict direction using lag through lag and volume
the glm function fits generalized glm
lab logistic regression lda qda and knn linear models class of models that includes logistic regression
the syntax generalized linear of the glm function is similar to that of lm except that we must pass in model the argument family binomial in order to tell to run logistic regression rather than some other type of generalized linear model glm fit glm direction lag lag lag lag lag volume data smarket family binomial summary glm fit call glm formula direction lag lag lag lag lag volume family binomial data smarket deviance residuals min median max coefficients estimate std
error value pr intercept lag lag lag lag lag volume dispersio parameter for binomial family taken to be null deviance on degrees of freedom residual deviance on degrees of freedom aic number of fisher scoring iterations the smallest value here is associated with lag
the negative coefficient for this predictor suggests that if the market had positive return yesterday then it is less likely to go up today
however at value of the value is still relatively large and so there is no clear evidence of real association between lag and direction
we use the coef function in order to access just the coefficients for this fitted model
we can also use the summary function to access particular aspects of the fitted model such as the values for the coefficients coef glm fit intercept lag lag lag lag lag volume summary glm fit coef estimate std
error value pr intercept lag
classification lag lag lag lag volume summary glm fit coef intercept lag lag lag lag lag volume the predict function can be used to predict the probability that the market will go up given values of the predictors
the type response option tells to output probabilities of the form as opposed to other information such as the logit
if no data set is supplied to the predict function then the probabilities are computed for the training data that was used to fit the logistic regression model
here we have printed only the first probabilities
we know that these values correspond to the probability of the market going up rather than down because the contrasts function indicates that has created dummy variable with for up glm probs predict glm fit type response glm probs contrasts direction up down up in order to make prediction as to whether the market will go up or down on particular day we must convert these predicted probabilities into class labels up or down
the following two commands create vector of class predictions based on whether the predicted probability of market increase is greater than or less than glm pred rep down glm pred glm probs up the first command creates vector of down elements
the second line transforms to up all of the elements for which the predicted probability of market increase exceeds
given these predictions the table function table can be used to produce confusion matrix in order to determine how many observations were correctly or incorrectly classified table glm pred direction direction glm pred down up down up
lab logistic regression lda qda and knn mean glm pred direction the diagonal elements of the confusion matrix indicate correct predictions while the off diagonals represent incorrect predictions
hence our model correctly predicted that the market would go up on days and that it would go down on days for total of correct predictions
the mean function can be used to compute the fraction of days for which the prediction was correct
in this case logistic regression correctly predicted the movement of the market of the time
at first glance it appears that the logistic regression model is working little better than random guessing
however this result is misleading because we trained and tested the model on the same set of observations
in other words is the training error rate
as we have seen previously the training error rate is often overly optimistic it tends to underestimate the test error rate
in order to better assess the accuracy of the logistic regression model in this setting we can fit the model using part of the data and then examine how well it predicts the held out data
this will yield more realistic error rate in the sense that in practice we will be interested in our model's performance not on the data that we used to fit the model but rather on days in the future for which the market's movements are unknown
to implement this strategy we will first create vector corresponding to the observations from through
we will then use this vector to create held out data set of observations from train year smarket smarket
train dim smarket direction direction
train the object train is vector of elements corresponding to the observations in our data set
the elements of the vector that correspond to observations that occurred before are set to true whereas those that correspond to observations in are set to false
the object train is boolean vector since its elements are true and false
boolean vectors boolean can be used to obtain subset of the rows or columns of matrix
for instance the command smarket train would pick out submatrix of the stock market data set corresponding only to the dates before since those are the ones for which the elements of train are true
symbol can be used to reverse all of the elements of boolean vector
that is
train is vector similar to train except that the elements that are true in train get swapped to false in
train and the elements that are false in train get swapped to true in
therefore smarket
train yields submatrix of the stock market data containing only the observations for
classification which train is false that is the observations with dates in
the output above indicates that there are such observations
we now fit logistic regression model using only the subset of the observations that correspond to dates before using the subset argument
we then obtain predicted probabilities of the stock market going up for each of the days in our test set that is for the days in glm fit glm direction lag lag lag lag lag volume data smarket family binomial subset train glm probs predict glm fit smarket type response notice that we have trained and tested our model on two completely separate data sets training was performed using only the dates before and testing was performed using only the dates in
finally we compute the predictions for and compare them to the actual movements of the market over that time period glm pred rep down glm pred glm probs up table glm pred direction direction glm pred down up down up mean glm pred direction mean glm pred
direction the
notation means not equal to and so the last command computes the test set error rate
the results are rather disappointing the test error rate is which is worse than random guessing
of course this result is not all that surprising given that one would not generally expect to be able to use previous days returns to predict future market performance
after all if it were possible to do so then the authors of this book would be out striking it rich rather than writing statistics textbook
we recall that the logistic regression model had very underwhelming pvalues associated with all of the predictors and that the smallest value though not very small corresponded to lag
perhaps by removing the variables that appear not to be helpful in predicting direction we can obtain more effective model
after all using predictors that have no relationship with the response tends to cause deterioration in the test error rate since such predictors cause in increase in variance without corresponding decrease in bias and so removing such predictors may in turn yield an improvement
below we have refit the logistic regression using just lag and lag which seemed to have the highest predictive power in the original logistic regression model glm fit glm direction lag lag data smarket family binomial subset train
lab logistic regression lda qda and knn glm probs predict glm fit smarket type response glm pred rep down glm pred glm probs up table glm pred direction direction glm pred down up down up mean glm pred direction now the results appear to be more promising of the daily movements have been correctly predicted
the confusion matrix suggests that on days when logistic regression predicts that the market will decline it is only correct of the time
however on days when it predicts an increase in the market it has accuracy rate
suppose that we want to predict the returns associated with particular values of lag and lag
in particular we want to predict direction on day when lag and lag equal and respectively and on day when they equal and
we do this using the predict function predict glm fit newdata data frame lag lag type response linear discriminant analysis now we will perform lda on the smarket data
in we fit lda model using the lda function which is part of the mass library
notice that the lda syntax for the lda function is identical to that of lm and to that of glm except for the absence of the family option
we fit the model using only the observations before library mass lda fit lda direction lag lag data smarket subset train lda fit call lda direction lag lag data smarket subset train prior of groups down up group means lag lag down up
classification of linear ld lag lag plot lda fit the lda output indicates that and in other words of the training observations correspond to days during which the market went down
it also provides the group means these are the average of each predictor within each class and are used by lda as estimates of k
these suggest that there is tendency for the previous two days returns to be negative on days when the market increases and tendency for the previous days returns to be positive on days when the market declines
the coefficients of linear discriminants output provides the linear combination of lag and lag that are used to form the lda decision rule
in other words these are the multipliers of the elements of in
if lag lag is large then the lda classifier will predict market increase and if it is small then the lda classifier will predict market decline
the plot function produces plots of the linear discriminants obtained by computing lag lag for each of the training observations
the predict function returns list with three elements
the first element class contains lda's predictions about the movement of the market
the second element posterior is matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class computed from
finally contains the linear discriminants described earlier lda pred predict lda fit smarket names lda pred class posterior as we observed in section the lda and logistic regression predictions are almost identical lda class lda pred class table lda class direction direction lda pred down up down up mean lda class direction applying threshold to the posterior probabilities allows us to recreate the predictions contained in lda pred class sum lda sum lda
lab logistic regression lda qda and knn notice that the posterior probability output by the model corresponds to the probability that the market will decrease lda lda class if we wanted to use posterior probability threshold other than in order to make predictions then we could easily do so
for instance suppose that we wish to predict market decrease only if we are very certain that the market will indeed decrease on that day say if the posterior probability is at least sum lda no days in meet that threshold
in fact the greatest posterior probability of decrease in all of was
quadratic discriminant analysis we will now fit qda model to the smarket data
qda is implemented in using the qda function which is also part of the mass library
the qda syntax is identical to that of lda qda fit qda direction lag lag data smarket subset train qda fit call qda direction lag lag data smarket subset train prior of groups down up group means lag lag down up the output contains the group means
but it does not contain the coefficients of the linear discriminants because the qda classifier involves quadratic rather than linear function of the predictors
the predict function works in exactly the same fashion as for lda qda class predict qda fit smarket class table qda class direction direction qda class down up down up mean qda class direction
classification interestingly the qda predictions are accurate almost of the time even though the data was not used to fit the model
this level of accuracy is quite impressive for stock market data which is known to be quite hard to model accurately
this suggests that the quadratic form assumed by qda may capture the true relationship more accurately than the linear forms assumed by lda and logistic regression
however we recommend evaluating this method's performance on larger test set before betting that this approach will consistently beat the market
nearest neighbors we will now perform knn using the knn function which is part of the knn class library
this function works rather differently from the other modelfitting functions that we have encountered thus far
rather than two step approach in which we first fit the model and then we use the model to make predictions knn forms predictions using single command
the function requires four inputs
matrix containing the predictors associated with the training data labeled train below
matrix containing the predictors associated with the data for which we wish to make predictions labeled test below
vector containing the class labels for the training observations labeled train direction below
value for the number of nearest neighbors to be used by the classifier
we use the cbind function short for column bind to bind the lag and cbind lag variables together into two matrices one for the training set and the other for the test set library class train
cbind lag lag train test
cbind lag lag
train train
direction direction train now the knn function can be used to predict the market's movement for the dates in
we set random seed before we apply knn because if several observations are tied as nearest neighbors then will randomly break the tie
therefore seed must be set in order to ensure reproducibility of results set seed knn pred knn train
test
train
direction table knn pred direction direction
lab logistic regression lda qda and knn knn pred down up down up the results using are not very good since only of the observations are correctly predicted
of course it may be that results in an overly flexible fit to the data
below we repeat the analysis using knn pred knn train
test
train
direction table knn pred direction direction knn pred down up down up mean knn pred direction the results have improved slightly
but increasing further turns out to provide no further improvements
it appears that for this data qda provides the best results of the methods that we have examined so far
an application to caravan insurance data finally we will apply the knn approach to the caravan data set which is part of the islr library
this data set includes predictors that measure demographic characteristics for individuals
the response variable is purchase which indicates whether or not given individual purchases caravan insurance policy
in this data set only of people purchased caravan insurance dim caravan attach caravan summary purchase no yes because the knn classifier predicts the class of given test observation by identifying the observations that are nearest to it the scale of the variables matters
any variables that are on large scale will have much larger effect on the distance between the observations and hence on the knn classifier than variables that are on small scale
for instance imagine data set that contains two variables salary and age measured in dollars and years respectively
as far as knn is concerned difference of in salary is enormous compared to difference of years in age
consequently salary will drive the knn classification results and age will have almost no effect
this is contrary to our intuition that salary difference
classification of is quite small compared to an age difference of years
furthermore the importance of scale to the knn classifier leads to another issue if we measured salary in japanese yen or if we measured age in minutes then we'd get quite different classification results from what we get if these two variables are measured in dollars and years
good way to handle this problem is to standardize the data so that all standardize variables are given mean of zero and standard deviation of one
then all variables will be on comparable scale
the scale function does just scale this
in standardizing the data we exclude column because that is the qualitative purchase variable
scale caravan var caravan var caravan var
var
now every column of standardized has standard deviation of one and mean of zero
we now split the observations into test set containing the first observations and training set containing the remaining observations
we fit knn model on the training data using and evaluate its performance on the test data test train

test test

test train
purchase test test
purchase test set seed knn pred knn train
test
train
mean test
knn pred mean test
no the vector test is numeric with values from through
typing standardized test yields the submatrix of the data containing the observations whose indices range from to whereas typing standardized test yields the submatrix containing the observations whose indices do not range from to
the knn error rate on the test observations is just under
at first glance this may appear to be fairly good
however since only of customers purchased insurance we could get the error rate down to by always predicting no regardless of the values of the predictors
suppose that there is some non trivial cost to trying to sell insurance to given individual
for instance perhaps salesperson must visit each
lab logistic regression lda qda and knn potential customer
if the company tries to sell insurance to random selection of customers then the success rate will be only which may be far too low given the costs involved
instead the company would like to try to sell insurance only to customers who are likely to buy it
so the overall error rate is not of interest
instead the fraction of individuals that are correctly predicted to buy insurance is of interest
it turns out that knn with does far better than random guessing among the customers that are predicted to buy insurance
among such customers or actually do purchase insurance
this is double the rate that one would obtain from random guessing table knn pred test
test
knn pred no yes no yes using the success rate increases to and with the rate is
this is over four times the rate that results from random guessing
it appears that knn is finding some real patterns in difficult data set
knn pred knn train
test
train
table knn pred test
test
knn pred no yes no yes knn pred knn train
test
train
table knn pred test
test
knn pred no yes no yes as comparison we can also fit logistic regression model to the data
if we use as the predicted probability cut off for the classifier then we have problem only seven of the test observations are predicted to purchase insurance
even worse we are wrong about all of these
however we are not required to use cut off of
if we instead predict purchase any time the predicted probability of purchase exceeds we get much better results we predict that people will purchase insurance and we are correct for about of these people
this is over times better than random guessing
glm fit glm purchase data caravan family binomial subset test
classification warning message glm fit fitted numerica ll or occurred glm probs predict glm fit caravan test type response glm pred rep no glm pred glm probs yes table glm pred test
test
glm pred no yes no yes glm pred rep no glm pred glm probs yes table glm pred test
test
glm pred no yes no yes exercises conceptual
using little bit of algebra prove that is equivalent to
in other words the logistic function representation and logit representation for the logistic regression model are equivalent
it was stated in the text that classifying an observation to the class for which is largest is equivalent to classifying an observation to the class for which is largest
prove that this is the case
in other words under the assumption that the observations in the kth class are drawn from k distribution the bayes classifier assigns an observation to the class for which the discriminant function is maximized
this problem relates to the qda model in which the observations within each class are drawn from normal distribution with classspecific mean vector and class specific covariance matrix
we consider the simple case where there is only one feature
suppose that we have classes and that if an observation belongs to the kth class then comes from one dimensional normal distribution k
recall that the density function for the one dimensional normal distribution is given in
prove that in this case the bayes classifier is not linear
argue that it is in fact quadratic
exercises hint for this problem you should follow the arguments laid out in section but without making the assumption that

when the number of features is large there tends to be deterioration in the performance of knn and other local approaches that perform prediction using only observations that are near the test observation for which prediction must be made
this phenomenon is known as the curse of dimensionality and it ties into the fact that curse of non parametric approaches often perform poorly when is large
we dimensionality will now investigate this curse suppose that we have set of observations each with measurements on feature
we assume that is uniformly evenly distributed on
associated with each observation is response value
suppose that we wish to predict test observation's response using only observations that are within of the range of closest to that test observation
for instance in order to predict the response for test observation with we will use observations in the range
on average what fraction of the available observations will we use to make the prediction
now suppose that we have set of observations each with measurements on features and
we assume that are uniformly distributed on
we wish to predict test observation's response using only observations that are within of the range of and within of the range of closest to that test observation
for instance in order to predict the response for test observation with and we will use observations in the range for and in the range for
on average what fraction of the available observations will we use to make the prediction
now suppose that we have set of observations on features
again the observations are uniformly distributed on each feature and again each feature ranges in value from to
we wish to predict test observation's response using observations within the of each feature's range that is closest to that test observation
what fraction of the available observations will we use to make the prediction
using your answers to parts argue that drawback of knn when is large is that there are very few training observations near any given test observation now suppose that we wish to make prediction for test observation by creating dimensional hypercube centered around the test observation that contains on average of the training observations
for and what is the length of
classification each side of the hypercube
comment on your answer
note hypercube is generalization of cube to an arbitrary number of dimensions
when hypercube is simply line segment when it is square and when it is dimensional cube
we now examine the differences between lda and qda if the bayes decision boundary is linear do we expect lda or qda to perform better on the training set
on the test set
if the bayes decision boundary is non linear do we expect lda or qda to perform better on the training set
on the test set
in general as the sample size increases do we expect the test prediction accuracy of qda relative to lda to improve decline or be unchanged
true or false even if the bayes decision boundary for given problem is linear we will probably achieve superior test error rate using qda rather than lda because qda is flexible enough to model linear decision boundary
justify your answer
suppose we collect data for group of students in statistics class with variables hours studied undergrad gpa and receive an
we fit logistic regression and produce estimated coefficients estimate the probability that student who studies for hours and has an undergrad gpa of gets an in the class how many hours would the student in part need to study to have chance of getting an in the class
suppose that we wish to predict whether given stock will issue dividend this year yes or no based on last year's percent profit
we examine large number of companies and discover that the mean value of for companies that issued dividend was while the mean for those that didn't was
in addition the variance of for these two sets of companies was
finally of companies issued dividends
predict the probability that company will issue dividend this year given that its percentage return was last year
hint recall that the density function for normal random variable is
you will need to use bayes theorem
suppose that we take data set divide it into equally sized training and test sets and then try out two different classification procedures
first we use logistic regression and get an error rate of on the training data and on the test data
next we use nearest neighbors
and get an average error rate averaged over both test and training data sets of
based on these results which method should we prefer to use for classification of new observations
this problem has to do with odds on average what fraction of people with an odds of of defaulting on their credit card payment will in fact default
suppose that an individual has chance of defaulting on her credit card payment
what are the odds that she will default
this question should be answered using the weekly data set which is part of the islr package
this data is similar in nature to the smarket data from this chapter's lab except that it contains weekly returns for years from the beginning of to the end of produce some numerical and graphical summaries of the weekly data
do there appear to be any patterns
use the full data set to perform logistic regression with direction as the response and the five lag variables plus volume as predictors
use the summary function to print the results
do any of the predictors appear to be statistically significant
if so which ones
compute the confusion matrix and overall fraction of correct predictions
explain what the confusion matrix is telling you about the types of mistakes made by logistic regression now fit the logistic regression model using training data period from to with lag as the only predictor
compute the confusion matrix and the overall fraction of correct predictions for the held out data that is the data from and repeat using lda repeat using qda repeat using knn with which of these methods appears to provide the best results on this data
classification experiment with different combinations of predictors including possible transformations and interactions for each of the methods
report the variables method and associated confusion matrix that appears to provide the best results on the held out data
note that you should also experiment with values for in the knn classifier
in this problem you will develop model to predict whether given car gets high or low gas mileage based on the auto data set create binary variable mpg that contains if mpg contains value above its median and if mpg contains value below its median
you can compute the median using the median function explore the data graphically in order to investigate the association between mpg and the other features
which of the other features seem most likely to be useful in predicting mpg
scatterplots and boxplots may be useful tools to answer this question
describe your findings split the data into training set and test set perform lda on the training data in order to predict mpg using the variables that seemed most associated with mpg in
what is the test error of the model obtained
perform qda on the training data in order to predict mpg using the variables that seemed most associated with mpg in
what is the test error of the model obtained
perform logistic regression on the training data in order to predict mpg using the variables that seemed most associated with mpg in
what is the test error of the model obtained
perform knn on the training data with several values of in order to predict mpg
use only the variables that seemed most associated with mpg in
what test errors do you obtain
which value of seems to perform the best on this data set
this problem involves writing functions write function power that prints out the result of raising to the rd power
in other words your function should compute and print out the results
hint recall that raises to the power
use the print function to output the result create new function power that allows you to pass any two numbers and and prints out the value of
you can do this by beginning your function with the line
exercises power function you should be able to call your function by entering for instance power on the command line
this should output the value of namely using the power function that you just wrote compute and now create new function power that actually returns the result as an object rather than simply printing it to the screen
that is if you store the value in an object called result within your function then you can simply return this return result using the following line return result the line above should be the last line in your function before the symbol now using the power function create plot of
the axis should display range of integers from to and the axis should display
label the axes appropriately and use an appropriate title for the figure
consider displaying either the axis the axis or both on the log scale
you can do this by using log log or log xy as arguments to the plot function create function plotpower that allows you to create plot of against for fixed and for range of values of
for instance if you call plotpower then plot should be created with an axis taking on values and axis taking on values
using the boston data set fit classification models in order to predict whether given suburb has crime rate above or below the median
explore logistic regression lda and knn models using various subsets of the predictors
describe your findings
this is page printer opaque this resampling methods resampling methods are an indispensable tool in modern statistics
they involve repeatedly drawing samples from training set and refitting model of interest on each sample in order to obtain additional information about the fitted model
for example in order to estimate the variability of linear regression fit we can repeatedly draw different samples from the training data fit linear regression to each new sample and then examine the extent to which the resulting fits differ
such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample
resampling approaches can be computationally expensive because they involve fitting the same statistical method multiple times using different subsets of the training data
however due to recent advances in computing power the computational requirements of resampling methods generally are not prohibitive
in this chapter we discuss two of the most commonly used resampling methods cross validation and the bootstrap
both methods are important tools in the practical application of many statistical learning procedures
for example cross validation can be used to estimate the test error associated with given statistical learning method in order to evaluate its performance or to select the appropriate level of flexibility
the process of evaluating model's performance is known as model assessment whereas model assessment the process of selecting the proper level of flexibility for model is known as model selection
the bootstrap is used in several contexts most commonly model selection to provide measure of accuracy of parameter estimate or of given statistical learning method
resampling methods cross validation in chapter we discuss the distinction between the test error rate and the training error rate
the test error is the average error that results from using statistical learning method to predict the response on new observation that is measurement that was not used in training the method
given data set the use of particular statistical learning method is warranted if it results in low test error
the test error can be easily calculated if designated test set is available
unfortunately this is usually not the case
in contrast the training error can be easily calculated by applying the statistical learning method to the observations used in its training
but as we saw in chapter the training error rate often is quite different from the test error rate and in particular the former can dramatically underestimate the latter
in the absence of very large designated test set that can be used to directly estimate the test error rate number of techniques can be used to estimate this quantity using the available training data
some methods make mathematical adjustment to the training error rate in order to estimate the test error rate
such approaches are discussed in chapter
in this section we instead consider class of methods that estimate the test error rate by holding out subset of the training observations from the fitting process and then applying the statistical learning method to those held out observations
in sections for simplicity we assume that we are interested in performing regression with quantitative response
in section we consider the case of classification with qualitative response
as we will see the key concepts remain the same regardless of whether the response is quantitative or qualitative
the validation set approach suppose that we would like to estimate the test error associated with fitting particular statistical learning method on set of samples
the validation set approach displayed in figure is very simple strategy for this task validation set it involves randomly dividing the available set of samples into two parts approach training set and validation set or hold out set
the model is fit on the validation set training set and the fitted model is used to predict the responses for the hold out set observations in the validation set
the resulting validation set error rate typically assessed using mse in the case of quantitative response provides an estimate of the test error rate
we illustrate the validation set approach on the auto data set
recall from chapter that there appears to be non linear relationship between mpg and horsepower and that model that predicts mpg using horsepower and horsepower gives better results than model that uses only linear term
it is natural to wonder whether cubic or higher order fit might provide
cross validation
schematic display of the validation set approach
set of observations are randomly split into training set shown in blue containing observations and among others and validation set shown in beige and containing observation among others
the statistical learning method is fit on the training set and its performance is evaluated on the validation set even better results
we answer this question in chapter by looking at the values associated with cubic term and higher order polynomial terms in linear regression
but we could also answer this question using the validation method
we randomly split the observations into two sets training set containing of the data points and validation set containing the remaining observations
the validation set error rates that result from fitting various regression models on the training sample and evaluating their performance on the validation sample using mse as measure of validation set error are shown in the left hand panel of figure
the validation set mse for the quadratic fit is considerably smaller than for the linear fit
however the validation set mse for the cubic fit is actually slightly larger than for the quadratic fit
this implies that including cubic term in the regression does not lead to better prediction than simply using quadratic term
recall that in order to create the left hand panel of figure we randomly divided the data set into two parts training set and validation set
if we repeat the process of randomly splitting the sample set into two parts we will get somewhat different estimate for the test mse
as an illustration the right hand panel of figure displays ten different validation set mse curves from the auto data set produced using ten different random splits of the observations into training and validation sets
all ten curves indicate that the model with quadratic term has dramatically smaller validation set mse than the model with only linear term
furthermore all ten curves indicate that there is not much benefit in including cubic or higher order polynomial terms in the model
but it is worth noting that each of the ten curves results in different test mse estimate for each of the ten regression models considered
and there is no consensus among the curves as to which model results in the smallest validation set mse
based on the variability among these curves all that we can conclude with any confidence is that the linear fit is not adequate for this data
the validation set approach is conceptually simple and is easy to implement
but it has two potential drawbacks
the validation set approach was used on the auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower
left validation error estimates for single split into training and validation data sets
right the validation method was repeated ten times each time using different random split of the observations into training set and validation set
this illustrates the variability in the estimated test mse that results from this approach
as is shown in the right hand panel of figure the validation estimate of the test error rate can be highly variable depending on precisely which observations are included in the training set and which observations are included in the validation set
in the validation approach only subset of the observations those that are included in the training set rather than in the validation set are used to fit the model
since statistical methods tend to perform worse when trained on fewer observations this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set
in the coming subsections we will present cross validation refinement of the validation set approach that addresses these two issues
leave one out cross validation leave one out cross validation loocv is closely related to the validation leave one out set approach of section but it attempts to address that method's cross validation drawbacks
like the validation set approach loocv involves splitting the set of observations into two parts
however instead of creating two subsets of comparable size single observation is used for the validation set and the remaining observations xn yn make up the training set
the statistical learning method is fit on the training
cross validation
schematic display of loocv
set of data points is repeatedly split into training set shown in blue containing all but one observation and validation set that contains only that observation shown in beige
the test error is then estimated by averaging the resulting mse's
the first training set contains all but observation the second training set contains all but observation and so forth observations and prediction is made for the excluded observation using its value
since was not used in the fitting process mse provides an approximately unbiased estimate for the test error
but even though mse is unbiased for the test error it is poor estimate because it is highly variable since it is based upon single observation
we can repeat the procedure by selecting for the validation data training the statistical learning procedure on the observations xn yn and computing mse
repeating this approach times produces squared errors mse msen
the loocv estimate for the test mse is the average of these test error estimates cv msei
schematic of the loocv approach is illustrated in figure
loocv has couple of major advantages over the validation set approach
first it has far less bias
in loocv we repeatedly fit the statistical learning method using training sets that contain observations almost as many as are in the entire data set
this is in contrast to the validation set approach in which the training set is typically around half the size of the original data set
consequently the loocv approach tends not to overestimate the test error rate as much as the validation set approach does
second in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the train
cross validation was used on the auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower
left the loocv error curve
right fold cv was run nine separate times each with different random split of the data into parts
the figure shows the nine slightly different cv error curves ing validation set splits performing loocv multiple times will always yield the same results there is no randomness in the training validation set splits
we used loocv on the auto data set in order to obtain an estimate of the test set mse that results from fitting linear regression model to predict mpg using polynomial functions of horsepower
the results are shown in the left hand panel of figure
loocv has the potential to be expensive to implement since the model has to be fit times
this can be very time consuming if is large and if each individual model is slow to fit
with least squares linear or polynomial regression an amazing shortcut makes the cost of loocv the same as that of single model fit
the following formula holds yi cv hi where is the ith fitted value from the original least squares fit and hi is the leverage defined in equation on page
this is like the ordinary mse except the ith residual is divided by hi
the leverage lies between and and reflects the amount that an observation influences its own fit
hence the residuals for high leverage points are inflated in this formula by exactly the right amount for this equality to hold
this is one of the most beautiful formulas in statistics
loocv is very general method and can be used with any kind of predictive modeling
for example we could use it with logistic regression or linear discriminant analysis or any of the methods discussed in later
cross validation
schematic display of fold cv
set of observations is randomly split into five non overlapping groups
each of these fifths acts as validation set shown in beige and the remainder as training set shown in blue
the test error is estimated by averaging the five resulting mse estimates chapters
the magic formula does not hold in general in which case the model has to be refit times
fold cross validation an alternative to loocv is fold cv
this approach involves randomly fold cv dividing the set of observations into groups or folds of approximately equal size
the first fold is treated as validation set and the method is fit on the remaining folds
the mean squared error mse is then computed on the observations in the held out fold
this procedure is repeated times each time different group of observations is treated as validation set
this process results in estimates of the test error mse mse msek
the fold cv estimate is computed by averaging these values cv msei
figure illustrates the fold cv approach
it is not hard to see that loocv is special case of fold cv in which is set to equal
in practice one typically performs fold cv using or
what is the advantage of using or rather than
the most obvious advantage is computational
loocv requires fitting the statistical learning method times
this has the potential to be computationally expensive except for linear models fit by least squares in which case formula can be used
but cross validation is very general approach that can be applied to almost any statistical learning method
some statistical learning methods have computationally intensive fitting procedures and so performing loocv may pose computational
true and estimated test mse for the simulated data sets in figures left center and right
the true test mse is shown in blue the loocv estimate is shown as black dashed line and the fold cv estimate is shown in orange
the crosses indicate the minimum of each of the mse curves problems especially if is extremely large
in contrast performing fold cv requires fitting the learning procedure only times which may be much more feasible
as we see in section there also can be other non computational advantages to performing fold or fold cv which involve the bias variance trade off
the right hand panel of figure displays nine different fold cv estimates for the auto data set each resulting from different random split of the observations into ten folds
as we can see from the figure there is some variability in the cv estimates as result of the variability in how the observations are divided into ten folds
but this variability is typically much lower than the variability in the test error estimates that results from the validation set approach right hand panel of figure
when we examine real data we do not know the true test mse and so it is difficult to determine the accuracy of the cross validation estimate
however if we examine simulated data then we can compute the true test mse and can thereby evaluate the accuracy of our cross validation results
in figure we plot the cross validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in figures and of chapter
the true test mse is displayed in blue
the black dashed and orange solid lines respectively show the estimated loocv and fold cv estimates
in all three plots the two cross validation estimates are very similar
in the right hand panel of figure the true test mse and the cross validation curves are almost identical
in the center panel of figure the two sets of curves are similar at the lower degrees of flexibility while the cv curves overestimate the test set mse for higher degrees of flexibility
in the left hand panel of figure
cross validation the cv curves have the correct general shape but they underestimate the true test mse
when we perform cross validation our goal might be to determine how well given statistical learning procedure can be expected to perform on independent data in this case the actual estimate of the test mse is of interest
but at other times we are interested only in the location of the minimum point in the estimated test mse curve
this is because we might be performing cross validation on number of statistical learning methods or on single method using different levels of flexibility in order to identify the method that results in the lowest test error
for this purpose the location of the minimum point in the estimated test mse curve is important but the actual value of the estimated test mse is not
we find in figure that despite the fact that they sometimes underestimate the true test mse all of the cv curves come close to identifying the correct level of flexibility that is the flexibility level corresponding to the smallest test mse
bias variance trade off for fold cross validation we mentioned in section that fold cv with has computational advantage to loocv
but putting computational issues aside less obvious but potentially more important advantage of fold cv is that it often gives more accurate estimates of the test error rate than does loocv
this has to do with bias variance trade off
it was mentioned in section that the validation set approach can lead to overestimates of the test error rate since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set
using this logic it is not hard to see that loocv will give approximately unbiased estimates of the test error since each training set contains observations which is almost as many as the number of observations in the full data set
and performing fold cv for say or will lead to an intermediate level of bias since each training set contains observations fewer than in the loocv approach but substantially more than in the validation set approach
therefore from the perspective of bias reduction it is clear that loocv is to be preferred to fold cv
however we know that bias is not the only source for concern in an estimating procedure we must also consider the procedure's variance
it turns out that loocv has higher variance than does fold cv with
why is this the case
when we perform loocv we are in effect averaging the outputs of fitted models each of which is trained on an almost identical set of observations therefore these outputs are highly positively correlated with each other
in contrast when we perform fold cv with we are averaging the outputs of fitted models that are somewhat less correlated with each other since the overlap between the training sets in
resampling methods each model is smaller
since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated the test error estimate resulting from loocv tends to have higher variance than does the test error estimate resulting from fold cv
to summarize there is bias variance trade off associated with the choice of in fold cross validation
typically given these considerations one performs fold cross validation using or as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance
cross validation on classification problems in this chapter so far we have illustrated the use of cross validation in the regression setting where the outcome is quantitative and so have used mse to quantify test error
but cross validation can also be very useful approach in the classification setting when is qualitative
in this setting cross validation works just as described earlier in this chapter except that rather than using mse to quantify test error we instead use the the number of misclassified observations
for instance in the classification setting the loocv error rate takes the form cv erri where erri yi
the fold cv error rate and validation set error rates are defined analogously
as an example we fit various logistic regression models on the twodimensional classification data displayed in figure
in the top left panel of figure the black solid line shows the estimated decision boundary resulting from fitting standard logistic regression model to this data set
since this is simulated data we can compute the true test error rate which takes value of and so is substantially larger than the bayes error rate of
clearly logistic regression does not have enough flexibility to model the bayes decision boundary in this setting
we can easily extend logistic regression to obtain non linear decision boundary by using polynomial functions of the predictors as we did in the regression setting in section
for example we can fit quadratic logistic regression model given by log
the top right panel of figure displays the resulting decision boundary which is now curved
however the test error rate has improved only slightly to
much larger improvement is apparent in the bottom left panel
logistic regression fits on the two dimensional classification data displayed in figure
the bayes decision boundary is represented using purple dashed line
estimated decision boundaries from linear quadratic cubic and quartic degrees logistic regressions are displayed in black
the test error rates for the four logistic regression fits are respectively and while the bayes error rate is
test error brown training error blue and fold cv error black on the two dimensional classification data displayed in figure
left logistic regression using polynomial functions of the predictors
the order of the polynomials used is displayed on the axis
right the knn classifier with different values of the number of neighbors used in the knn classifier of figure in which we have fit logistic regression model involving cubic polynomials of the predictors
now the test error rate has decreased to
going to quartic polynomial bottom right slightly increases the test error
in practice for real data the bayes decision boundary and the test error rates are unknown
so how might we decide between the four logistic regression models displayed in figure
we can use cross validation in order to make this decision
the left hand panel of figure displays in black the fold cv error rates that result from fitting ten logistic regression models to the data using polynomial functions of the predictors up to tenth order
the true test errors are shown in brown and the training errors are shown in blue
as we have seen previously the training error tends to decrease as the flexibility of the fit increases
the figure indicates that though the training error rate doesn't quite decrease monotonically it tends to decrease on the whole as the model complexity increases
in contrast the test error displays characteristic shape
the fold cv error rate provides pretty good approximation to the test error rate
while it somewhat underestimates the error rate it reaches minimum when fourth order polynomials are used which is very close to the minimum of the test curve which occurs when third order polynomials are used
in fact using fourth order polynomials would likely lead to good test set performance as the true test error rate is approximately the same for third fourth fifth and sixth order polynomials
the right hand panel of figure displays the same three curves using the knn approach for classification as function of the value of which in this context indicates the number of neighbors used in the knn
the bootstrap classifier rather than the number of cv folds used
again the training error rate declines as the method becomes more flexible and so we see that the training error rate cannot be used to select the optimal value for
though the cross validation error curve slightly underestimates the test error rate it takes on minimum very close to the best value for
the bootstrap the bootstrap is widely applicable and extremely powerful statistical tool bootstrap that can be used to quantify the uncertainty associated with given estimator or statistical learning method
as simple example the bootstrap can be used to estimate the standard errors of the coefficients from linear regression fit
in the specific case of linear regression this is not particularly useful since we saw in chapter that standard statistical software such as outputs such standard errors automatically
however the power of the bootstrap lies in the fact that it can be easily applied to wide range of statistical learning methods including some for which measure of variability is otherwise difficult to obtain and is not automatically output by statistical software
in this section we illustrate the bootstrap on toy example in which we wish to determine the best investment allocation under simple model
in section we explore the use of the bootstrap to assess the variability associated with the regression coefficients in linear model fit
suppose that we wish to invest fixed sum of money in two financial assets that yield returns of and respectively where and are random quantities
we will invest fraction of our money in and will invest the remaining in
since there is variability associated with the returns on these two assets we wish to choose to minimize the total risk or variance of our investment
in other words we want to minimize var
one can show that the value that minimizes the risk is given by xy xy where var var and xy cov
in reality the quantities and xy are unknown
we can compute estimates for these quantities and xy using data set that contains past measurements for and
we can then estimate the value of that minimizes the variance of our investment using xy
xy figure illustrates this approach for estimating on simulated data set
in each panel we simulated pairs of returns for the investments
each panel displays simulated returns for investments and
from left to right and top to bottom the resulting estimates for are and
and
we used these returns to estimate and xy which we then substituted into in order to obtain estimates for
the value of resulting from each simulated data set ranges from to
it is natural to wish to quantify the accuracy of our estimate of
to estimate the standard deviation of we repeated the process of simulating paired observations of and and estimating using times
we thereby obtained estimates for which we can call
the left hand panel of figure displays histogram of the resulting estimates
for these simulations the parameters were set to and xy and so we know that the true value of is
we indicated this value using solid vertical line on the histogram
the mean over all estimates for is very close to and the standard deviation of the estimates is

left histogram of the estimates of obtained by generating simulated data sets from the true population
center histogram of the estimates of obtained from bootstrap samples from single data set
right the estimates of displayed in the left and center panels are shown as boxplots
in each panel the pink line indicates the true value of
this gives us very good idea of the accuracy of se
so roughly speaking for random sample from the population we would expect to differ from by approximately on average
in practice however the procedure for estimating se outlined above cannot be applied because for real data we cannot generate new samples from the original population
however the bootstrap approach allows us to use computer to emulate the process of obtaining new sample sets so that we can estimate the variability of without generating additional samples
rather than repeatedly obtaining independent data sets from the population we instead obtain distinct data sets by repeatedly sample observations from the original data set
this approach is illustrated in figure on simple data set which we call that contains only observations
we randomly select observations from the data set in order to produce bootstrap data set
the sampling is performed with replacement which means that the replacement same observation can occur more than once in the bootstrap data set
in this example contains the third observation twice the first observation once and no instances of the second observation
note that if an observation is contained in then both its and values are included
we can use to produce new bootstrap estimate for which we call
this procedure is repeated times for some large value of in order to produce different bootstrap data sets and corresponding estimates
we can compute the standard error of these
resampling methods obs obs obs

original data
graphical illustration of the bootstrap approach on small sample containing observations
each bootstrap data set contains observations sampled with replacement from the original data set
each bootstrap data set is used to obtain an estimate of bootstrap estimates using the formula
seb
this serves as an estimate of the standard error of estimated from the original data set
the bootstrap approach is illustrated in the center panel of figure which displays histogram of bootstrap estimates of each computed using distinct bootstrap data set
this panel was constructed on the basis of single data set and hence could be created using real data
note that the histogram looks very similar to the left hand panel which displays the idealized histogram of the estimates of obtained by generating simulated data sets from the true population
in particular the bootstrap estimate se from is very close to the estimate of obtained using simulated data sets
the right hand panel displays the information in the center and left panels in different way via boxplots of the estimates for obtained by generating simulated data sets from the true population and using the bootstrap approach
again the boxplots
lab cross validation and the bootstrap are quite similar to each other indicating that the bootstrap approach can be used to effectively estimate the variability associated with
lab cross validation and the bootstrap in this lab we explore the resampling techniques covered in this chapter
some of the commands in this lab may take while to run on your computer
the validation set approach we explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the auto data set
before we begin we use the set seed function in order to set seed for seed r's random number generator so that the reader of this book will obtain precisely the same results as those shown below
it is generally good idea to set random seed when performing an analysis such as cross validation that contains an element of randomness so that the results obtained can be reproduced precisely at later time
we begin by using the sample function to split the set of samples into sample two halves by selecting random subset of observations out of the original observations
we refer to these observations as the training set library islr set seed train sample here we use shortcut in the sample command see
sample for details
we then use the subset option in lm to fit linear regression using only the observations corresponding to the training set lm fit lm mpg horsepower data auto subset train we now use the predict function to estimate the response for all observations and we use the mean function to calculate the mse of the observations in the validation set
note that the train index below selects only the observations that are not in the training set attach auto mean mpg predict lm fit auto train therefore the estimated test mse for the linear regression fit is
we can use the poly function to estimate the test error for the polynomial and cubic regressions
resampling methods lm fit lm mpg poly horsepower data auto subset train mean mpg predict lm fit auto train lm fit lm mpg poly horsepower data auto subset train mean mpg predict lm fit auto train these error rates are and respectively
if we choose different training set instead then we will obtain somewhat different errors on the validation set set seed train sample lm fit lm mpg horsepower subset train mean mpg predict lm fit auto train lm fit lm mpg poly horsepower data auto subset train mean mpg predict lm fit auto train lm fit lm mpg poly horsepower data auto subset train mean mpg predict lm fit auto train using this split of the observations into training set and validation set we find that the validation set error rates for the models with linear quadratic and cubic terms are and respectively
these results are consistent with our previous findings model that predicts mpg using quadratic function of horsepower performs better than model that involves only linear function of horsepower and there is little evidence in favor of model that uses cubic function of horsepower
leave one out cross validation the loocv estimate can be automatically computed for any generalized linear model using the glm and cv glm functions
in the lab for chapcv glm ter we used the glm function to perform logistic regression by passing in the family binomial argument
but if we use glm to fit model without passing in the family argument then it performs linear regression just like the lm function
so for instance glm fit glm mpg horsepower data auto coef glm fit intercept horsepower and lm fit lm mpg horsepower data auto coef lm fit intercept horsepower
lab cross validation and the bootstrap yield identical linear regression models
in this lab we will perform linear regression using the glm function rather than the lm function because the latter can be used together with cv glm
the cv glm function is part of the boot library library boot glm fit glm mpg horsepower data auto cv err cv glm auto glm fit cv err delta the cv glm function produces list with several components
the two numbers in the delta vector contain the cross validation results
in this case both numbers are identical up to two decimal places and correspond to the loocv statistic given in
below we discuss situation in which the two numbers differ
our cross validation estimate for the test error is approximately
we can repeat this procedure for increasingly complex polynomial fits
to automate the process we use the for function to initiate for loop for which iteratively fits polynomial regressions for polynomials of order for loop to computes the associated cross validation error and stores it in the ith element of the vector cv error
we begin by initializing the vector
this command will likely take couple of minutes to run cv error rep for in glm fit glm mpg poly horsepower data auto cv error cv glm auto glm fit delta cv error as in figure we see sharp drop in the estimated test mse between the linear and quadratic fits but then no clear improvement from using higher order polynomials
fold cross validation the cv glm function can also be used to implement fold cv
below we use common choice for on the auto data set
we once again set random seed and initialize vector in which we will store the cv errors corresponding to the polynomial fits of orders one to ten set seed cv error rep for in glm fit glm mpg poly horsepower data auto cv error cv glm auto glm fit delta cv error
resampling methods notice that the computation time is much shorter than that of loocv
in principle the computation time for loocv for least squares linear model should be faster than for fold cv due to the availability of the formula for loocv however unfortunately the cv glm function does not make use of this formula
we still see little evidence that using cubic or higher order polynomial terms leads to lower test error than simply using quadratic fit
we saw in section that the two numbers associated with delta are essentially the same when loocv is performed
when we instead perform fold cv then the two numbers associated with delta differ slightly
the first is the standard fold cv estimate as in
the second is biascorrected version
on this data set the two estimates are very similar to each other
the bootstrap we illustrate the use of the bootstrap in the simple example of section as well as on an example involving estimating the accuracy of the linear regression model on the auto data set
estimating the accuracy of statistic of interest one of the great advantages of the bootstrap approach is that it can be applied in almost all situations
no complicated mathematical calculations are required
performing bootstrap analysis in entails only two steps
first we must create function that computes the statistic of interest
second we use the the boot function which is part of the boot library to boot perform the bootstrap by repeatedly sampling observations from the data set with replacement
the portfolio data set in the islr package is described in section
to illustrate the use of the bootstrap on this data we must first create function alpha fn which takes as input the data as well as vector indicating which observations should be used to estimate
the function then outputs the estimate for based on the selected observations alpha fn function data index data index data index return var cov var var cov this function returns or outputs an estimate for based on applying to the observations indexed by the argument index
for instance the following command tells to estimate using all observations alpha fn portfolio
lab cross validation and the bootstrap the next command uses the sample function to randomly select observations from the range to with replacement
this is equivalent to constructing new bootstrap data set and recomputing based on the new data set set seed alpha fn portfolio sample replace we can implement bootstrap analysis by performing this command many times recording all of the corresponding estimates for and computing the resulting standard deviation
however the boot function automates boot this approach
below we produce bootstrap estimates for boot portfolio alpha fn ordinary bootstrap call boot data portfolio statistic alpha fn bootstrap statistics original bias std error the final output shows that using the original data and that the bootstrap estimate for se is
estimating the accuracy of linear regression model the bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from statistical learning method
here we use the bootstrap approach in order to assess the variability of the estimates for and the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the auto data set
we will compare the estimates obtained using the bootstrap to those obtained using the formulas for se and se described in section
we first create simple function boot fn which takes in the auto data set as well as set of indices for the observations and returns the intercept and slope estimates for the linear regression model
we then apply this function to the full set of observations in order to compute the estimates of and on the entire data set using the usual linear regression coefficient estimate formulas from chapter
note that we do not need the and at the beginning and end of the function because it is only one line long boot fn function data index return coef lm mpg horsepower data data subset index boot fn auto intercept horsepowe
resampling methods the boot fn function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement
here we give two examples set seed boot fn auto sample replace intercept horsepowe boot fn auto sample replace intercept horsepowe next we use the boot function to compute the standard errors of bootstrap estimates for the intercept and slope terms boot auto boot fn ordinary bootstrap call boot data auto statistic boot fn bootstrap statistics original bias std error this indicates that the bootstrap estimate for se is and that the bootstrap estimate for se is
as discussed in section standard formulas can be used to compute the standard errors for the regression coefficients in linear model
these can be obtained using the summary function summary lm mpg horsepower data auto coef estimate std
error value pr intercept horsepower the standard error estimates for and obtained using the formulas from section are for the intercept and for the slope
interestingly these are somewhat different from the estimates obtained using the bootstrap
does this indicate problem with the bootstrap
in fact it suggests the opposite
recall that the standard formulas given in equation on page rely on certain assumptions
for example they depend on the unknown parameter the noise variance
we then estimate using the rss
now although the formula for the standard errors do not rely on the linear model being correct the estimate for does
we see in figure on page that there is non linear relationship in the data and so the residuals from linear fit will be inflated and so will
secondly the standard formulas assume somewhat unrealistically that the xi are fixed and all the variability comes from the variation in the errors oi
exercises bootstrap approach does not rely on any of these assumptions and so it is likely giving more accurate estimate of the standard errors of and than is the summary function
below we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data
since this model provides good fit to the data figure there is now better correspondence between the bootstrap estimates and the standard estimates of se se and se boot fn function data index lm mpg horsepower horsepower data data subset index set seed boot auto boot fn ordinary bootstrap call boot data auto statistic boot fn bootstrap statistics original bias std error summary lm mpg horsepower horsepowe data auto coef estimate std
error value pr intercept horsepower horsepower exercises conceptual
using basic statistical properties of the variance as well as singlevariable calculus derive
in other words prove that given by does indeed minimize var
we will now derive the probability that given observation is part of bootstrap sample
suppose that we obtain bootstrap sample from set of observations what is the probability that the first bootstrap observation is not the jth observation from the original sample
justify your answer
resampling methods what is the probability that the second bootstrap observation is not the jth observation from the original sample
argue that the probability that the jth observation is not in the bootstrap sample is when what is the probability that the jth observation is in the bootstrap sample
when what is the probability that the jth observation is in the bootstrap sample
when what is the probability that the jth observation is in the bootstrap sample
create plot that displays for each integer value of from to the probability that the jth observation is in the bootstrap sample
comment on what you observe we will now investigate numerically the probability that bootstrap sample of size contains the jth observation
here
we repeatedly create bootstrap samples and each time we record whether or not the fourth observation is contained in the bootstrap sample store rep na for in store sum sample rep true mean store comment on the results obtained
we now review fold cross validation explain how fold cross validation is implemented what are the advantages and disadvantages of fold crossvalidation relative to
the validation set approach
suppose that we use some statistical learning method to make prediction for the response for particular value of the predictor
carefully describe how we might estimate the standard deviation of our prediction
in chapter we used logistic regression to predict the probability of default using income and balance on the default data set
we will now estimate the test error of this logistic regression model using the
exercises validation set approach
do not forget to set random seed before beginning your analysis fit multiple logistic regression model that uses income and balance to predict the probability of default using only the observations using the validation set approach estimate the test error of this model
in order to do this you must perform the following steps
split the sample set into training set and validation set ii
fit multiple logistic regression model using only the training observations iii
obtain prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual and classifying the individual to the default category if the posterior probability equals iv
compute the validation set error which is the fraction of the observations in the validation set that are misclassified repeat the process in three times using three different splits of the observations into training set and validation set
comment on the results obtained now consider logistic regression model that predicts the probability of default using income balance and dummy variable for student
estimate the test error for this model using the validation set approach
comment on whether or not including dummy variable for student leads to reduction in the test error rate
we continue to consider the use of logistic regression model to predict the probability of default using income and balance on the default data set
in particular we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways using the bootstrap and using the standard formula for computing the standard errors in the glm function
do not forget to set random seed before beginning your analysis using the summary and glm functions determine the estimated standard errors for the coefficients associated with income and balance in multiple logistic regression model that uses both predictors write function boot fn that takes as input the default data set as well as an index of the observations and that outputs the coefficient estimates for income and balance in the multiple logistic regression model
resampling methods use the boot function together with your boot fn function to estimate the standard errors of the logistic regression coefficients for income and balance comment on the estimated standard errors obtained using the glm function and using your bootstrap function
in sections and we saw that the cv glm function can be used in order to compute the loocv test error estimate
alternatively one could compute those quantities using just the glm and predict glm functions and for loop
you will now take this approach in order to compute the loocv error for simple logistic regression model on the default data set
recall that in the context of classification problems the loocv error is given in fit logistic regression model that predicts the probability of default using balance fit logistic regression model that predicts the probability of default using balance using all but the first observation use the model from to predict the default status of the first observation
you can do this by predicting that the first observation will default if default balance
was this observation correctly classified
write for loop from to where is the number of observations in the data set that performs each of the following steps
fit logistic regression model using all but the ith observation to predict probability of default using balance ii
compute the posterior probability of default for the ith observation iii
use the posterior probability of default for the ith observation in order to predict whether or not the observation defaults iv
determine whether or not an error was made in predicting the default status for the ith observation
if an error was made then indicate this as and otherwise indicate it as take the average of the numbers obtained in iv in order to obtain the loocv estimate for the test error
comment on the results
we will now perform cross validation on simulated data set generate simulated data set as follows
exercises set seed rnorm rnorm rnorm in this data set what is and what is
write out the model used to generate the data in equation form create scatterplot of against
comment on what you find set random seed and then compute the loocv errors that result from fitting the following four models using least squares
ii
iii
iv
repeat using another random seed and report your results
are your results the same as what you got in
which of the models in had the smallest loocv error
is this what you expected
explain your answer comment on the statistical significance of the coefficient estimates that results from fitting each of the models in using least squares
do these results agree with the conclusions drawn based on the cross validation results
we will now consider the boston housing data set from the mass library based on this data set provide an estimate for the population mean of medv
call this estimate provide an estimate of the standard error of
interpret this result
hint we can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations now estimate the standard error of using the bootstrap
how does this compare to your answer from
based on your bootstrap estimate from provide confidence interval for the mean of medv
compare it to the results obtained using test boston medv
hint you can approximate confidence interval using the formula se se based on this data set provide an estimate med for the median value of medv in the population
resampling methods we now would like to estimate the standard error of med
unfortunately there is no simple formula for computing the standard error of the median
instead estimate the standard error of the median using the bootstrap
comment on your findings based on this data set provide an estimate for the tenth percentile of medv in boston suburbs
call this quantity
you can use the quantile function use the bootstrap to estimate the standard error of
comment on your findings
this is page printer opaque this linear model selection and regularization in the regression setting the standard linear model is commonly used to describe the relationship between response and set of variables xp
we have seen in chapter that one typically fits this model using least squares
in the chapters that follow we consider some approaches for extending the linear model framework
in chapter we generalize in order to accommodate non linear but still additive relationships while in chapter we consider even more general non linear models
however the linear model has distinct advantages in terms of inference and on real world problems is often surprisingly competitive in relation to non linear methods
hence before moving to the non linear world we discuss in this chapter some ways in which the simple linear model can be improved by replacing plain least squares fitting with some alternative fitting procedures
why might we want to use another fitting procedure instead of least squares
as we will see alternative fitting procedures can yield better prediction accuracy and model interpretability
if that is if the number of observations is much larger than the number of variables then the least squares estimates tend to also have low variance and hence will perform well on test observations
however if is not much larger
linear model selection and regularization than then there can be lot of variability in the least squares fit resulting in overfitting and consequently poor predictions on future observations not used in model training
and if then there is no longer unique least squares coefficient estimate the variance is infinite so the method cannot be used at all
by constraining or shrinking the estimated coefficients we can often substantially reduce the variance at the cost of negligible increase in bias
this can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training
including such irrelevant variables leads to unnecessary complexity in the resulting model
by removing these variables that is by setting the corresponding coefficient estimates to zero we can obtain model that is more easily interpreted
now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero
in this chapter we see some approaches for automatically performing feature selection or variable selection that is feature selection for excluding irrelevant variables from multiple regression model variable selection there are many alternatives both classical and modern to using least squares to fit
in this chapter we discuss three important classes of methods
this approach involves identifying subset of the predictors that we believe to be related to the response
we then fit model using least squares on the reduced set of variables
this approach involves fitting model involving all predictors
however the estimated coefficients are shrunken towards zero relative to the least squares estimates
this shrinkage also known as regularization has the effect of reducing variance
depending on what type of shrinkage is performed some of the coefficients may be estimated to be exactly zero
hence shrinkage methods can also perform variable selection
this approach involves projecting the predictors into dimensional subspace where
this is achieved by computing different linear combinations or projections of the variables
then these projections are used as predictors to fit linear regression model by least squares
in the following sections we describe each of these approaches in greater detail along with their advantages and disadvantages
although this chapter describes extensions and modifications to the linear model for regression seen in chapter the same concepts apply to other methods such as the classification models seen in chapter
subset selection subset selection in this section we consider some methods for selecting subsets of predictors
these include best subset and stepwise model selection procedures
best subset selection to perform best subset selection we fit separate least squares regression best subset for each possible combination of the predictors
that is we fit all models selection that contain exactly one predictor all models that contain exactly two predictors and so forth
we then look at all of the resulting models with the goal of identifying the one that is best
the problem of selecting the best model from among the possibilities considered by best subset selection is not trivial
this is usually broken up into two stages as described in algorithm
algorithm best subset selection
let denote the null model which contains no predictors
this model simply predicts the sample mean for each observation
for fit all kp models that contain exactly predictors pick the best among these kp models and call it mk
here best is defined as having the smallest rss or equivalently largest
select single best model from among mp using crossvalidated prediction error cp aic bic or adjusted
in algorithm step identifies the best model on the training data for each subset size in order to reduce the problem from one of possible models to one of possible models
in figure these models form the lower frontier depicted in red
now in order to select single best model we must simply choose among these options
this task must be performed with care because the rss of these models decreases monotonically and the increases monotonically as the number of features included in the models increases
therefore if we use these statistics to select the best model then we will always end up with model involving all of the variables
the problem is that low rss or high indicates model with low training error whereas we wish to choose model that has low test error
as shown in chapter in figures training error tends to be quite bit smaller than test error and low training error by no means guarantees low test error
therefore in step we use cross validated prediction error
for each possible model containing subset of the ten predictors in the credit data set the rss and are displayed
the red frontier tracks the best model for given number of predictors according to rss and
though the data set contains only ten predictors the axis ranges from to since one of the variables is categorical and takes on three values leading to the creation of two dummy variables
cp bic or adjusted in order to select among mp
these approaches are discussed in section
an application of best subset selection is shown in figure
each plotted point corresponds to least squares regression model fit using different subset of the eleven predictors in the credit data set discussed in chapter
here the variable ethnicity is three level qualitative variable and so is represented by two dummy variables which are selected separately in this case
we have plotted the rss and statistics for each model as function of the number of variables
the red curves connect the best models for each model size according to rss or
the figure shows that as expected these quantities improve as the number of variables increases however from the three variable model on there is little improvement in rss and as result of including additional predictors
although we have presented best subset selection here for least squares regression the same ideas apply to other types of models such as logistic regression
in the case of logistic regression instead of ordering models by rss in step of algorithm we instead use the deviance measure deviance that plays the role of rss for broader class of models
the deviance is negative two times the maximized log likelihood the smaller the deviance the better the fit
while best subset selection is simple and conceptually appealing approach it suffers from computational limitations
the number of possible models that must be considered grows rapidly as increases
in general there are models that involve subsets of predictors
so if then
subset selection there are approximately one thousand possible models to be considered and if then there are over one million possibilities
consequently best subset selection becomes computationally infeasible for values of greater than around even with extremely fast modern computers
there are computational shortcuts so called branch and bound techniques for eliminating some choices but these have their limitations as gets large
they also only work for least squares linear regression
we present computationally efficient alternatives to best subset selection next
stepwise selection for computational reasons best subset selection cannot be applied with very large
best subset selection may also suffer from statistical problems when is large
the larger the search space the higher the chance of finding models that look good on the training data even though they might not have any predictive power on future data
thus an enormous search space can lead to overfitting and high variance of the coefficient estimates
for both of these reasons stepwise methods which explore far more restricted set of models are attractive alternatives to best subset selection
forward stepwise selection forward stepwise selection is computationally efficient alternative to best forward stepwise subset selection
while the best subset selection procedure considers all selection possible models containing subsets of the predictors forward stepwise considers much smaller set of models
forward stepwise selection begins with model containing no predictors and then adds predictors to the model one at time until all of the predictors are in the model
in particular at each step the variable that gives the greatest additional improvement to the fit is added to the model
more formally the forward stepwise selection procedure is given in algorithm
unlike best subset selection which involved fitting models forward stepwise selection involves fitting one null model along with models kth iteration for
this amounts to total of in the models
this is substantial difference when best subset selection requires fitting models whereas forward stepwise selection requires fitting only models in step of algorithm we must identify the best model from among those that augment mk with one additional predictor
we though forward stepwise selection considers models it performs guided search over model space and so the effective model space considered contains substantially more than models
linear model selection and regularization algorithm forward stepwise selection
let denote the null model which contains no predictors
for consider all models that augment the predictors in mk with one additional predictor choose the best among these models and call it mk
here best is defined as having smallest rss or highest
select single best model from among mp using crossvalidated prediction error cp aic bic or adjusted can do this by simply choosing the model with the lowest rss or the highest
however in step we must identify the best model among set of models with different numbers of variables
this is more challenging and is discussed in section
forward stepwise selection's computational advantage over best subset selection is clear
though forward stepwise tends to do well in practice it is not guaranteed to find the best possible model out of all models containing subsets of the predictors
for instance suppose that in given data set with predictors the best possible one variable model contains and the best possible two variable model instead contains and
then forward stepwise selection will fail to select the best possible two variable model because will contain so must also contain together with one additional variable
table which shows the first four selected models for best subset and forward stepwise selection on the credit data set illustrates this phenomenon
both best subset selection and forward stepwise selection choose rating for the best one variable model and then include income and student for the two variable and three variable models
however best subset selection replaces rating by cards in the four variable model while forward stepwise selection must maintain rating in its four variable model
in this example figure indicates that there is not much difference between the three variable and four variable models in terms of rss so either of the four variable models will likely be adequate
forward stepwise selection can be applied even in the high dimensional setting where however in this case it is possible to construct submodels mn only since each submodel is fit using least squares which will not yield unique solution if
backward stepwise selection like forward stepwise selection backward stepwise selection provides an efbackward stepwise ficient alternative to best subset selection
however unlike forward stepwise selection
subset selection variables best subset forward stepwise one rating rating two rating income rating income three rating income student rating income student four cards income rating income student limit student limit table
the first four selected models for best subset selection and forward stepwise selection on the credit data set
the first three models are identical but the fourth models differ selection it begins with the full least squares model containing all predictors and then iteratively removes the least useful predictor one at time
details are given in algorithm
algorithm backward stepwise selection
let mp denote the full model which contains all predictors
for consider all models that contain all but one of the predictors in mk for total of predictors choose the best among these models and call it mk
here best is defined as having smallest rss or highest
select single best model from among mp using crossvalidated prediction error cp aic bic or adjusted
like forward stepwise selection the backward selection approach searches through only models and so can be applied in settings where is too large to apply best subset selection also like forward stepwise selection backward stepwise selection is not guaranteed to yield the best model containing subset of the predictors
backward selection requires that the number of samples is larger than the number of variables so that the full model can be fit
in contrast forward stepwise can be used even when and so is the only viable subset method when is very large
like forward stepwise selection backward stepwise selection performs guided search over model space and so effectively considers substantially more than models
linear model selection and regularization hybrid approaches the best subset forward stepwise and backward stepwise selection approaches generally give similar but not identical models
as another alternative hybrid versions of forward and backward stepwise selection are available in which variables are added to the model sequentially in analogy to forward selection
however after adding each new variable the method may also remove any variables that no longer provide an improvement in the model fit
such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection
choosing the optimal model best subset selection forward selection and backward selection result in the creation of set of models each of which contains subset of the predictors
in order to implement these methods we need way to determine which of these models is best
as we discussed in section the model containing all of the predictors will always have the smallest rss and the largest since these quantities are related to the training error
instead we wish to choose model with low test error
as is evident here and as we show in chapter the training error can be poor estimate of the test error
therefore rss and are not suitable for selecting the best model among collection of models with different numbers of predictors
in order to select the best model with respect to test error we need to estimate this test error
there are two common approaches
we can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting
we can directly estimate the test error using either validation set approach or cross validation approach as discussed in chapter
we consider both of these approaches below
cp aic bic and adjusted we show in chapter that the training set mse is generally an underestimate of the test mse
recall that mse rss
this is because when we fit model to the training data using least squares we specifically estimate the regression coefficients such that the training rss but not the test rss is as small as possible
in particular the training error will decrease as more variables are included in the model but the test error may not
therefore training set rss and training set cannot be used to select from among set of models with different numbers of variables
however number of techniques for adjusting the training error for the model size are available
these approaches can be used to select among set
cp bic and adjusted are shown for the best models of each size for the credit data set the lower frontier in figure
cp and bic are estimates of test mse
in the middle plot we see that the bic estimate of test error shows an increase after four variables are selected
the other two plots are rather flat after four variables are included of models with different numbers of variables
we now consider four such approaches cp akaike information criterion aic bayesian information cp criterion bic and adjusted
figure displays cp bic and adjusted akaike information for the best model of each size produced by best subset selection on the criterion credit data set
bayesian for fitted least squares model containing predictors the cp estimate information of test mse is computed using the equation criterion adjusted cp rss where is an estimate of the variance of the error associated with each response measurement in
essentially the cp statistic adds penalty of to the training rss in order to adjust for the fact that the training error tends to underestimate the test error
clearly the penalty increases as the number of predictors in the model increases this is intended to adjust for the corresponding decrease in training rss
though it is beyond the scope of this book one can show that if is an unbiased estimate of in then cp is an unbiased estimate of test mse
as consequence the cp statistic tends to take on small value for models with low test error so when determining which of set of models is best we choose the model with the lowest cp value
in figure cp selects the six variable model containing the predictors income limit rating cards age and student
the aic criterion is defined for large class of models fit by maximum likelihood
in the case of the model with gaussian errors maximum mallow's is sometimes defined as rss
this is equivalent to the definition given above in the sense that cp cp and so the model with smallest cp also has smallest cp
linear model selection and regularization likelihood and least squares are the same thing
in this case aic is given by aic rss where for simplicity we have omitted an additive constant
hence for least squares models cp and aic are proportional to each other and so only cp is displayed in figure
bic is derived from bayesian point of view but ends up looking similar to cp and aic as well
for the least squares model with predictors the bic is up to irrelevant constants given by bic rss log
like cp the bic will tend to take on small value for model with low test error and so generally we select the model that has the lowest bic value
notice that bic replaces the used by cp with log term where is the number of observations
since log for any the bic statistic generally places heavier penalty on models with many variables and hence results in the selection of smaller models than cp
in figure we see that this is indeed the case for the credit data set bic chooses model that contains only the four predictors income limit cards and student
in this case the curves are very flat and so there does not appear to be much difference in accuracy between the four variable and six variable models
the adjusted statistic is another popular approach for selecting among set of models that contain different numbers of variables
recall from chapter that the usual is defined as rss tss where tss yi is the total sum of squares for the response
since rss always decreases as more variables are added to the model the always increases as more variables are added
for least squares model with variables the adjusted statistic is calculated as rss adjusted
tss unlike cp aic and bic for which small value indicates model with low test error large value of adjusted indicates model with small test error
maximizing the adjusted is equivalent to minimizing rss
while rss always decreases as the number of variables in the model increases rss may increase or decrease due to the presence of in the denominator
the intuition behind the adjusted is that once all of the correct variables have been included in the model adding additional noise variables will lead to only very small decrease in rss
since adding noise variables rss leads to an increase in such variables will lead to an increase in
subset selection and consequently decrease in the adjusted
therefore in theory the model with the largest adjusted will have only correct variables and no noise variables
unlike the statistic the adjusted statistic pays price for the inclusion of unnecessary variables in the model
figure displays the adjusted for the credit data set
using this statistic results in the selection of model that contains seven variables adding gender to the model selected by cp and aic
cp aic and bic all have rigorous theoretical justifications that are beyond the scope of this book
these justifications rely on asymptotic arguments scenarios where the sample size is very large
despite its popularity and even though it is quite intuitive the adjusted is not as well motivated in statistical theory as aic bic and cp
all of these measures are simple to use and compute
here we have presented the formulas for aic bic and cp in the case of linear model fit using least squares however these quantities can also be defined for more general types of models
validation and cross validation as an alternative to the approaches just discussed we can directly estimate the test error using the validation set and cross validation methods discussed in chapter
we can compute the validation set error or the cross validation error for each model under consideration and then select the model for which the resulting estimated test error is smallest
this procedure has an advantage relative to aic bic cp and adjusted in that it provides direct estimate of the test error and makes fewer assumptions about the true underlying model
it can also be used in wider range of model selection tasks even in cases where it is hard to pinpoint the model degrees of freedom the number of predictors in the model or hard to estimate the error variance
in the past performing cross validation was computationally prohibitive for many problems with large and or large and so aic bic cp and adjusted were more attractive approaches for choosing among set of models
however nowadays with fast computers the computations required to perform cross validation are hardly ever an issue
thus crossvalidation is very attractive approach for selecting from among number of models under consideration
figure displays as function of the bic validation set errors and cross validation errors on the credit data for the best variable model
the validation errors were calculated by randomly selecting three quarters of the observations as the training set and the remainder as the validation set
the cross validation errors were computed using folds
in this case the validation and cross validation methods both result in six variable model
however all three approaches suggest that the four
for the credit data set three quantities are displayed for the best model containing predictors for ranging from to
the overall best model based on each of these quantities is shown as blue cross
left square root of bic
center validation set errors
right cross validation errors five and six variable models are roughly equivalent in terms of their test errors
in fact the estimated test error curves displayed in the center and righthand panels of figure are quite flat
while three variable model clearly has lower estimated test error than two variable model the estimated test errors of the three variable to eleven variable models are quite similar
furthermore if we repeated the validation set approach using different split of the data into training set and validation set or if we repeated crossvalidation using different set of cross validation folds then the precise model with the lowest estimated test error would surely change
in this setting we can select model using the one standard error rule
we first one standard error calculate the standard error of the estimated test mse for each model size rule and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve
the rationale here is that if set of models appear to be more or less equally good then we might as well choose the simplest model that is the model with the smallest number of predictors
in this case applying the one standard error rule to the validation set or cross validation approach leads to selection of the three variable model
shrinkage methods the subset selection methods described in section involve using least squares to fit linear model that contains subset of the predictors
as an alternative we can fit model containing all predictors using technique that constrains or regularizes the coefficient estimates or equivalently that shrinks the coefficient estimates towards zero
it may not be immediately obvious why such constraint should improve the fit but it turns out that
shrinkage methods shrinking the coefficient estimates can significantly reduce their variance
the two best known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso
ridge regression recall from chapter that the least squares fitting procedure estimates using the values that minimize eb rss ed xij ridge regression is very similar to least squares except that the coefficients ridge regression are estimated by minimizing slightly different quantity
in particular the ridge regression coefficient estimates are the values that minimize eb ed yi xij bb rss bb where bb is tuning parameter to be determined separately
equatuning parameter tion trades off two different criteria
as with least squares ridge regression seeks coefficient estimates that fit the data well by making the rss small
however the second term bb called shrinkage penalty is shrinkage penalty small when are close to zero and so it has the effect of shrinking the estimates of towards zero
the tuning parameter bb serves to control the relative impact of these two terms on the regression coefficient estimates
when bb the penalty term has no effect and ridge regression will produce the least squares estimates
however as bb the impact of the shrinkage penalty grows and the ridge regression coefficient estimates will approach zero
unlike least squares which generates only one set of coefficient estimates ridge regression will produce different set of coefficient estimates bb for each value of bb
selecting good value for bb is critical we defer this discussion to section where we use cross validation
note that in the shrinkage penalty is applied to but not to the intercept
we want to shrink the estimated association of each variable with the response however we do not want to shrink the intercept which is simply measure of the mean value of the response when xi xi xip
if we assume that the variables that is the columns of the data matrix have been centered to have mean zero before ridge regression pn is performed then the estimated intercept will take the form yi
the standardized ridge regression coefficients are displayed for the credit data set as function of bb and bb
an application to the credit data in figure the ridge regression coefficient estimates for the credit data set are displayed
in the left hand panel each curve corresponds to the ridge regression coefficient estimate for one of the ten variables plotted as function of bb
for example the black solid line represents the ridge regression estimate for the income coefficient as bb is varied
at the extreme left hand side of the plot bb is essentially zero and so the corresponding ridge coefficient estimates are the same as the usual least squares estimates
but as bb increases the ridge coefficient estimates shrink towards zero
when bb is extremely large then all of the ridge coefficient estimates are basically zero this corresponds to the null model that contains no predictors
in this plot the income limit rating and student variables are displayed in distinct colors since these variables tend to have by far the largest coefficient estimates
while the ridge coefficient estimates tend to decrease in aggregate as bb increases individual coefficients such as rating and income may occasionally increase as bb increases
the right hand panel of figure displays the same ridge coefficient estimates as the left hand panel but instead of displaying bb on the axis we now display bb where denotes the vector of least squares coefficient estimates
the notation denotes theqp norm pronounced norm ell of vector and is defined as
it measures the distance of from zero
as bb increases the norm of bb will always decrease and so will bb
the latter quantity ranges from when bb in which case the ridge regression coefficient estimate is the same as the least squares estimate and so their norms are the same to when bb in which case the ridge regression coefficient estimate is
shrinkage methods vector of zeros with norm equal to zero
therefore we can think of the axis in the right hand panel of figure as the amount that the ridge regression coefficient estimates have been shrunken towards zero small value indicates that they have been shrunken very close to zero
the standard least squares coefficient estimates discussed in chapter are scale invariant multiplying xj by constant simply leads to scaling scale invariant of the least squares coefficient estimates by factor of
in other words regardless of how the jth predictor is scaled xj will remain the same
in contrast the ridge regression coefficient estimates can change substantially when multiplying given predictor by constant
for instance consider the income variable which is measured in dollars
one could reasonably have measured income in thousands of dollars which would result in reduction in the observed values of income by factor of
now due to the sum of squared coefficients term in the ridge regression formulation such change in scale will not simply cause the ridge regression coefficient estimate for income to change by factor of
in other words xj bb will depend not only on the value of bb but also on the scaling of the jth predictor
in fact the value of xj bb may even depend on the scaling of the other predictors
therefore it is best to apply ridge regression after standardizing the predictors using the formula xij ij xij xj so that they are all on the same scale
in the denominator is the estimated standard deviation of the jth predictor
consequently all of the standardized predictors will have standard deviation of one
as result the final fit will not depend on the scale on which the predictors are measured
in figure the axis displays the standardized ridge regression coefficient estimates that is the coefficient estimates that result from performing ridge regression using standardized predictors
why does ridge regression improve over least squares
ridge regression's advantage over least squares is rooted in the bias variance trade off
as bb increases the flexibility of the ridge regression fit decreases leading to decreased variance but increased bias
this is illustrated in the left hand panel of figure using simulated data set containing predictors and observations
the green curve in the left hand panel of figure displays the variance of the ridge regression predictions as function of bb
at the least squares coefficient estimates which correspond to ridge regression with bb the variance is high but there is no bias
but as bb increases the shrinkage of the ridge coefficient estimates leads to substantial reduction in the variance of the predictions at the expense of slight increase in bias
recall that the test mean squared error mse plotted in purple is function of the variance plus the squared bias
for values
squared bias black variance green and test mean squared error purple for the ridge regression predictions on simulated data set as function of bb and bb
the horizontal dashed lines indicate the minimum possible mse
the purple crosses indicate the ridge regression models for which the mse is smallest of bb up to about the variance decreases rapidly with very little increase in bias plotted in black
consequently the mse drops considerably as bb increases from to
beyond this point the decrease in variance due to increasing bb slows and the shrinkage on the coefficients causes them to be significantly underestimated resulting in large increase in the bias
the minimum mse is achieved at approximately bb
interestingly because of its high variance the mse associated with the least squares fit when bb is almost as high as that of the null model for which all coefficient estimates are zero when bb
however for an intermediate value of bb the mse is considerably lower
the right hand panel of figure displays the same curves as the lefthand panel this time plotted against the norm of the ridge regression coefficient estimates divided by the norm of the least squares estimates
now as we move from left to right the fits become more flexible and so the bias decreases and the variance increases
in general in situations where the relationship between the response and the predictors is close to linear the least squares estimates will have low bias but may have high variance
this means that small change in the training data can cause large change in the least squares coefficient estimates
in particular when the number of variables is almost as large as the number of observations as in the example in figure the least squares estimates will be extremely variable
and if then the least squares estimates do not even have unique solution whereas ridge regression can still perform well by trading off small increase in bias for large decrease in variance
hence ridge regression works best in situations where the least squares estimates have high variance
shrinkage methods ridge regression also has substantial computational advantages over best subset selection which requires searching through models
as we discussed previously even for moderate values of such search can be computationally infeasible
in contrast for any fixed value of bb ridge regression only fits single model and the model fitting procedure can be performed quite quickly
in fact one can show that the computations required to solve simultaneously for all values of bb are almost identical to those for fitting model using least squares
the lasso ridge regression does have one obvious disadvantage
unlike best subset forward stepwise and backward stepwise selection which will generally select models that involve just subset of the variables ridge regression will include all predictors in the final model
the penalty bb in will shrink all of the coefficients towards zero but it will not set any of them exactly to zero unless bb
this may not be problem for prediction accuracy but it can create challenge in model interpretation in settings in which the number of variables is quite large
for example in the credit data set it appears that the most important variables are income limit rating and student
so we might wish to build model including just these predictors
however ridge regression will always generate model involving all ten predictors
increasing the value of bb will tend to reduce the magnitudes of the coefficients but will not result in exclusion of any of the variables
the lasso is relatively recent alternative to ridge regression that overlasso comes this disadvantage
the lasso coefficients bb minimize the quantity eb ed xij bb rss bb
comparing to we see that the lasso and ridge regression have similar formulations
the only difference is that the term in the ridge regression penalty has been replaced by in the lasso penalty
in statistical parlance the lasso uses an pronounced ell penalty instead pof an penalty
the norm of coefficient vector is given by
as with ridge regression the lasso shrinks the coefficient estimates towards zero
however in the case of the lasso the penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter bb is sufficiently large
hence much like best subset selection the lasso performs variable selection
as result models generated from the lasso are generally much easier to interpret than those produced by ridge regression
we say that the lasso yields sparse models that is sparse
the standardized lasso coefficients on the credit data set are shown as function of bb and bb models that involve only subset of the variables
as in ridge regression selecting good value of bb for the lasso is critical we defer this discussion to section where we use cross validation
as an example consider the coefficient plots in figure which are generated from applying the lasso to the credit data set
when bb then the lasso simply gives the least squares fit and when bb becomes sufficiently large the lasso gives the null model in which all coefficient estimates equal zero
however in between these two extremes the ridge regression and lasso models are quite different from each other
moving from left to right in the right hand panel of figure we observe that at first the lasso results in model that contains only the rating predictor
then student and limit enter the model almost simultaneously shortly followed by income
eventually the remaining variables enter the model
hence depending on the value of bb the lasso can produce model involving any number of variables
in contrast ridge regression will always include all of the variables in the model although the magnitude of the coefficient estimates will depend on bb
another formulation for ridge regression and the lasso one can show that the lasso and ridge regression coefficient estimates solve the problems eb fc fd minimize ed xij subject to fe
shrinkage methods and eb fc xp fd minimize ed xij subject to fe respectively
in other words for every value of bb there is some such that the equations and will give the same lasso coefficient estimates
similarly for every value of bb there is corresponding such that equations and will give the same ridge regression coefficient estimates
when then indicates that the lasso coefficient estimates have the smallest rss out of all points that lie within the diamond defined by
similarly the ridge regression estimates have the smallest rss out of all points that lie within the circle defined by
we can think of as follows
when we perform the lasso we are trying pp rss subject to find the set of coefficient estimates that lead to the smallest to the constraint that there is budget for how large can be
when is extremely large then this budget is not very restrictive and so the coefficient estimates can be large
in fact if is large enough that the least squares solution falls within the budget then pwill simply yield the least squares solution
in contrast if is small then must be small in order to avoid violating the budget
similarly indicates that when we perform ridge regression we seek set of coefficient estimates such pp that the rss is as small as possible subject to the requirement that not exceed the budget
the formulations and reveal close connection between the lasso ridge regression and best subset selection
consider the problem eb fc xp fd xp minimize ed xij subject to
fe here is an indicator variable it takes on value of if and equals zero otherwise
then amounts to finding set of coefficient estimates such that rss is as small as possible subject to the constraint that no more than coefficients can be nonzero
the problem is equivalent to best subset selection
unfortunately solving is computationally infeasible when is large since it requires considering all ps models containing predictors
therefore we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in with forms that are much easier to solve
of course the lasso is much more closely related to best subset selection since only the lasso performs feature selection for sufficiently small in
contours of the error and constraint functions for the lasso left and ridge regression right
the solid blue areas are the constraint regions and while the red ellipses are the contours of the rss
the variable selection property of the lasso why is it that the lasso unlike ridge regression results in coefficient estimates that are exactly equal to zero
the formulations and can be used to shed light on the issue
figure illustrates the situation
the least squares solution is marked as while the blue diamond and circle represent the lasso and ridge regression constraints in and respectively
if is sufficiently large then the constraint regions will contain and so the ridge regression and lasso estimates will be the same as the least squares estimates
such large value of corresponds to bb in and
however in figure the least squares estimates lie outside of the diamond and the circle and so the least squares estimates are not the same as the lasso and ridge regression estimates
the ellipses that are centered around represent regions of constant rss
in other words all of the points on given ellipse share common value of the rss
as the ellipses expand away from the least squares coefficient estimates the rss increases
equations and indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region
since ridge regression has circular constraint with no sharp points this intersection will not generally occur on an axis and so the ridge regression coefficient estimates will be exclusively non zero
however the lasso constraint has corners at each of the axes and so the ellipse will often intersect the constraint region at an axis
when this occurs one of the coefficients will equal
left plots of squared bias black variance green and test mse purple for the lasso on simulated data set
right comparison of squared bias variance and test mse between lasso solid and ridge dashed
both are plotted against their on the training data as common form of indexing
the crosses in both plots indicate the lasso model for which the mse is smallest zero
in higher dimensions many of the coefficient estimates may equal zero simultaneously
in figure the intersection occurs at and so the resulting model will only include
in figure we considered the simple case of
when then the constraint region for ridge regression becomes sphere and the constraint region for the lasso becomes polyhedron
when the constraint for ridge regression becomes hypersphere and the constraint for the lasso becomes polytope
however the key ideas depicted in figure still hold
in particular the lasso leads to feature selection when due to the sharp corners of the polyhedron or polytope
comparing the lasso and ridge regression it is clear that the lasso has major advantage over ridge regression in that it produces simpler and more interpretable models that involve only subset of the predictors
however which method leads to better prediction accuracy
figure displays the variance squared bias and test mse of the lasso applied to the same simulated data as in figure
clearly the lasso leads to qualitatively similar behavior to ridge regression in that as bb increases the variance decreases and the bias increases
in the right hand panel of figure the dotted lines represent the ridge regression fits
here we plot both against their on the training data
this is another useful way to index models and can be used to compare models with different types of regularization as is the case here
in this example the lasso and ridge regression result in almost identical biases
however the variance of ridge regression is slightly lower than the variance of the lasso
left plots of squared bias black variance green and test mse purple for the lasso
the simulated data is similar to that in figure except that now only two predictors are related to the response
right comparison of squared bias variance and test mse between lasso solid and ridge dashed
both are plotted against their on the training data as common form of indexing
the crosses in both plots indicate the lasso model for which the mse is smallest
consequently the minimum mse of ridge regression is slightly smaller than that of the lasso
however the data in figure were generated in such way that all predictors were related to the response that is none of the true coefficients equaled zero
the lasso implicitly assumes that number of the coefficients truly equal zero
consequently it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting
figure illustrates similar situation except that now the response is function of only two out of predictors
now the lasso tends to outperform ridge regression in terms of bias variance and mse
these two examples illustrate that neither ridge regression nor the lasso will universally dominate the other
in general one might expect the lasso to perform better in setting where relatively small number of predictors have substantial coefficients and the remaining predictors have coefficients that are very small or that equal zero
ridge regression will perform better when the response is function of many predictors all with coefficients of roughly equal size
however the number of predictors that is related to the response is never known priori for real data sets
technique such as cross validation can be used in order to determine which approach is better on particular data set
as with ridge regression when the least squares estimates have excessively high variance the lasso solution can yield reduction in variance at the expense of small increase in bias and consequently can gener
shrinkage methods ate more accurate predictions
unlike ridge regression the lasso performs variable selection and hence results in models that are easier to interpret
there are very efficient algorithms for fitting both ridge and lasso models in both cases the entire coefficient paths can be computed with about the same amount of work as single least squares fit
we will explore this further in the lab at the end of this chapter
simple special case for ridge regression and the lasso in order to obtain better intuition about the behavior of ridge regression and the lasso consider simple special case with and diagonal matrix with on the diagonal and in all off diagonal elements
to simplify the problem further assume also that we are performing regression without an intercept
with these assumptions the usual least squares problem simplifies to finding that minimize yj
in this case the least squares solution is given by yj
and in this setting ridge regression amounts to finding such that yj bb is minimized and the lasso amounts to finding the coefficients such that yj bb is minimized
one can show that in this setting the ridge regression estimates take the form jr yj bb and the lasso estimates take the form yj bb if yj bb jl yj bb if yj bb if yj bb
figure displays the situation
we can see that ridge regression and the lasso perform two very different types of shrinkage
in ridge regression each least squares coefficient estimate is shrunken by the same proportion
the ridge regression and lasso coefficient estimates for simple setting with and diagonal matrix with on the diagonal
left the ridge regression coefficient estimates are shrunken proportionally towards zero relative to the least squares estimates
right the lasso coefficient estimates are soft thresholded towards zero
in contrast the lasso shrinks each least squares coefficient towards zero by constant amount bb the least squares coefficients that are less than bb in absolute value are shrunken entirely to zero
the type of shrinkage performed by the lasso in this simple setting is known as softthresholding
the fact that some lasso coefficients are shrunken entirely to soft thresholding zero explains why the lasso performs feature selection
in the case of more general data matrix the story is little more complicated than what is depicted in figure but the main ideas still hold approximately ridge regression more or less shrinks every dimension of the data by the same proportion whereas the lasso more or less shrinks all coefficients toward zero by similar amount and sufficiently small coefficients are shrunken all the way to zero
bayesian interpretation for ridge regression and the lasso we now show that one can view ridge regression and the lasso through bayesian lens
bayesian viewpoint for regression assumes that the coefficient vector has some prior distribution say where
the likelihood of the data can be written as where xp
multiplying the prior distribution by the likelihood gives us up to proportionality constant the posterior distribution posterior which takes the form distribution
left ridge regression is the posterior mode for under gaussian prior
right the lasso is the posterior mode for under double exponential prior where the proportionality above follows from bayes theorem and the equality above follows from the assumption that is fixed
we assume the usual linear model
xp and suppose that the errors are independent andq drawn from normal distribution
furthermore assume that pj for some density function
in fact the ridge regression solution is also the posterior mean
however the lasso solution is not the posterior mean and in fact the posterior mean does not yield sparse coefficient vector
the gaussian and double exponential priors are displayed in figure
therefore from bayesian viewpoint ridge regression and the lasso follow directly from assuming the usual linear model with normal errors together with simple prior distribution for
notice that the lasso prior is steeply peaked at zero while the gaussian is flatter and fatter at zero
hence the lasso expects priori that many of the coefficients are exactly zero while ridge assumes the coefficients are randomly distributed about zero
left cross validation errors that result from applying ridge regression to the credit data set with various value of bb
right the coefficient estimates as function of bb
the vertical dashed lines indicate the value of bb selected by cross validation
selecting the tuning parameter just as the subset selection approaches considered in section require method to determine which of the models under consideration is best implementing ridge regression and the lasso requires method for selecting value for the tuning parameter bb in and or equivalently the value of the constraint in and
cross validation provides simple way to tackle this problem
we choose grid of bb values and compute the cross validation error for each value of bb as described in chapter
we then select the tuning parameter value for which the cross validation error is smallest
finally the model is re fit using all of the available observations and the selected value of the tuning parameter
figure displays the choice of bb that results from performing leaveone out cross validation on the ridge regression fits from the credit data set
the dashed vertical lines indicate the selected value of bb
in this case the value is relatively small indicating that the optimal fit only involves small amount of shrinkage relative to the least squares solution
in addition the dip is not very pronounced so there is rather wide range of values that would give very similar error
in case like this we might simply use the least squares solution
figure provides an illustration of fold cross validation applied to the lasso fits on the sparse simulated data from figure
the left hand panel of figure displays the cross validation error while the right hand panel displays the coefficient estimates
the vertical dashed lines indicate the point at which the cross validation error is smallest
the two colored lines in the right hand panel of figure represent the two predictors that are related to the response while the grey lines represent the unrelated
left ten fold cross validation mse for the lasso applied to the sparse simulated data set from figure
right the corresponding lasso coefficient estimates are displayed
the vertical dashed lines indicate the lasso fit for which the cross validation error is smallest predictors these are often referred to as signal and noise variables respecsignal tively
not only has the lasso correctly given much larger coefficient estimates to the two signal predictors but also the minimum cross validation error corresponds to set of coefficient estimates for which only the signal variables are non zero
hence cross validation together with the lasso has correctly identified the two signal variables in the model even though this is challenging setting with variables and only observations
in contrast the least squares solution displayed on the far right of the right hand panel of figure assigns large coefficient estimate to only one of the two signal variables
dimension reduction methods the methods that we have discussed so far in this chapter have controlled variance in two different ways either by using subset of the original variables or by shrinking their coefficients toward zero
all of these methods are defined using the original predictors xp
we now explore class of approaches that transform the predictors and then fit least squares model using the transformed variables
we will refer to these techniques as dimension reduction methods dimension let zm represent linear combinations of our original reduction predictors
that is linear combination xp zm jm xj
linear model selection and regularization for some constants pm
we can then fit the linear regression model zim oi using least squares
note that in the regression coefficients are given by
if the constants pm are chosen wisely then such dimension reduction approaches can often outperform least squares regression
in other words fitting using least squares can lead to better results than fitting using least squares
the term dimension reduction comes from the fact that this approach reduces the problem of estimating the coefficients to the simpler problem of estimating the coefficients where
in other words the dimension of the problem has been reduced from to
notice that from zim jm xij jm xij xij where jm
hence can be thought of as special case of the original linear regression model given by
dimension reduction serves to constrain the estimated coefficients since now they must take the form
this constraint on the form of the coefficients has the potential to bias the coefficient estimates
however in situations where is large relative to selecting value of can significantly reduce the variance of the fitted coefficients
if and all the zm are linearly independent then poses no constraints
in this case no dimension reduction occurs and so fitting is equivalent to performing least squares on the original predictors
all dimension reduction methods work in two steps
first the transformed predictors zm are obtained
second the model is fit using these predictors
however the choice of zm or equivalently the selection of the jm can be achieved in different ways
in this chapter we will consider two approaches for this task principal components and partial least squares
principal components regression principal components analysis pca is popular approach for deriving principal components analysis
the population size pop and ad spending ad for different cities are shown as purple circles
the green solid line indicates the first principal component and the blue dashed line indicates the second principal component low dimensional set of features from large set of variables
pca is discussed in greater detail as tool for unsupervised learning in chapter
here we describe its use as dimension reduction technique for regression
an overview of principal components analysis pca is technique for reducing the dimension of data matrix
the first principal component direction of the data is that along which the observations vary the most
for instance consider figure which shows population size pop in tens of thousands of people and ad spending for particular company ad in thousands of dollars for cities
the green solid line represents the first principal component direction of the data
we can see by eye that this is the direction along which there is the greatest variability in the data
that is if we projected the observations onto this line as shown in the left hand panel of figure then the resulting projected observations would have the largest possible variance projecting the observations onto any other line would yield projected observations with lower variance
projecting point onto line simply involves finding the location on the line which is closest to the point
the first principal component is displayed graphically in figure but how can it be summarized mathematically
it is given by the formula pop pop ad ad
here and are the principal component loadings which define the direction referred to above
in pop indicates the
subset of the advertising data
the mean pop and ad budgets are indicated with blue circle
left the first principal component direction is shown in green
it is the dimension along which the data vary the most and it also defines the line that is closest to all of the observations
the distances from each observation to the principal component are represented using the black dashed line segments
the blue dot represents pop ad
right the left hand panel has been rotated so that the first principal component direction coincides with the axis mean of all pop values in this data set and ad indicates the mean of all advertising spending
the idea is that out of every possible linear combination of pop and ad such that this particular linear combination yields the highest variance this is the linear combination for which var pop pop ad ad is maximized
it is necessary to consider only linear combinations of the form since otherwise we could increase and arbitrarily in order to blow up the variance
in the two loadings are both positive and have similar size and so is almost an average of the two variables
since pop and ad are vectors of length and so is in
for instance zi popi pop adi ad
the values of zn are known as the principal component scores and can be seen in the right hand panel of figure
there is also another interpretation for pca the first principal component vector defines the line that is as close as possible to the data
for instance in figure the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line
these distances are plotted as dashed line segments in the left hand panel of figure in which the crosses represent the projection of each point onto the first principal component line
the first principal component has been chosen so that the projected observations are as close as possible to the original observations
plots of the first principal component scores zi versus pop and ad
the relationships are strong
in the right hand panel of figure the left hand panel has been rotated so that the first principal component direction coincides with the axis
it is possible to show that the first principal component score for the ith observation given in is the distance in the direction of the ith cross from zero
so for example the point in the bottom left corner of the left hand panel of figure has large negative principal component score zi while the point in the top right corner has large positive score zi
these scores can be computed directly using
we can think of the values of the principal component as singlenumber summaries of the joint pop and ad budgets for each location
in this example if zi popi pop adi ad then this indicates city with below average population size and belowaverage ad spending
positive score suggests the opposite
how well can single number represent both pop and ad
in this case figure indicates that pop and ad have approximately linear relationship and so we might expect that single number summary will work well
figure displays zi versus both pop and ad
the plots show strong relationship between the first principal component and the two features
in other words the first principal component appears to capture most of the information contained in the pop and ad predictors
so far we have concentrated on the first principal component
in general one can construct up to distinct principal components
the second principal component is linear combination of the variables that is uncorrelated with and has largest variance subject to this constraint
the second principal component direction is illustrated as dashed blue line in figure
it turns out that the zero correlation condition of with is equivalent to the condition that the direction must be perpendicular or perpendicular orthogonal to the first principal component direction
the second principal orthogonal
plots of the second principal component scores zi versus pop and ad
the relationships are weak component is given by the formula pop pop ad ad
since the advertising data has two predictors the first two principal components contain all of the information that is in pop and ad
however by construction the first component will contain the most information
consider for example the much larger variability of zi the axis versus zi the axis in the right hand panel of figure
the fact that the second principal component scores are much closer to zero indicates that this component captures far less information
as another illustration figure displays zi versus pop and ad
there is little relationship between the second principal component and these two predictors again suggesting that in this case one only needs the first principal component in order to accurately represent the pop and ad budgets
with two dimensional data such as in our advertising example we can construct at most two principal components
however if we had other predictors such as population age income level education and so forth then additional components could be constructed
they would successively maximize variance subject to the constraint of being uncorrelated with the preceding components
the principal components regression approach the principal components regression pcr approach involves constructprincipal ing the first principal components zm and then using these components components as the predictors in linear regression model that is fit us regression ing least squares
the key idea is that often small number of principal components suffice to explain most of the variability in the data as well as the relationship with the response
in other words we assume that the
pcr was applied to two simulated data sets
left simulated data from figure
right simulated data from figure directions in which xp show the most variation are the directions that are associated with
while this assumption is not guaranteed to be true it often turns out to be reasonable enough approximation to give good results
if the assumption underlying pcr holds then fitting least squares model to zm will lead to better results than fitting least squares model to xp since most or all of the information in the data that relates to the response is contained in zm and by estimating only coefficients we can mitigate overfitting
in the advertising data the first principal component explains most of the variance in both pop and ad so principal component regression that uses this single variable to predict some response of interest such as sales will likely perform quite well
figure displays the pcr fits on the simulated data sets from figures and
recall that both data sets were generated using observations and predictors
however while the response in the first data set was function of all the predictors the response in the second data set was generated using only two of the predictors
the curves are plotted as function of the number of principal components used as predictors in the regression model
as more principal components are used in the regression model the bias decreases but the variance increases
this results in typical shape for the mean squared error
when then pcr amounts simply to least squares fit using all of the original predictors
the figure indicates that performing pcr with an appropriate choice of can result in substantial improvement over least squares especially in the left hand panel
however by examining the ridge regression and lasso results in figures and we see that pcr does not perform as well as the two shrinkage methods in this example
the relatively worse performance of pcr in figure is consequence of the fact that the data were generated in such way that many princi
pcr ridge regression and the lasso were applied to simulated data set in which the first five principal components of contain all the information about the response
in each panel the irreducible error var is shown as horizontal dashed line
left results for pcr
right results for lasso solid and ridge regression dotted
the axis displays the shrinkage factor of the coefficient estimates defined as the norm of the shrunken coefficient estimates divided by the norm of the least squares estimate pal components are required in order to adequately model the response
in contrast pcr will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response
the left hand panel of figure illustrates the results from another simulated data set designed to be more favorable to pcr
here the response was generated in such way that it depends exclusively on the first five principal components
now the bias drops to zero rapidly as the number of principal components used in pcr increases
the mean squared error displays clear minimum at
the right hand panel of figure displays the results on these data using ridge regression and the lasso
all three methods offer significant improvement over least squares
however pcr and ridge regression slightly outperform the lasso
we note that even though pcr provides simple way to perform regression using predictors it is not feature selection method
this is because each of the principal components used in the regression is linear combination of all of the original features
for instance in was linear combination of both pop and ad
therefore while pcr often performs quite well in many practical settings it does not result in the development of model that relies upon small set of the original features
in this sense pcr is more closely related to ridge regression than to the lasso
in fact one can show that pcr and ridge regression are very closely
left pcr standardized coefficient estimates on the credit data set for different values of
right the fold cross validation mse obtained using pcr as function of related
one can even think of ridge regression as continuous version of pcr
in pcr the number of principal components is typically chosen by cross validation
the results of applying pcr to the credit data set are shown in figure the right hand panel displays the cross validation errors obtained as function of
on these data the lowest cross validation error occurs when there are components this corresponds to almost no dimension reduction at all since pcr with is equivalent to simply performing least squares
when performing pcr we generally recommend standardizing each predictor using prior to generating the principal components
this standardization ensures that all variables are on the same scale
in the absence of standardization the high variance variables will tend to play larger role in the principal components obtained and the scale on which the variables are measured will ultimately have an effect on the final pcr model
however if the variables are all measured in the same units say kilograms or inches of annual rainfall then one might choose not to standardize them
partial least squares the pcr approach that we just described involves identifying linear combinations or directions that best represent the predictors xp
these directions are identified in an unsupervised way since the response is not more details can be found in section of elements of statistical learning by hastie tibshirani and friedman
for the advertising data the first pls direction solid line and first pcr direction dotted line are shown used to help determine the principal component directions
that is the response does not supervise the identification of the principal components
consequently pcr suffers from drawback there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response
unsupervised methods are discussed further in chapter
we now present partial least squares pls supervised alternative to partial least pcr
like pcr pls is dimension reduction method which first identifies squares new set of features zm that are linear combinations of the original features and then fits linear model via least squares using these new features
but unlike pcr pls identifies these new features in supervised way that is it makes use of the response in order to identify new features that not only approximate the old features well but also that are related to the response
roughly speaking the pls approach attempts to find directions that help explain both the response and the predictors
we now describe how the first pls direction is computed
after standardizing the predictors pls computes the first direction by setting each in equal to the coefficient from the simple linear regression of onto xj
one can show that this coefficient is proportional pp to the correlation between and xj
hence in computing xj pls places the highest weight on the variables that are most strongly related to the response
figure displays an example of pls on the advertising data
the solid green line indicates the first pls direction while the dotted line shows the first principal component direction
pls has chosen direction that has less change in the ad dimension per unit change in the pop dimension relative
considerations in high dimensions to pca
this suggests that pop is more highly correlated with the response than is ad
the pls direction does not fit the predictors as closely as does pca but it does better job explaining the response
to identify the second pls direction we first adjust each of the variables for by regressing each variable on and taking residuals
these residuals can be interpreted as the remaining information that has not been explained by the first pls direction
we then compute using this orthogonalized data in exactly the same fashion as was computed based on the original data
this iterative approach can be repeated times to identify multiple pls components zm
finally at the end of this procedure we use least squares to fit linear model to predict using zm in exactly the same fashion as for pcr
as with pcr the number of partial least squares directions used in pls is tuning parameter that is typically chosen by cross validation
we generally standardize the predictors and response before performing pls
pls is popular in in the field of chemometrics where many variables arise from digitized spectrometry signals
in practice it often performs no better than ridge regression or pcr
while the supervised dimension reduction of pls can reduce bias it also has the potential to increase variance so that the overall benefit of pls relative to pcr is wash
considerations in high dimensions high dimensional data most traditional statistical techniques for regression and classification are intended for the low dimensional setting in which the number of oblow dimensional servations is much greater than the number of features
this is due in part to the fact that throughout most of the field's history the bulk of scientific problems requiring the use of statistics have been low dimensional
for instance consider the task of developing model to predict patient's blood pressure on the basis of his or her age gender and body mass index bmi
there are three predictors or four if an intercept is included in the model and perhaps several thousand patients for whom blood pressure and age gender and bmi are available
hence and so the problem is low dimensional
by dimension here we are referring to the size of
in the past twenty years new technologies have changed the way that data are collected in fields as diverse as finance marketing and medicine
it is now commonplace to collect an almost unlimited number of feature measurements very large
while can be extremely large the number of observations is often limited due to cost sample availability or other considerations
two examples are as follows
linear model selection and regularization
rather than predicting blood pressure on the basis of just age gender and bmi one might also collect measurements for half million single nucleotide polymorphisms snps these are individual dna mutations that are relatively common in the population for inclusion in the predictive model
then and
marketing analyst interested in understanding people's online shopping patterns could treat as features all of the search terms entered by users of search engine
this is sometimes known as the bag ofwords model
the same researcher might have access to the search histories of only few hundred or few thousand search engine users who have consented to share their information with the researcher
for given user each of the search terms is scored present or absent creating large binary feature vector
then and is much larger
data sets containing more features than observations are often referred to as high dimensional
classical approaches such as least squares linear high dimensional regression are not appropriate in this setting
many of the issues that arise in the analysis of high dimensional data were discussed earlier in this book since they apply also when these include the role of the bias variance trade off and the danger of overfitting
though these issues are always relevant they can become particularly important when the number of features is very large relative to the number of observations
we have defined the high dimensional setting as the case where the number of features is larger than the number of observations
but the considerations that we will now discuss certainly also apply if is slightly smaller than and are best always kept in mind when performing supervised learning
what goes wrong in high dimensions
in order to illustrate the need for extra care and specialized techniques for regression and classification when we begin by examining what can go wrong if we apply statistical technique not intended for the highdimensional setting
for this purpose we examine least squares regression
but the same concepts apply to logistic regression linear discriminant analysis and other classical statistical approaches
when the number of features is as large as or larger than the number of observations least squares as described in chapter cannot or rather should not be performed
the reason is simple regardless of whether or not there truly is relationship between the features and the response least squares will yield set of coefficient estimates that result in perfect fit to the data such that the residuals are zero
an example is shown in figure with feature plus an intercept in two cases when there are observations and when there are only
left least squares regression in the low dimensional setting
right least squares regression with observations and two parameters to be estimated an intercept and coefficient two observations
when there are observations and the least squares regression line does not perfectly fit the data instead the regression line seeks to approximate the observations as well as possible
on the other hand when there are only two observations then regardless of the values of those observations the regression line will fit the data exactly
this is problematic because this perfect fit will almost certainly lead to overfitting of the data
in other words though it is possible to perfectly fit the training data in the high dimensional setting the resulting linear model will perform extremely poorly on an independent test set and therefore does not constitute useful model
in fact we can see that this happened in figure the least squares line obtained in the right hand panel will perform very poorly on test set comprised of the observations in the lefthand panel
the problem is simple when or simple least squares regression line is too flexible and hence overfits the data
figure further illustrates the risk of carelessly applying least squares when the number of features is large
data were simulated with observations and regression was performed with between and features each of which was completely unrelated to the response
as shown in the figure the model increases to as the number of features included in the model increases and correspondingly the training set mse decreases to as the number of features increases even though the features are completely unrelated to the response
on the other hand the mse on an independent test set becomes extremely large as the number of features included in the model increases because including the additional predictors leads to vast increase in the variance of the coefficient estimates
looking at the test set mse it is clear that the best model contains at most few variables
however someone who carelessly examines only the or the training set
on simulated example with training observations features that are completely unrelated to the outcome are added to the model
left the increases to as more features are included
center the training set mse decreases to as more features are included
right the test set mse increases as more features are included
mse might erroneously conclude that the model with the greatest number of variables is best
this indicates the importance of applying extra care when analyzing data sets with large number of variables and of always evaluating model performance on an independent test set
in section we saw number of approaches for adjusting the training set rss or in order to account for the number of variables used to fit least squares model
unfortunately the cp aic and bic approaches are not appropriate in the high dimensional setting because estimating is problematic
for instance the formula for from chapter yields an estimate in this setting
similarly problems arise in the application of adjusted in the high dimensional setting since one can easily obtain model with an adjusted value of
clearly alternative approaches that are better suited to the high dimensional setting are required
regression in high dimensions it turns out that many of the methods seen in this chapter for fitting less flexible least squares models such as forward stepwise selection ridge regression the lasso and principal components regression are particularly useful for performing regression in the high dimensional setting
essentially these approaches avoid overfitting by using less flexible fitting approach than least squares
figure illustrates the performance of the lasso in simple simulated example
there are or features of which are truly associated with the outcome
the lasso was performed on training observations and the mean squared error was evaluated on an independent test set
as the number of features increases the test set error increases
when the lowest validation set error was achieved when bb in
the lasso was performed with observations and three values of the number of features
of the features were associated with the response
the boxplots show the test mses that result using three different values of the tuning parameter bb in
for ease of interpretation rather than reporting bb the degrees of freedom are reported for the lasso this turns out to be simply the number of estimated non zero coefficients
when the lowest test mse was obtained with the smallest amount of regularization
when the lowest test mse was achieved when there is substantial amount of regularization
when the lasso performed poorly regardless of the amount of regularization due to the fact that only of the features truly are associated with the outcome
was small however when was larger then the lowest validation set error was achieved using larger value of bb
in each boxplot rather than reporting the values of bb used the degrees of freedom of the resulting lasso solution is displayed this is simply the number of non zero coefficient estimates in the lasso solution and is measure of the flexibility of the lasso fit
figure highlights three important points regularization or shrinkage plays key role in high dimensional problems appropriate tuning parameter selection is crucial for good predictive performance and the test error tends to increase as the dimensionality of the problem the number of features or predictors increases unless the additional features are truly associated with the response
the third point above is in fact key principle in the analysis of highdimensional data which is known as the curse of dimensionality
one might curse of think that as the number of features used to fit model increases the dimensionality quality of the fitted model will increase as well
however comparing the left hand and right hand panels in figure we see that this is not necessarily the case in this example the test set mse almost doubles as increases from to
in general adding additional signal features that are truly associated with the response will improve the fitted model
linear model selection and regularization in the sense of leading to reduction in test set error
however adding noise features that are not truly associated with the response will lead to deterioration in the fitted model and consequently an increased test set error
this is because noise features increase the dimensionality of the problem exacerbating the risk of overfitting since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set without any potential upside in terms of improved test set error
thus we see that new technologies that allow for the collection of measurements for thousands or millions of features are double edged sword they can lead to improved predictive models if these features are in fact relevant to the problem at hand but will lead to worse results if the features are not relevant
even if they are relevant the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring
interpreting results in high dimensions when we perform the lasso ridge regression or other regression procedures in the high dimensional setting we must be quite cautious in the way that we report the results obtained
in chapter we learned about multicollinearity the concept that the variables in regression might be correlated with each other
in the high dimensional setting the multicollinearity problem is extreme any variable in the model can be written as linear combination of all of the other variables in the model
essentially this means that we can never know exactly which variables if any truly are predictive of the outcome and we can never identify the best coefficients for use in the regression
at most we can hope to assign large regression coefficients to variables that are correlated with the variables that truly are predictive of the outcome
for instance suppose that we are trying to predict blood pressure on the basis of half million snps and that forward stepwise selection indicates that seventeen of those snps lead to good predictive model on the training data
it would be incorrect to conclude that these seventeen snps predict blood pressure more effectively than the other snps not included in the model
there are likely to be many sets of seventeen snps that would predict blood pressure just as well as the selected model
if we were to obtain an independent data set and perform forward stepwise selection on that data set we would likely obtain model containing different and perhaps even non overlapping set of snps
this does not detract from the value of the model obtained for instance the model might turn out to be very effective in predicting blood pressure on an independent set of patients and might be clinically useful for physicians
but we must be careful not to overstate the results obtained and to make it clear that what we have identified is simply one of many possible models for predicting blood pressure and that it must be further validated on independent data sets
lab subset selection methods it is also important to be particularly careful in reporting errors and measures of model fit in the high dimensional setting
we have seen that when it is easy to obtain useless model that has zero residuals
therefore one should never use sum of squared errors values statistics or other traditional measures of model fit on the training data as evidence of good model fit in the high dimensional setting
for instance as we saw in figure one can easily obtain model with when
reporting this fact might mislead others into thinking that statistically valid and useful model has been obtained whereas in fact this provides absolutely no evidence of compelling model
it is important to instead report results on an independent test set or cross validation errors
for instance the mse or on an independent test set is valid measure of model fit but the mse on the training set certainly is not
lab subset selection methods best subset selection here we apply the best subset selection approach to the hitters data
we wish to predict baseball player's salary on the basis of various statistics associated with performance in the previous year
first of all we note that the salary variable is missing for some of the players
the is na function can be used to identify the missing observais na tions
it returns vector of the same length as the input vector with true for any elements that are missing and false for non missing elements
the sum function can then be used to count all of the missing elements sum library islr fix hitters names hitters atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks league division putouts assists errors salary newleague dim hitters sum is na hence we see that salary is missing for players
the na omit function removes all of the rows that have missing values in any variable
hitters na omit hitters dim hitters sum is na hitters the regsubsets function part of the leaps library performs best subregsubsets
linear model selection and regularization set selection by identifying the best model that contains given number of predictors where best is quantified using rss
the syntax is the same as for lm
the summary command outputs the best set of variables for each model size library leaps regfit full regsubsets salary hitters summary regfit full subset selection object call regsubset formula salary hitters variables and intercept subsets of each size up to selection algorithm exhaustiv atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleague an asterisk indicates that given variable is included in the corresponding model
for instance this output indicates that the best two variable model contains only hits and crbi
by default regsubsets only reports results up to the best eight variable model
but the nvmax option can be used in order to return as many variables as are desired
here we fit up to nineteen variable model regfit full regsubsets salary data hitters nvmax reg summary summary regfit full the summary function also returns rss adjusted cp and bic
we can examine these to try to select the best overall model
lab subset selection methods names reg summary which rsq rss adjr cp bic outmat obj for instance we see that the statistic increases from when only one variable is included in the model to almost when all variables are included
as expected the statistic increases monotonically as more variables are included reg summary plotting rss adjusted cp and bic for all of the models at once will help us decide which model to select
note the type option tells to connect the plotted points with lines par mfrow plot reg summary rss xlab number of variables ylab rss type plot reg summary adjr xlab number of variables ylab adjusted rsq type the points command works like the plot command except that it points puts points on plot that has already been created instead of creating new plot
the which max function can be used to identify the location of the maximum point of vector
we will now plot red dot to indicate the model with the largest adjusted statistic which max reg points reg col red cex pch in similar fashion we can plot the cp and bic statistics and indicate the models with the smallest statistic using which min which min plot reg summary cp xlab number of variables ylab cp type which min reg summary cp points reg summary cp col red cex pch which min reg summary bi plot reg summary bic xlab number of variables ylab bic type points reg summary col red cex pch the regsubsets function has built in plot command which can be used to display the selected variables for the best model with given number of predictors ranked according to the bic cp adjusted or aic
to find out more about this function type
plot regsubsets plot regfit full scale
linear model selection and regularization plot regfit full scale adjr plot regfit full scale cp plot regfit full scale bic the top row of each plot contains black square for each variable selected according to the optimal model associated with that statistic
for instance we see that several models share bic close to
however the model with the lowest bic is the six variable model that contains only atbat hits walks crbi divisionw and putouts
we can use the coef function to see the coefficient estimates associated with this model coef regfit full intercept atbat hits walks crbi divisionw putouts forward and backward stepwise selection we can also use the regsubsets function to perform forward stepwise or backward stepwise selection using the argument method forward or method backward regfit fwd regsubsets salary data hitters nvmax method forward summary regfit fwd regfit bwd regsubsets salary data hitters nvmax method backward summary regfit bwd for instance we see that using forward stepwise selection the best onevariable model contains only crbi and the best two variable model additionally includes hits
for this data the best one variable through sixvariable models are each identical for best subset and forward selection
however the best seven variable models identified by forward stepwise selection backward stepwise selection and best subset selection are different coef regfit full intercept hits walks catbat chits chmrun divisionw putouts coef regfit fwd intercept atbat hits walks crbi cwalks divisionw putouts coef regfit bwd intercept atbat hits walks cruns cwalks divisionw putouts
lab subset selection methods choosing among models using the validation set approach and cross validation we just saw that it is possible to choose among set of models of different sizes using cp bic and adjusted
we will now consider how to do this using the validation set and cross validation approaches
in order for these approaches to yield accurate estimates of the test error we must use only the training observations to perform all aspects of model fitting including variable selection
therefore the determination of which model of given size is best must be made using only the training observations
this point is subtle but important
if the full data set is used to perform the best subset selection step the validation set errors and cross validation errors that we obtain will not be accurate estimates of the test error
in order to use the validation set approach we begin by splitting the observations into training set and test set
we do this by creating random vector train of elements equal to true if the corresponding observation is in the training set and false otherwise
the vector test has true if the observation is in the test set and false otherwise
note the
in the command to create test causes trues to be switched to falses and vice versa
we also set random seed so that the user will obtain the same training set test set split set seed train sample true false nrow hitters rep true test
train now we apply regsubsets to the training set in order to perform best subset selection regfit best regsubsets salary data hitters train nvmax notice that we subset the hitters data frame directly in the call in order to access only the training subset of the data using the expression hitters train
we now compute the validation set error for the best model of each model size
we first make model matrix from the test data test mat model matrix salary data hitters test the model matrix function is used in many regression packages for buildmodel matrix ing an matrix from data
now we run loop and for each size we extract the coefficients from regfit best for the best model of that size multiply them into the appropriate columns of the test model matrix to form the predictions and compute the test mse val errors rep na for in coefi coef regfit best id
linear model selection and regularization pred test mat names coefi coefi val errors mean test pred we find that the best model is the one that contains ten variables val errors which min val errors coef regfit best intercept atbat hits walks catbat chits chmrun cwalks leaguen divisionw putouts this was little tedious partly because there is no predict method for regsubsets
since we will be using this function again we can capture our steps above and write our own predict method predict regsubsets function object newdata id form as formula object cal mat model matrix form newdata coefi coef object id id xvars names coefi mat xvars coefi our function pretty much mimics what we did above
the only complex part is how we extracted the formula used in the call to regsubsets
we demonstrate how we use this function below when we do cross validation
finally we perform best subset selection on the full data set and select the best ten variable model
it is important that we make use of the full data set in order to obtain more accurate coefficient estimates
note that we perform best subset selection on the full data set and select the best tenvariable model rather than simply using the variables that were obtained from the training set because the best ten variable model on the full data set may differ from the corresponding model on the training set regfit best regsubsets salary data hitters nvmax coef regfit best intercept atbat hits walks catbat cruns crbi cwalks divisionw putouts assists
lab subset selection methods in fact we see that the best ten variable model on the full data set has different set of variables than the best ten variable model on the training set
we now try to choose among the models of different sizes using crossvalidation
this approach is somewhat involved as we must perform best subset selection within each of the training sets
despite this we see that with its clever subsetting syntax makes this job quite easy
first we create vector that allocates each observation to one of folds and we create matrix in which we will store the results set seed folds sample nrow hitters replace true cv errors matrix na dimnames list null paste now we write for loop that performs cross validation
in the jth fold the elements of folds that equal are in the test set and the remainder are in the training set
we make our predictions for each model size using our new predict method compute the test errors on the appropriate subset and store them in the appropriate slot in the matrix cv errors for in best fit regsubset salary data hitters folds
nvmax for in pred predict best fit hitters folds id cv errors mean folds pred this has given us matrix of which the th element corresponds to the test mse for the ith cross validation fold for the best variable model
we use the apply function to average over the columns of this apply matrix in order to obtain vector for which the jth element is the crossvalidation error for the variable model mean cv errors apply cv errors mean mean cv errors par mfrow plot mean cv errors type we see that cross validation selects an eleven variable model
we now perform best subset selection on the full data set in order to obtain the elevenvariable model reg best regsubsets salary data hitters nvmax coef reg best intercept atbat hits walks catbat
linear model selection and regularization cruns crbi cwalks leaguen divisionw putouts assists lab ridge regression and the lasso we will use the glmnet package in order to perform ridge regression and the lasso
the main function in this package is glmnet which can be used glmnet to fit ridge regression models lasso models and more
this function has slightly different syntax from other model fitting functions that we have encountered thus far in this book
in particular we must pass in an matrix as well as vector and we do not use the syntax
we will now perform ridge regression and the lasso in order to predict salary on the hitters data
before proceeding ensure that the missing values have been removed from the data as described in section model matrix salary hitters hitters salary the model matrix function is particularly useful for creating not only does it produce matrix corresponding to the predictors but it also automatically transforms any qualitative variables into dummy variables
the latter property is important because glmnet can only take numerical quantitative inputs
ridge regression the glmnet function has an alpha argument that determines what type of model is fit
if alpha then ridge regression model is fit and if alpha then lasso model is fit
we first fit ridge regression model library glmnet grid seq length ridge mod glmnet alpha lambda grid by default the glmnet function performs ridge regression for an automatically selected range of bb values
however here we have chosen to implement the function over grid of values ranging from bb to bb essentially covering the full range of scenarios from the null model containing only the intercept to the least squares fit
as we will see we can also compute model fits for particular value of bb that is not one of the original grid values
note that by default the glmnet function standardizes the variables so that they are on the same scale
to turn off this default setting use the argument standardize false
associated with each value of bb is vector of ridge regression coefficients stored in matrix that can be accessed by coef
in this case it is
lab ridge regression and the lasso matrix with rows one for each predictor plus an intercept and columns one for each value of bb dim coef ridge mod we expect the coefficient estimates to be much smaller in terms of norm when large value of bb is used as compared to when small value of bb is used
these are the coefficients when bb along with their norm ridge mod lambda coef ridge mod intercept atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleaguen sqrt sum coef ridge mod in contrast here are the coefficients when bb along with their norm
note the much larger norm of the coefficients associated with this smaller value of bb ridge mod lambda coef ridge mod intercept atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleaguen sqrt sum coef ridge mod we can use the predict function for number of purposes
for instance we can obtain the ridge regression coefficients for new value of bb say predict ridge mod type intercept atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleaguen
linear model selection and regularization we now split the samples into training set and test set in order to estimate the test error of ridge regression and the lasso
there are two common ways to randomly split data set
the first is to produce random vector of true false elements and select the observations corresponding to true for the training data
the second is to randomly choose subset of numbers between and these can then be used as the indices for the training observations
the two approaches work equally well
we used the former method in section
here we demonstrate the latter approach
we first set random seed so that the results obtained will be reproducible set seed train sample nrow nrow test train test test next we fit ridge regression model on the training set and evaluate its mse on the test set using bb
note the use of the predict function again
this time we get predictions for test set by replacing type coefficients with the newx argument ridge mod glmnet train train alpha lambda grid thresh ridge pred predict ridge mod newx test mean ridge pred test the test mse is
note that if we had instead simply fit model with just an intercept we would have predicted each test observation using the mean of the training observations
in that case we could compute the test set mse like this mean mean train test we could also get the same result by fitting ridge regression model with very large value of bb
note that means ridge pred predict ridge mod newx test mean ridge pred test so fitting ridge regression model with bb leads to much lower test mse than fitting model with just an intercept
we now check whether there is any benefit to performing ridge regression with bb instead of just performing least squares regression
recall that least squares is simply ridge regression with bb in order for glmnet to yield the exact least squares coefficients when bb we use the argument exact when calling the predict function
otherwise the predict function will interpolate over the grid of bb values used in fitting the glmnet model
lab ridge regression and the lasso ridge pred predict ridge mod newx test exact mean ridge pred test lm subset train predict ridge mod exact type in general if we want to fit unpenalized least squares model then we should use the lm function since that function provides more useful outputs such as standard errors and values for the coefficients
in general instead of arbitrarily choosing bb it would be better to use cross validation to choose the tuning parameter bb
we can do this using the built in cross validation function cv glmnet
by default the function cv glmnet performs ten fold cross validation though this can be changed using the argument folds
note that we set random seed first so our results will be reproducible since the choice of the cross validation folds is random set seed cv out cv glmnet train train alpha plot cv out bestlam cv out lambda min bestlam therefore we see that the value of bb that results in the smallest crossvalidation error is
what is the test mse associated with this value of bb
ridge pred predict ridge mod bestlam newx test mean ridge pred test this represents further improvement over the test mse that we got using bb
finally we refit our ridge regression model on the full data set using the value of bb chosen by cross validation and examine the coefficient estimates out glmnet alpha predict out type bestlam intercept atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleaguen as expected none of the coefficients are zero ridge regression does not perform variable selection
yielding approximate results
when we use exact there remains slight discrepancy in the third decimal place between the output of glmnet when bb and the output of lm this is due to numerical approximation on the part of glmnet
linear model selection and regularization the lasso we saw that ridge regression with wise choice of bb can outperform least squares as well as the null model on the hitters data set
we now ask whether the lasso can yield either more accurate or more interpretable model than ridge regression
in order to fit lasso model we once again use the glmnet function however this time we use the argument alpha
other than that change we proceed just as we did in fitting ridge model lasso mod glmnet train train alpha lambda grid plot lasso mod we can see from the coefficient plot that depending on the choice of tuning parameter some of the coefficients will be exactly equal to zero
we now perform cross validation and compute the associated test error set seed cv out cv glmnet train train alpha plot cv out bestlam cv out lambda min lasso pred predict lasso mod bestlam newx test mean lasso pred test this is substantially lower than the test set mse of the null model and of least squares and very similar to the test mse of ridge regression with bb chosen by cross validation
however the lasso has substantial advantage over ridge regression in that the resulting coefficient estimates are sparse
here we see that twelve of the nineteen coefficient estimates are exactly zero
so the lasso model with bb chosen by cross validation contains only seven variables out glmnet alpha lambda grid lasso coef predict out type bestlam lasso coef intercept atbat hits hmrun runs rbi walks years catbat chits chmrun cruns crbi cwalks leaguen divisionw putouts assists errors newleaguen lasso coef lasso coef
intercept hits walks cruns crbi leaguen divisionw putouts lab pcr and pls regression
lab pcr and pls regression principal components regression principal components regression pcr can be performed using the pcr pcr function which is part of the pls library
we now apply pcr to the hitters data in order to predict salary
again ensure that the missing values have been removed from the data as described in section library pls set seed pcr fit pcr salary data hitters scale true validatio cv the syntax for the pcr function is similar to that for lm with few additional options
setting scale true has the effect of standardizing each predictor using prior to generating the principal components so that the scale on which each variable is measured will not have an effect
setting validation cv causes pcr to compute the fold cross validation error for each possible value of the number of principal components used
the resulting fit can be examined using summary summary pcr fit data dimension dimension fit method svdpc number of components considere validation rmsep cross validated using random segments
intercept comps comps comps comps cv adjcv
training variance explained comps comps comps comps comps comps salary
the cv score is provided for each possible number of components ranging from onwards
we have printed the cv output only up to
note that pcr reports the root mean squared error in order to obtain the usual mse we must square this quantity
for instance root mean squared error of corresponds to an mse of
one can also plot the cross validation scores using the validationplot validationplot function
using val type msep will cause the cross validation mse to be plotted pcr fit val type msep we see that the smallest cross validation error occurs when components are used
this is barely fewer than which amounts to
linear model selection and regularization simply performing least squares because when all of the components are used in pcr no dimension reduction occurs
however from the plot we also see that the cross validation error is roughly the same when only one component is included in the model
this suggests that model that uses just small number of components might suffice
the summary function also provides the percentage of variance explained in the predictors and in the response using different numbers of components
this concept is discussed in greater detail in chapter
briefly we can think of this as the amount of information about the predictors or the response that is captured using principal components
for example setting only captures of all the variance or information in the predictors
in contrast using increases the value to
if we were to use all components this would increase to
we now perform pcr on the training data and evaluate its test set performance set seed pcr fit pcr salary data hitters subset train scale true validatio cv pcr fit val type msep now we find that the lowest cross validation error occurs when component are used
we compute the test mse as follows pcr pred predict pcr fit test ncomp mean pcr pred test this test set mse is competitive with the results obtained using ridge regression and the lasso
however as result of the way pcr is implemented the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates
finally we fit pcr on the full data set using the number of components identified by cross validation pcr fit pcr scale true ncomp summary pcr fit data dimension dimension fit method svdpc number of components considere training variance explained comps comps comps comps comps comps comps
lab pcr and pls regression partial least squares we implement partial least squares pls using the plsr function also plsr in the pls library
the syntax is just like that of the pcr function set seed pls fit plsr salary data hitters subset train scale true validatio cv summary pls fit data dimension dimension fit method kernelpls number of components considere validation rmsep cross validated using random segments
intercept comps comps comps comps cv adjcv
training variance explained comps comps comps comps comps comps salary pls fit val type msep the lowest cross validation error occurs when only partial least squares directions are used
we now evaluate the corresponding test set mse pls pred predict pls fit test ncomp mean pls pred test the test mse is comparable to but slightly higher than the test mse obtained using ridge regression the lasso and pcr
finally we perform pls using the full data set using the number of components identified by cross validation pls fit plsr salary data hitters scale true ncomp summary pls fit data dimension dimension fit method kernelpls number of components considere training variance explained comps comps salary notice that the percentage of variance in salary that the two component pls fit explains is almost as much as that explained using the
linear model selection and regularization final seven component model pcr fit
this is because pcr only attempts to maximize the amount of variance explained in the predictors while pls searches for directions that explain variance in both the predictors and the response
exercises conceptual
we perform best subset forward stepwise and backward stepwise selection on single data set
for each approach we obtain models containing predictors
explain your answers which of the three models with predictors has the smallest training rss
which of the three models with predictors has the smallest test rss
true or false
the predictors in the variable model identified by forward stepwise are subset of the predictors in the variable model identified by forward stepwise selection ii
the predictors in the variable model identified by backward stepwise are subset of the predictors in the variable model identified by backward stepwise selection iii
the predictors in the variable model identified by backward stepwise are subset of the predictors in the variable model identified by forward stepwise selection iv
the predictors in the variable model identified by forward stepwise are subset of the predictors in the variable model identified by backward stepwise selection
the predictors in the variable model identified by best subset are subset of the predictors in the variable model identified by best subset selection
for parts through indicate which of through iv is correct
justify your answer the lasso relative to least squares is
more flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance ii
more flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias
exercises iii
less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance iv
less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias repeat for ridge regression relative to least squares repeat for non linear methods relative to least squares
suppose we estimate the regression coefficients in linear regression model by minimizing eb xn xp ed xij subject to for particular value of
for parts through indicate which of through is correct
justify your answer as we increase from the training rss will
increase initially and then eventually start decreasing in an inverted shape ii
decrease initially and then eventually start increasing in shape iii
steadily increase iv
steadily decrease
remain constant repeat for test rss repeat for variance repeat for squared bias repeat for bayes error rate
suppose we estimate the regression coefficients in linear regression model by minimizing eb xn xp ed xij bb for particular value of bb
for parts through indicate which of through is correct
justify your answer as we increase bb from the training rss will
linear model selection and regularization
increase initially and then eventually start decreasing in an inverted shape ii
decrease initially and then eventually start increasing in shape iii
steadily increase iv
steadily decrease
remain constant repeat for test rss repeat for variance repeat for squared bias repeat for bayes error rate
it is well known that ridge regression tends to give similar coefficient values to correlated variables whereas the lasso may give quite different coefficient values to correlated variables
we will now explore this property in very simple setting
suppose that
furthermore suppose that and and so that the estimate for the intercept in least squares ridge regression or lasso model is zero write out the ridge regression optimization problem in this setting argue that in this setting the ridge coefficient estimates satisfy write out the lasso optimization problem in this setting argue that in this setting the lasso coefficients and are not unique in other words there are many possible solutions to the optimization problem in
describe these solutions
we will now explore and further consider with
for some choice of and bb plot as function of
your plot should confirm that is solved by consider with
for some choice of and bb plot as function of
your plot should confirm that is solved by
we will now derive the bayesian connection to the lasso and ridge regression discussed in section
exercises pp suppose that yi xij oi where on are independent and identically distributed from distribution
write out the likelihood for the data assume the following prior for are independent and identically distributed according to double exponential distribution with mean and common scale parameter
write out the posterior for in this setting argue that the lasso estimate is the mode for under this posterior distribution now assume the following prior for are independent and identically distributed according to normal distribution with mean zero and variance
write out the posterior for in this setting argue that the ridge regression estimate is the mode or the mean for under this posterior distribution
in this exercise we will generate simulated data and will then use this data to perform best subset selection use the rnorm function to generate predictor of length as well as noise vector of length generate response vector of length according to the model where and are constants of your choice use the regsubsets function to perform best subset selection in order to choose the best model containing the predictors
what is the best model obtained according to cp bic aic and adjusted
show some plots to provide evidence for your answer and report the coefficients of the best model obtained repeat using forward stepwise selection and also using backwards stepwise selection
how does your answer compare to the results in
now fit lasso model to the simulated data again using as predictors
use cross validation to select the optimal value of bb
create plots of the cross validation error as function of bb
report the resulting coefficient estimates and discuss the results obtained
linear model selection and regularization now generate response vector according to the model and perform best subset selection and the lasso
discuss the results obtained
in this exercise we will predict the number of applications received using the other variables in the college data set split the data set into training set and test set fit linear model using least squares on the training set and report the test error obtained fit ridge regression model on the training set with bb chosen by cross validation
report the test error obtained fit lasso model on the training set with bb chosen by crossvalidation
report the test error obtained along with the number of non zero coefficient estimates fit pcr model on the training set with chosen by crossvalidation
report the test error obtained along with the value of selected by cross validation fit pls model on the training set with chosen by crossvalidation
report the test error obtained along with the value of selected by cross validation comment on the results obtained
how accurately can we predict the number of college applications received
is there much difference among the test errors resulting from these five approaches
we have seen that as the number of features used in model increases the training error will necessarily decrease but the test error may not
we will now explore this in simulated data set generate data set with features observations and an associated quantitative response vector generated according to the model where has some elements that are exactly equal to zero split your data set into training set containing observations and test set containing observations perform best subset selection on the training set and plot the training set mse associated with the best model of each size
exercises plot the test set mse associated with the best model of each size for which model size does the test set mse take on its minimum value
comment on your results
if it takes on its minimum value for model containing only an intercept or model containing all of the features then play around with the way that you are generating the data in until you come up with scenario in which the test set mse is minimized for an intermediate model size how does the model at which the test set mse is minimized compare to the true model used to generate the data
comment on the coefficient values qp create plot displaying for range of values of where jr is the jth coefficient estimate for the best model containing coefficients
comment on what you observe
how does this compare to the test mse plot from
we will now try to predict per capita crime rate in the boston data set try out some of the regression methods explored in this chapter such as best subset selection the lasso ridge regression and pcr
present and discuss results for the approaches that you consider propose model or set of models that seem to perform well on this data set and justify your answer
make sure that you are evaluating model performance using validation set error crossvalidation or some other reasonable alternative as opposed to using training error does your chosen model involve all of the features in the data set
why or why not
linear model selection and regularization
this is page printer opaque this moving beyond linearity so far in this book we have mostly focused on linear models
linear models are relatively simple to describe and implement and have advantages over other approaches in terms of interpretation and inference
however standard linear regression can have significant limitations in terms of predictive power
this is because the linearity assumption is almost always an approximation and sometimes poor one
in chapter we see that we can improve upon least squares using ridge regression the lasso principal components regression and other techniques
in that setting the improvement is obtained by reducing the complexity of the linear model and hence the variance of the estimates
but we are still using linear model which can only be improved so far
in this chapter we relax the linearity assumption while still attempting to maintain as much interpretability as possible
we do this by examining very simple extensions of linear models like polynomial regression and step functions as well as more sophisticated approaches such as splines local regression and generalized additive models
for example cubic regression uses three variables and as predictors
this approach provides simple way to provide nonlinear fit to data
this has the effect of fitting piecewise constant function
they involve dividing the range of into distinct regions
within each region polynomial function is fit to the data
however these polynomials are constrained so that they join smoothly at the region boundaries or knots
provided that the interval is divided into enough regions this can produce an extremely flexible fit
smoothing splines result from minimizing residual sum of squares criterion subject to smoothness penalty
the regions are allowed to overlap and indeed they do so in very smooth way
in sections we present number of approaches for modeling the relationship between response and single predictor in flexible way
in section we show that these approaches can be seamlessly integrated in order to model response as function of several predictors xp
polynomial regression historically the standard way to extend linear regression to settings in which the relationship between the predictors and the response is nonlinear has been to replace the standard linear model yi xi oi with polynomial function yi xi
xdi oi where oi is the error term
this approach is known as polynomial regression polynomial and in fact we saw an example of this method in section
for large regression enough degree polynomial regression allows us to produce an extremely non linear curve
notice that the coefficients in can be easily estimated using least squares linear regression because this is just standard linear model with predictors xi xdi
generally speaking it is unusual to use greater than or because for large values of the polynomial curve can become overly flexible and can take on some very strange shapes
this is especially true near the boundary of the variable
the wage data
left the solid blue curve is degree polynomial of wage in thousands of dollars as function of age fit by least squares
the dotted curves indicate an estimated confidence interval
right we model the binary event wage using logistic regression again with degree polynomial
the fitted posterior probability of wage exceeding is shown in blue along with an estimated confidence interval
the left hand panel in figure is plot of wage against age for the wage data set which contains income and demographic information for males who reside in the central atlantic region of the united states
we see the results of fitting degree polynomial using least squares solid blue curve
even though this is linear regression model like any other the individual coefficients are not of particular interest
instead we look at the entire fitted function across grid of values for age from to in order to understand the relationship between age and wage
in figure pair of dotted curves accompanies the fit these are standard error curves
let's see how these arise
suppose we have computed the fit at particular value of age
what is the variance of the fit
varf
least squares returns variance estimates for each of the fitted coefficients as well as the covariances between pairs of coefficient estimates
we can use these to compute the estimated variance of the estimated pointwise standard error of if is the covariance matrix of the and if then var
moving beyond linearity is the square root of this variance
this computation is repeated at each reference point and we plot the fitted curve as well as twice the standard error on either side of the fitted curve
we plot twice the standard error because for normally distributed error terms this quantity corresponds to an approximate confidence interval
it seems like the wages in figure are from two distinct populations there appears to be high earners group earning more than per annum as well as low earners group
we can treat wage as binary variable by splitting it into these two groups
logistic regression can then be used to predict this binary response using polynomial functions of age as predictors
in other words we fit the model exp xi
xdi pr yi xi
exp xi
xdi the result is shown in the right hand panel of figure
the gray marks on the top and bottom of the panel indicate the ages of the high earners and the low earners
the solid blue curve indicates the fitted probabilities of being high earner as function of age
the estimated confidence interval is shown as well
we see that here the confidence intervals are fairly wide especially on the right hand side
although the sample size for this data set is substantial there are only high earners which results in high variance in the estimated coefficients and consequently wide confidence intervals
step functions using polynomial functions of the features as predictors in linear model imposes global structure on the non linear function of
we can instead use step functions in order to avoid imposing such global structure
here step function we break the range of into bins and fit different constant in each bin
this amounts to converting continuous variable into an ordered categorical variable ordered categorical in greater detail we create cutpoints ck in the range of variable and then construct new variables
ck ck ck ck ck where is an indicator function that returns if the condition is true indicator function
the wage data
left the solid curve displays the fitted value from least squares regression of wage in thousands of dollars using step functions of age
the dotted curves indicate an estimated confidence interval
right we model the binary event wage using logistic regression again using step functions of age
the fitted posterior probability of wage exceeding is shown along with an estimated confidence interval and returns otherwise
for example ck equals if ck and equals otherwise
these are sometimes called dummy variables
notice that for any value of
ck since must be in exactly one of the intervals
we then use least squares to fit linear model using ck as predictors yi xi xi
ck xi oi
for given value of at most one of ck can be non zero
note that when all of the predictors in are zero so can be interpreted as the mean value of for
by comparison predicts response of for cj cj so represents the average increase in the response for in cj cj relative to
an example of fitting step functions to the wage data from figure is shown in the left hand panel of figure
we also fit the logistic regression we exclude as predictor in because it is redundant with the intercept
this is similar to the fact that we need only two dummy variables to code qualitative variable with three levels provided that the model will contain an intercept
the decision to exclude instead of some other ck in is arbitrary
alternatively we could include ck and exclude the intercept
moving beyond linearity model exp xi
ck xi pr yi xi exp xi
ck xi in order to predict the probability that an individual is high earner on the basis of age
the right hand panel of figure displays the fitted posterior probabilities obtained using this approach
unfortunately unless there are natural breakpoints in the predictors piecewise constant functions can miss the action
for example in the lefthand panel of figure the first bin clearly misses the increasing trend of wage with age
nevertheless step function approaches are very popular in biostatistics and epidemiology among other disciplines
for example five year age groups are often used to define the bins
basis functions polynomial and piecewise constant regression models are in fact special cases of basis function approach
the idea is to have at hand fambasis function ily of functions or transformations that can be applied to variable bk
instead of fitting linear model in we fit the model yi xi xi xi
bk xi oi
note that the basis functions bk are fixed and known
in other words we choose the functions ahead of time
for polynomial regression the basis functions are bj xi xji and for piecewise constant functions they are bj xi cj xi cj
we can think of as standard linear model with predictors xi xi bk xi
hence we can use least squares to estimate the unknown regression coefficients in
importantly this means that all of the inference tools for linear models that are discussed in chapter such as standard errors for the coefficient estimates and statistics for the model's overall significance are available in this setting
thus far we have considered the use of polynomial functions and piecewise constant functions for our basis functions however many alternatives are possible
for instance we can use wavelets or fourier series to construct basis functions
in the next section we investigate very common choice for basis function regression splines regression spline
regression splines regression splines now we discuss flexible class of basis functions that extends upon the polynomial regression and piecewise constant regression approaches that we have just seen
piecewise polynomials instead of fitting high degree polynomial over the entire range of piecewise polynomial regression involves fitting separate low degree polynomials piecewise over different regions of
for example piecewise cubic polynomial works polynomial by fitting cubic regression model of the form regression yi xi oi where the coefficients and differ in different parts of the range of
the points where the coefficients change are called knots knot for example piecewise cubic with no knots is just standard cubic polynomial as in with
piecewise cubic polynomial with single knot at point takes the form xi oi if xi yi xi oi if xi
in other words we fit two different polynomial functions to the data one on the subset of the observations with xi and one on the subset of the observations with xi
the first polynomial function has coefficients and the second has coefficients
each of these polynomial functions can be fit using least squares applied to simple functions of the original predictor
using more knots leads to more flexible piecewise polynomial
in general if we place different knots throughout the range of then we will end up fitting different cubic polynomials
note that we do not need to use cubic polynomial
for example we can instead fit piecewise linear functions
in fact our piecewise constant functions of section are piecewise polynomials of degree
the top left panel of figure shows piecewise cubic polynomial fit to subset of the wage data with single knot at age
we immediately see problem the function is discontinuous and looks ridiculous
since each polynomial has four parameters we are using total of degrees of freedom degrees of freedom in fitting this piecewise polynomial model
constraints and splines the top left panel of figure looks wrong because the fitted curve is just too flexible
to remedy this problem we can fit piecewise polynomial
various piecewise polynomials are fit to subset of the wage data with knot at age
top left the cubic polynomials are unconstrained
top right the cubic polynomials are constrained to be continuous at age
bottom left the cubic polynomials are constrained to be continuous and to have continuous first and second derivatives
bottom right linear spline is shown which is constrained to be continuous under the constraint that the fitted curve must be continuous
in other words there cannot be jump when age
the top right plot in figure shows the resulting fit
this looks better than the top left plot but the vshaped join looks unnatural
in the lower left plot we have added two additional constraints now both the first and second derivatives of the piecewise polynomials are continuous derivative at age
in other words we are requiring that the piecewise polynomial be not only continuous when age but also very smooth
each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom by reducing the complexity of the resulting piecewise polynomial fit
so in the top left plot we are using eight degrees of freedom but in the bottom left plot we imposed three constraints continuity continuity of the first derivative and continuity of the second derivative
regression splines and so are left with five degrees of freedom
the curve in the bottom left plot is called cubic spline in general cubic spline with knots uses cubic spline total of degrees of freedom
in figure the lower right plot is linear spline which is continuous linear spline at age
the general definition of degree spline is that it is piecewise degree polynomial with continuity in derivatives up to degree at each knot
therefore linear spline is obtained by fitting line in each region of the predictor space defined by the knots requiring continuity at each knot
in figure there is single knot at age
of course we could add more knots and impose continuity at each
the spline basis representation the regression splines that we just saw in the previous section may have seemed somewhat complex how can we fit piecewise degree polynomial under the constraint that it and possibly its first derivatives be continuous
it turns out that we can use the basis model to represent regression spline
cubic spline with knots can be modeled as yi xi xi bk xi oi for an appropriate choice of basis functions bk
the model can then be fit using least squares
just as there were several ways to represent polynomials there are also many equivalent ways to represent cubic splines using different choices of basis functions in
the most direct way to represent cubic spline using is to start off with basis for cubic polynomial namely and then add one truncated power basis function per knot
truncated power truncated power basis function is defined as basis be if be be be otherwise where be is the knot
one can show that adding term of the form be to the model for cubic polynomial will lead to discontinuity in only the third derivative at be the function will remain continuous with continuous first and second derivatives at each of the knots
in other words in order to fit cubic spline to data set with knots we perform least squares regression with an intercept and predictors of the form be be be where be be are the knots
this amounts to estimating total of regression coefficients for this reason fitting cubic spline with knots uses degrees of freedom
cubic splines are popular because most human eyes cannot detect the discontinuity at the knots
cubic spline and natural cubic spline with three knots fit to subset of the wage data
unfortunately splines can have high variance at the outer range of the predictors that is when takes on either very small or very large value
figure shows fit to the wage data with three knots
we see that the confidence bands in the boundary region appear fairly wild
natural spline is regression spline with additional boundary constraints natural spline the function is required to be linear at the boundary in the region where is smaller than the smallest knot or larger than the largest knot
this additional constraint means that natural splines generally produce more stable estimates at the boundaries
in figure natural cubic spline is also displayed as red line
note that the corresponding confidence intervals are narrower
choosing the number and locations of the knots when we fit spline where should we place the knots
the regression spline is most flexible in regions that contain lot of knots because in those regions the polynomial coefficients can change rapidly
hence one option is to place more knots in places where we feel the function might vary most rapidly and to place fewer knots where it seems more stable
while this option can work well in practice it is common to place knots in uniform fashion
one way to do this is to specify the desired degrees of freedom and then have the software automatically place the corresponding number of knots at uniform quantiles of the data
natural cubic spline function with four degrees of freedom is fit to the wage data
left spline is fit to wage in thousands of dollars as function of age
right logistic regression is used to model the binary event wage as function of age
the fitted posterior probability of wage exceeding is shown
figure shows an example on the wage data
as in figure we have fit natural cubic spline with three knots except this time the knot locations were chosen automatically as the th th and th percentiles of age
this was specified by requesting four degrees of freedom
the argument by which four degrees of freedom leads to three interior knots is somewhat technical how many knots should we use or equivalently how many degrees of freedom should our spline contain
one option is to try out different numbers of knots and see which produces the best looking curve
somewhat more objective approach is to use cross validation as discussed in chapters and
with this method we remove portion of the data say fit spline with certain number of knots to the remaining data and then use the spline to make predictions for the held out portion
we repeat this process multiple times until each observation has been left out once and then compute the overall cross validated rss
this procedure can be re there are actually five knots including the two boundary knots
cubic spline with five knots would have nine degrees of freedom
but natural cubic splines have two additional natural constraints at each boundary to enforce linearity resulting in degrees of freedom
since this includes constant which is absorbed in the intercept we count it as four degrees of freedom
ten fold cross validated mean squared errors for selecting the degrees of freedom when fitting splines to the wage data
the response is wage and the predictor age
left natural cubic spline
right cubic spline peated for different numbers of knots
then the value of giving the smallest rss is chosen
figure shows ten fold cross validated mean squared errors for splines with various degrees of freedom fit to the wage data
the left hand panel corresponds to natural spline and the right hand panel to cubic spline
the two methods produce almost identical results with clear evidence that one degree fit linear regression is not adequate
both curves flatten out quickly and it seems that three degrees of freedom for the natural spline and four degrees of freedom for the cubic spline are quite adequate
in section we fit additive spline models simultaneously on several variables at time
this could potentially require the selection of degrees of freedom for each variable
in cases like this we typically adopt more pragmatic approach and set the degrees of freedom to fixed number say four for all terms
comparison to polynomial regression regression splines often give superior results to polynomial regression
this is because unlike polynomials which must use high degree exponent in the highest monomial term
to produce flexible fits splines introduce flexibility by increasing the number of knots but keeping the degree fixed
generally this approach produces more stable estimates
splines also allow us to place more knots and hence flexibility over regions where the function seems to be changing rapidly and fewer knots where appears more stable
figure compares natural cubic spline with degrees of freedom to degree polynomial on the wage data set
the extra flexibility in the polynomial produces undesirable results at the boundaries while the natural cubic spline still provides reasonable fit to the data
on the wage data set natural cubic spline with degrees of freedom is compared to degree polynomial
polynomials can show wild behavior especially near the tails
smoothing splines an overview of smoothing splines in the last section we discussed regression splines which we create by specifying set of knots producing sequence of basis functions and then using least squares to estimate the spline coefficients
we now introduce somewhat different approach that also produces spline
in fitting smooth curve to set of data what we really want to do is find some function say that fits the observed data well that is we want rss ni yi xi to be small
however there is problem with this approach
if we don't put any constraints on xi then we can always make rss zero simply by choosing such that it interpolates all of the yi
such function would woefully overfit the data it would be far too flexible
what we really want is function that makes rss small but that is also smooth
how might we ensure that is smooth
there are number of ways to do this
natural approach is to find the function that minimizes xn yi xi bb dt where bb is nonnegative tuning parameter
the function that minimizes is known as smoothing spline smoothing spline
moving beyond linearity what does mean
equation takes the loss penalty formulation that we encounter in the context of ridge regression and the lasso in chapter
the term ni yi xri is loss function that encourages loss function to fit the data well and the term bb dt is penalty term that penalizes the variability in
the notation indicates the second derivative of the function
the first derivative measures the slope of function at and the second derivative corresponds to the amount by which the slope is changing
hence broadly speaking the second derivative of function is measure of its roughness it is large in absolute value if is very wiggly near and it is close to zero otherwise
the second derivative of straight line is zero note that line is perfectly smooth
the notation is an integral which we can think of as summation over the range of
in other words dt is simply measure of the total change in the function over itsr entire range
if is very smooth then will be close to constant and dt will take on small value
conversely if is jumpy and variable than will vary significantly and dt will take on large value
therefore in bb dt encourages to be smooth
the larger the value of bb the smoother will be
when bb then the penalty term in has no effect and so the function will be very jumpy and will exactly interpolate the training observations
when bb will be perfectly smooth it will just be straight line that passes as closely as possible to the training points
in fact in this case will be the linear least squares line since the loss function in amounts to minimizing the residual sum of squares
for an intermediate value of bb will approximate the training observations but will be somewhat smooth
we see that bb controls the bias variance trade off of the smoothing spline
the function that minimizes can be shown to have some special properties it is piecewise cubic polynomial with knots at the unique values of xn and continuous first and second derivatives at each knot
furthermore it is linear in the region outside of the extreme knots
in other words the function that minimizes is natural cubic spline with knots at xn
however it is not the same natural cubic spline that one would get if one applied the basis function approach described in section with knots at xn rather it is shrunken version of such natural cubic spline where the value of the tuning parameter bb in controls the level of shrinkage
choosing the smoothing parameter bb we have seen that smoothing spline is simply natural cubic spline with knots at every unique value of xi
it might seem that smoothing spline will have far too many degrees of freedom since knot at each data point allows great deal of flexibility
but the tuning parameter bb controls the roughness of the smoothing spline and hence the effective degrees of
smoothing splines freedom
it is possible to show that as bb increases from to the effective effective degrees of degrees of freedom which we write df bb decrease from to freedom in the context of smoothing splines why do we discuss effective degrees of freedom instead of degrees of freedom
usually degrees of freedom refer to the number of free parameters such as the number of coefficients fit in polynomial or cubic spline
although smoothing spline has parameters and hence nominal degrees of freedom these parameters are heavily constrained or shrunk down
hence df bb is measure of the flexibility of the smoothing spline the higher it is the more flexible and the lowerbias but higher variance the smoothing spline
the definition of effective degrees of freedom is somewhat technical
we can write bb bb where is the solution to for particular choice of bb that is it is vector containing the fitted values of the smoothing spline at the training points xn
equation indicates that the vector of fitted values when applying smoothing spline to the data can be written as matrix bb for which there is formula times the response vector
then the effective degrees of freedom is defined to be df bb bb ii the sum of the diagonal elements of the matrix bb
in fitting smoothing spline we do not need to select the number or location of the knots there will be knot at each training observation xn
instead we have another problem we need to choose the value of bb
it should come as no surprise that one possible solution to this problem is cross validation
in other words we can find the value of bb that makes the cross validated rss as small as possible
it turns out that the leaveone out cross validation error loocv can be computed very efficiently for smoothing splines with essentially the same cost as computing single fit using the following formula xn xn yi bb xi rsscv bb yi bb xi bb ii the notation bb xi indicates the fitted value for this smoothing spline evaluated at xi where the fit uses all of the training observations except for the ith observation xi yi
in contrast bb xi indicates the smoothing spline function fit to all of the training observations and evaluated at xi
this remarkable formula says that we can compute each of these leaveone out fits using only bb the original fit to all of the data
we have the exact formulas for computing and are very technical however efficient bb algorithms are available for computing these quantities
smoothing spline fits to the wage data
the red curve results from specifying effective degrees of freedom
for the blue curve bb was found automatically by leave one out cross validation which resulted in effective degrees of freedom very similar formula on page in chapter for least squares linear regression
using we can very quickly perform loocv for the regression splines discussed earlier in this chapter as well as for least squares regression using arbitrary basis functions
figure shows the results from fitting smoothing spline to the wage data
the red curve indicates the fit obtained from pre specifying that we would like smoothing spline with effective degrees of freedom
the blue curve is the smoothing spline obtained when bb is chosen using loocv in this case the value of bb chosen results in effective degrees of freedom computed using
for this data there is little discernible difference between the two smoothing splines beyond the fact that the one with degrees of freedom seems slightly wigglier
since there is little difference between the two fits the smoothing spline fit with degrees of freedom is preferable since in general simpler models are better unless the data provides evidence in support of more complex model
local regression local regression is different approach for fitting flexible non linear funclocal regression tions which involves computing the fit at target point using only the nearby training observations
figure illustrates the idea on some simu
local regression illustrated on some simulated data where the blue curve represents from which the data were generated and the light orange curve corresponds to the local regression estimate
the orange colored points are local to the target point represented by the orange vertical line
the yellow bell shape superimposed on the plot indicates weights assigned to each point decreasing to zero with distance from the target point
the fit at is obtained by fitting weighted linear regression orange line segment and using the fitted value at orange solid dot as the estimate lated data with one target point near and another near the boundary at
in this figure the blue line represents the function from which the data were generated and the light orange line corresponds to the local regression estimate
local regression is described in algorithm
note that in step of algorithm the weights ki will differ for each value of
in other words in order to obtain the local regression fit at new point we need to fit new weighted least squares regression model by minimizing for new set of weights
local regression is sometimes referred to as memory based procedure because like nearest neighbors we need all the training data each time we wish to compute prediction
we will avoid getting into the technical details of local regression here there are books written on the topic
in order to perform local regression there are number of choices to be made such as how to define the weighting function and whether to fit linear constant or quadratic regression in step above
equation corresponds to linear regression
while all of these choices make some difference the most important choice is the span defined in step above
the span plays role like that of the tuning parameter bb in smoothing splines it controls the flexibility of the non linear fit
the smaller the value of the more local and wiggly will be our fit alternatively very large value of will lead to global fit to the data using all of the training observations
we can again use cross validation to choose or we can
moving beyond linearity algorithm local regression at
gather the fraction of training points whose xi are closest to
assign weight ki xi to each point in this neighborhood so that the point furthest from has weight zero and the closest has the highest weight
all but these nearest neighbors get weight zero
fit weighted least squares regression of the yi on the xi using the aforementioned weights by finding and that minimize ki yi xi
the fitted value at is given by specify it directly
figure displays local linear regression fits on the wage data using two values of and
as expected the fit obtained using is smoother than that obtained using
the idea of local regression can be generalized in many different ways
in setting with multiple features xp one very useful generalization involves fitting multiple linear regression model that is global in some variables but local in another such as time
such varying coefficient models are useful way of adapting model to the most recently gathered varying coefficient data
local regression also generalizes very naturally when we want to fit model models that are local in pair of variables and rather than one
we can simply use two dimensional neighborhoods and fit bivariate linear regression models using the observations that are near each target point in two dimensional space
theoretically the same approach can be implemented in higher dimensions using linear regressions fit to dimensional neighborhoods
however local regression can perform poorly if is much larger than about or because there will generally be very few training observations close to
nearest neighbors regression discussed in chapter suffers from similar problem in high dimensions
generalized additive models in sections we present number of approaches for flexibly predicting response on the basis of single predictor
these approaches can be seen as extensions of simple linear regression
here we explore the prob
local linear fits to the wage data
the span specifies the fraction of the data used to compute the fit at each target point lem of flexibly predicting on the basis of several predictors xp
this amounts to an extension of multiple linear regression
generalized additive models gams provide general framework for generalized extending standard linear model by allowing non linear functions of each additive model of the variables while maintaining additivity
just like linear models gams additivity can be applied with both quantitative and qualitative responses
we first examine gams for quantitative response in section and then for qualitative response in section
gams for regression problems natural way to extend the multiple linear regression model yi xi xi xip oi in order to allow for non linear relationships between each feature and the response is to replace each linear component xij with smooth nonlinear function fj xij
we would then write the model as yi fj xij oi xi xi fp xip oi
for the wage data plots of the relationship between each feature and the response wage in the fitted model
each plot displays the fitted function and pointwise standard errors
the first two functions are natural splines in year and age with four and five degrees of freedom respectively
the third function is step function fit to the qualitative variable education
this is an example of gam
it is called an additive model because we calculate separate fj for each xj and then add together all of their contributions
in sections we discuss many methods for fitting functions to single variable
the beauty of gams is that we can use these methods as building blocks for fitting an additive model
in fact for most of the methods that we have seen so far in this chapter this can be done fairly trivially
take for example natural splines and consider the task of fitting the model wage year age education on the wage data
here year and age are quantitative variables and education is qualitative variable with five levels hs hs coll coll coll referring to the amount of high school or college education that an individual has completed
we fit the first two functions using natural splines
we fit the third function using separate constant for each level via the usual dummy variable approach of section
figure shows the results of fitting the model using least squares
this is easy to do since as discussed in section natural splines can be constructed using an appropriately chosen set of basis functions
hence the entire model is just big regression onto spline basis variables and dummy variables all packed into one big regression matrix
figure can be easily interpreted
the left hand panel indicates that holding age and education fixed wage tends to increase slightly with year this may be due to inflation
the center panel indicates that holding education and year fixed wage tends to be highest for intermediate values of age and lowest for the very young and very old
the right hand panel indicates
details are as in figure but now and are smoothing splines with four and five degrees of freedom respectively that holding year and age fixed wage tends to increase with education the more educated person is the higher their salary on average
all of these findings are intuitive
figure shows similar triple of plots but this time and are smoothing splines with four and five degrees of freedom respectively
fitting gam with smoothing spline is not quite as simple as fitting gam with natural spline since in the case of smoothing splines least squares cannot be used
however standard software such as the gam function in can be used to fit gams using smoothing splines via an approach known as backfitting
this method fits model involving multiple predictors by backfitting repeatedly updating the fit for each predictor in turn holding the others fixed
the beauty of this approach is that each time we update function we simply apply the fitting method for that variable to partial residual the fitted functions in figures and look rather similar
in most situations the differences in the gams obtained using smoothing splines versus natural splines are small
we do not have to use splines as the building blocks for gams we can just as well use local regression polynomial regression or any combination of the approaches seen earlier in this chapter in order to create gam
gams are investigated in further detail in the lab at the end of this chapter
pros and cons of gams before we move on let us summarize the advantages and limitations of gam
partial residual for for example has the form if we know and then we can fit by treating this residual as response in non linear regression on
moving beyond linearity gams allow us to fit non linear fj to each xj so that we can automatically model non linear relationships that standard linear regression will miss
this means that we do not need to manually try out many different transformations on each variable individually
the non linear fits can potentially make more accurate predictions for the response
because the model is additive we can still examine the effect of each xj on individually while holding all of the other variables fixed
hence if we are interested in inference gams provide useful representation
the smoothness of the function fj for the variable xj can be summarized via degrees of freedom
the main limitation of gams is that the model is restricted to be additive
with many variables important interactions can be missed
however as with linear regression we can manually add interaction terms to the gam model by including additional predictors of the form xj xk
in addition we can add low dimensional interaction functions of the form fjk xj xk into the model such terms can be fit using two dimensional smoothers such as local regression or two dimensional splines not covered here
for fully general models we have to look for even more flexible approaches such as random forests and boosting described in chapter
gams provide useful compromise between linear and fully nonparametric models
gams for classification problems gams can also be used in situations where is qualitative
for simplicity here we will assume takes on values zero or one and let pr be the conditional probability given the predictors that the response equals one
recall the logistic regression model log
this logit is the log of the odds of versus which represents as linear function of the predictors
natural way to extend to allow for non linear relationships is to use the model log fp xp
equation is logistic regression gam
it has all the same pros and cons as discussed in the previous section for quantitative responses
for the wage data the logistic regression gam given in is fit to the binary response wage
each plot displays the fitted function and pointwise standard errors
the first function is linear in year the second function smoothing spline with five degrees of freedom in age and the third step function for education
there are very wide standard errors for the first level hs of education
we fit gam to the wage data in order to predict the probability that an individual's income exceeds per year
the gam that we fit takes the form log year age education where pr wage year age education
once again is fit using smoothing spline with five degrees of freedom and is fit as step function by creating dummy variables for each of the levels of education
the resulting fit is shown in figure
the last panel looks suspicious with very wide confidence intervals for level hs
in fact there are no ones for that category no individuals with less than high school education make more than per year
hence we refit the gam excluding the individuals with less than high school education
the resulting model is shown in figure
as in figures and all three panels have the same vertical scale
this allows us to visually assess the relative contributions of each of the variables
we observe that age and education have much larger effect than year on the probability of being high earner
lab non linear modeling in this lab we re analyze the wage data considered in the examples throughout this chapter in order to illustrate the fact that many of the complex
the same model is fit as in figure this time excluding the observations for which education is hs
now we see that increased education tends to be associated with higher salaries non linear fitting procedures discussed can be easily implemented in
we begin by loading the islr library which contains the data library islr attach wage polynomial regression and step functions we now examine how figure was produced
we first fit the model using the following command fit lm wage poly age data wage coef summary fit estimate std
error value pr intercept poly age poly age poly age poly age this syntax fits linear model using the lm function in order to predict wage using fourth degree polynomial in age poly age
the poly command allows us to avoid having to write out long formula with powers of age
the function returns matrix whose columns are basis of orthogonal polynomials which essentially means that each column is linear orthogonal combination of the variables age age age and age polynomial however we can also use poly to obtain age age age and age directly if we prefer
we can do this by using the raw true argument to the poly function
later we see that this does not affect the model in meaningful way though the choice of basis clearly affects the coefficient estimates it does not affect the fitted values obtained fit lm wage poly age raw data wage
lab non linear modeling coef summary fit estimate std
error value pr intercept poly age raw poly age raw poly age raw poly age raw there are several other equivalent ways of fitting this model which showcase the flexibility of the formula language in
for example fit lm wage age age age age data wage coef fit intercept age age age age this simply creates the polynomial basis functions on the fly taking care to protect terms like age via the wrapper function the symbol has wrapper special meaning in formulas fit lm wage cbind age age age age data wage this does the same more compactly using the cbind function for building matrix from collection of vectors any function call such as cbind inside formula also serves as wrapper
we now create grid of values for age at which we want predictions and then call the generic predict function specifying that we want standard errors as well agelims range age age grid seq from agelims to agelims preds predict fit newdata list age age grid se true se bands cbind preds fit preds se fit preds fit preds se fit finally we plot the data and add the fit from the degree polynomial par mfrow mar oma plot age wage xlim agelims cex col darkgrey title degree polynomia outer lines age grid preds fit lwd col blue matlines age grid se bands lwd col blue lty here the mar and oma arguments to par allow us to control the margins of the plot and the title function creates figure title that spans both title subplots
we mentioned earlier that whether or not an orthogonal set of basis functions is produced in the poly function will not affect the model obtained in meaningful way
what do we mean by this
the fitted values obtained in either case are identical preds predict fit newdata list age age grid se true max abs preds fit preds fit
moving beyond linearity in performing polynomial regression we must decide on the degree of the polynomial to use
one way to do this is by using hypothesis tests
we now fit models ranging from linear to degree polynomial and seek to determine the simplest model which is sufficient to explain the relationship between wage and age
we use the anova function which performs an anova analysis of variance anova using an test in order to test the null analysis of variance hypothesis that model is sufficient to explain the data against the alternative hypothesis that more complex model is required
in order to use the anova function and must be nested models the predictors in must be subset of the predictors in
in this case we fit five different models and sequentially compare the simpler model to the more complex model fit lm wage age data wage fit lm wage poly age data wage fit lm wage poly age data wage fit lm wage poly age data wage fit lm wage poly age data wage anova fit fit fit fit fit analysis of variance table model wage age model wage poly age model wage poly age model wage poly age model wage poly age res
df rss df sum of sq pr
signif codes
the value comparing the linear model to the quadratic model is essentially zero indicating that linear fit is not sufficient
similarly the value comparing the quadratic model to the cubic model is very low so the quadratic fit is also insufficient
the value comparing the cubic and degree polynomials model and model is approximately while the degree polynomial model seems unnecessary because its value is
hence either cubic or quartic polynomial appear to provide reasonable fit to the data but loweror higher order models are not justified
in this case instead of using the anova function we could have obtained these values more succinctly by exploiting the fact that poly creates orthogonal polynomials coef summary fit estimate std
error value pr
lab non linear modeling intercept poly age poly age poly age poly age poly age notice that the values are the same and in fact the square of the tstatistics are equal to the statistics from the anova function for example however the anova method works whether or not we used orthogonal polynomials it also works when we have other terms in the model as well
for example we can use anova to compare these three models fit lm wage education age data wage fit lm wage education poly age data wage fit lm wage education poly age data wage anova fit fit fit as an alternative to using hypothesis tests and anova we could choose the polynomial degree using cross validation as discussed in chapter
next we consider the task of predicting whether an individual earns more than per year
we proceed much as before except that first we create the appropriate response vector and then apply the glm function using family binomial in order to fit polynomial logistic regression model fit glm wage poly age data wage family binomial note that we again use the wrapper to create this binary response variable on the fly
the expression wage evaluates to logical variable containing trues and falses which glm coerces to binary by setting the trues to and the falses to
once again we make predictions using the predict function preds predict fit newdata list age age grid se however calculating the confidence intervals is slightly more involved than in the linear regression case
the default prediction type for glm model is type link which is what we use here
this means we get predictions for the logit that is we have fit model of the form pr log pr and the predictions given are of the form
the standard errors given are also of this form
in order to obtain confidence intervals for pr
moving beyond linearity we use the transformation exp pr exp pfit exp preds fit exp preds fit se bands logit cbind preds fit preds se fit preds fit preds se fit se bands exp se bands logit exp se bands logit note that we could have directly computed the probabilities by selecting the type response option in the predict function preds predict fit newdata list age age grid type response se however the corresponding confidence intervals would not have been sensible because we would end up with negative probabilities
finally the right hand plot from figure was made as follows plot age wage xlim agelims type ylim points jitter age wage cex pch col darkgrey lines age grid pfit lwd col blue matlines age grid se bands lwd col blue lty we have drawn the age values corresponding to the observations with wage values above as gray marks on the top of the plot and those with wage values below are shown as gray marks on the bottom of the plot
we used the jitter function to jitter the age values bit so that observations jitter with the same age value do not cover each other up
this is often called rug plot rug plot in order to fit step function as discussed in section we use the cut function cut table cut age fit lm wage cut age data wage coef summary fit estimate std
error value pr intercept cut age cut age cut age here cut automatically picked the cutpoints at and years of age
we could also have specified our own cutpoints directly using the breaks option
the function cut returns an ordered categorical variable the lm function then creates set of dummy variables for use in the regression
the age category is left out so the intercept coefficient of can be interpreted as the average salary for those under years
lab non linear modeling of age and the other coefficients can be interpreted as the average additional salary for those in the other age groups
we can produce predictions and plots just as we did in the case of the polynomial fit
splines in order to fit regression splines in we use the splines library
in section we saw that regression splines can be fit by constructing an appropriate matrix of basis functions
the bs function generates the entire bs matrix of basis functions for splines with the specified set of knots
by default cubic splines are produced
fitting wage to age using regression spline is simple library splines fit lm wage bs age knots data wage pred predict fit newdata list age age grid se plot age wage col gray lines age grid pred fit lwd lines age grid pred fit pred se lty dashed lines age grid pred fit pred se lty dashed here we have prespecified knots at ages and
this produces spline with six basis functions
recall that cubic spline with three knots has seven degrees of freedom these degrees of freedom are used up by an intercept plus six basis functions
we could also use the df option to produce spline with knots at uniform quantiles of the data dim bs age knots dim bs age df attr bs age df knots in this case chooses knots at ages and which correspond to the th th and th percentiles of age
the function bs also has degree argument so we can fit splines of any degree rather than the default degree of which yields cubic spline
in order to instead fit natural spline we use the ns function
here ns we fit natural spline with four degrees of freedom fit lm wage ns age df data wage pred predict fit newdata list age age grid se lines age grid pred fit col red lwd as with the bs function we could instead specify the knots directly using the knots option
in order to fit smoothing spline we use the smooth spline function smooth spline figure was produced with the following code
moving beyond linearity plot age wage xlim agelims cex col darkgrey title smoothing spline fit smooth spline age wage df fit smooth spline age wage cv true fit df lines fit col red lwd lines fit col blue lwd legend topright legend df df col red blue lty lwd cex notice that in the first call to smooth spline we specified df
the function then determines which value of bb leads to degrees of freedom
in the second call to smooth spline we select the smoothness level by crossvalidation this results in value of bb that yields degrees of freedom
in order to perform local regression we use the loess function loess plot age wage xlim agelims cex col darkgrey title local regressio fit loess wage age span data wage fit loess wage age span data wage lines age grid predict fit data frame age age grid col red lwd lines age grid predict fit data frame age age grid col blue lwd legend topright legend span span col red blue lty lwd cex here we have performed local linear regression using spans of and that is each neighborhood consists of or of the observations
the larger the span the smoother the fit
the locfit library can also be used for fitting local regression models in
gams we now fit gam to predict wage using natural spline functions of year and age treating education as qualitative predictor as in
since this is just big linear regression model using an appropriate choice of basis functions we can simply do this using the lm function gam lm wage ns year ns age education data wage we now fit the model using smoothing splines rather than natural splines
in order to fit more general sorts of gams using smoothing splines or other components that cannot be expressed in terms of basis functions and then fit using least squares regression we will need to use the gam library in
the function which is part of the gam library is used to indicate that we would like to use smoothing spline
we specify that the function of year should have degrees of freedom and that that the function of age will have degrees of freedom
since education is qualitative we leave it as
lab non linear modeling is and it is converted into dummy variables
we use the gam function in gam order to fit gam using these components
all of the terms in are fit simultaneously taking each other into account to explain the response library gam gam gam wage year age education data wage in order to produce figure we simply call the plot function par mfrow plot gam se true col blue the generic plot function recognizes that gam is an object of class gam and invokes the appropriate plot gam method
conveniently even though plot gam gam is not of class gam but rather of class lm we can still use plot gam on it
figure was produced using the following expression plot gam gam se true col red notice here we had to use plot gam rather than the generic plot function
in these plots the function of year looks rather linear
we can perform series of anova tests in order to determine which of these three models is best gam that excludes year gam that uses linear function of year or gam that uses spline function of year gam gam wage age education data wage gam gam wage year age education data wage anova gam gam gam test analysis of deviance table model wage age education model wage year age education model wage year age education resid
df resid
dev df deviance pr signif codes
we find that there is compelling evidence that gam with linear function of year is better than gam that does not include year at all pvalue
however there is no evidence that non linear function of year is needed value
in other words based on the results of this anova is preferred
the summary function produces summary of the gam fit summary gam call gam formula wage year age education data wage deviance residuals min median max
moving beyond linearity dispersio parameter for gaussian family taken to be null deviance on degrees of freedom residual deviance on degrees of freedom aic number of local scoring iteration df for terms and values for effects df npar df npar pr intercept year age education signif codes
the values for year and age correspond to null hypothesis of linear relationship versus the alternative of non linear relationship
the large pvalue for year reinforces our conclusion from the anova test that linear function is adequate for this term
however there is very clear evidence that non linear term is required for age
we can make predictions from gam objects just like from lm objects using the predict method for the class gam
here we make predictions on the training set preds predict gam newdata wage we can also use local regression fits as building blocks in gam using the lo function lo gam lo gam wage year df lo age span education data wage plot gam gam lo se true col green here we have used local regression for the age term with span of
we can also use the lo function to create interactions before calling the gam function
for example gam lo gam wage lo year age span education data wage fits two term model in which the first term is an interaction between year and age fit by local regression surface
we can plot the resulting two dimensional surface if we first install the akima package library akima plot gam lo in order to fit logistic regression gam we once again use the function in constructing the binary response variable and set family binomial
exercises gam lr gam wage year age df education family binomial data wage par mfrow plot gam lr se col green it is easy to see that there are no high earners in the hs category table education wage education false true
hs grad
hs grad
some college
college grad
advanced degree hence we fit logistic regression gam using all but this category
this provides more sensible results gam lr gam wage year age df education family binomial data wage subset education
hs grad plot gam lr se col green exercises conceptual
it was mentioned in the chapter that cubic regression spline with one knot at be can be obtained using basis of the form be where be be if be and equals otherwise
we will now show that function of the form be is indeed cubic regression spline regardless of the values of find cubic polynomial such that for all be
express in terms of find cubic polynomial such that for all be
express in terms of
we have now established that is piecewise polynomial
moving beyond linearity show that be be
that is is continuous at be show that be be
that is is continuous at be show that be be
that is is continuous at be
therefore is indeed cubic spline
hint parts and of this problem require knowledge of singlevariable calculus
as reminder given cubic polynomial the first derivative takes the form and the second derivative takes the form
suppose that curve is computed to smoothly fit set of points using the following formula
xn arg min yi xi bb dx where represents the mth derivative of and
provide example sketches of in each of the following scenarios bb bb bb bb bb
suppose we fit curve with basis functions
note that equals for and otherwise
we fit the linear regression model and obtain coefficient estimates
sketch the estimated curve between and
note the intercepts slopes and other relevant information
suppose we fit curve with basis functions
we fit the linear regression model and obtain coefficient estimates
sketch the estimated curve between and
note the intercepts slopes and other relevant information
consider two curves and defined by
arg min yi xi bb dx
arg min yi xi bb dx where represents the mth derivative of as bb will or have the smaller training rss
as bb will or have the smaller test rss
for bb will or have the smaller training and test rss
in this exercise you will further analyze the wage data set considered throughout this chapter perform polynomial regression to predict wage using age
use cross validation to select the optimal degree for the polynomial
what degree was chosen and how does this compare to the results of hypothesis testing using anova
make plot of the resulting polynomial fit to the data fit step function to predict wage using age and perform crossvalidation to choose the optimal number of cuts
make plot of the fit obtained
the wage data set contains number of other features not explored in this chapter such as marital status maritl job class jobclass and others
explore the relationships between some of these other predictors and wage and use non linear fitting techniques in order to fit flexible models to the data
create plots of the results obtained and write summary of your findings
moving beyond linearity
fit some of the non linear models investigated in this chapter to the auto data set
is there evidence for non linear relationships in this data set
create some informative plots to justify your answer
this question uses the variables dis the weighted mean of distances to five boston employment centers and nox nitrogen oxides concentration in parts per million from the boston data
we will treat dis as the predictor and nox as the response use the poly function to fit cubic polynomial regression to predict nox using dis
report the regression output and plot the resulting data and polynomial fits plot the polynomial fits for range of different polynomial degrees say from to and plot the associated residual sum of squares perform cross validation or another approach to select the optimal degree for the polynomial and explain your results use the bs function to fit regression spline to predict nox using dis
report the output for the fit using four degrees of freedom
how did you choose the knots
plot the resulting fit now fit regression spline for range of degrees of freedom and plot the resulting fits as well as the resulting rss
describe the results obtained perform cross validation or another approach in order to select the best degrees of freedom for regression spline on this data
describe your results
this question relates to the college data set using out of state tuition as the response and the other variables as the predictors perform forward stepwise selection in order to identify satisfactory model that uses just subset of the predictors divide the observations into training set and test set
fit gam on the training data using out of state tuition as the response and the features selected in the previous step as the predictors
plot the results and explain your findings evaluate the model obtained on the test set and explain the results obtained for which variables if any is there evidence of non linear relationship with the response
in section it was mentioned that gams are generally fit using backfitting approach
the idea behind backfitting is actually quite
exercises simple
we will now explore backfitting in the context of multiple linear regression
suppose that we would like to perform multiple linear regression but we do not have software to do so
instead we only have software to perform simple linear regression
therefore we take the following iterative approach we repeatedly hold all but one coefficient estimate fixed at its current value and update only that coefficient estimate using simple linear regression
the process is continued until convergence that is until the coefficient estimates stop changing
we now try this out on toy example generate response and two predictors and with initialize to take on value of your choice
it does not matter what value you choose keeping fixed fit the model
you can do this as follows beta beta lm coef keeping fixed fit the model
you can do this as follows beta beta lm coef write for loop to repeat and times
report the estimates of and at each iteration of the for loop
create plot in which each of these values is displayed with and each shown in different color compare your answer in to the results of simply performing multiple linear regression to predict using and
use the abline function to overlay those multiple linear regression coefficient estimates on the plot obtained in on this data set how many backfitting iterations were required in order to obtain good approximation to the multiple regression coefficient estimates
moving beyond linearity
this problem is continuation of the previous exercise
in toy example with show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in backfitting procedure
how many backfitting iterations are required in order to obtain good approximation to the multiple regression coefficient estimates
create plot to justify your answer
this is page printer opaque this tree based methods in this chapter we describe tree based methods for regression and classification
these involve stratifying or segmenting the predictor space into number of simple regions
in order to make prediction for given observation we typically use the mean or the mode of the training observations in the region to which it belongs
since the set of splitting rules used to segment the predictor space can be summarized in tree these types of approaches are known as decision tree methods decision tree tree based methods are simple and useful for interpretation
however they typically are not competitive with the best supervised learning approaches such as those seen in chapters and in terms of prediction accuracy
hence in this chapter we also introduce bagging random forests and boosting
each of these approaches involves producing multiple trees which are then combined to yield single consensus prediction
we will see that combining large number of trees can often result in dramatic improvements in prediction accuracy at the expense of some loss in interpretation
the basics of decision trees decision trees can be applied to both regression and classification problems
we first consider regression problems and then move on to classification
for the hitters data regression tree for predicting the log salary of baseball player based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year
at given internal node the label of the form xj tk indicates the left hand branch emanating from that split and the right hand branch corresponds to xj tk
for instance the split at the top of the tree results in two large branches
the left hand branch corresponds to years and the right hand branch corresponds to years
the tree has two internal nodes and three terminal nodes or leaves
the number in each leaf is the mean of the response for the observations that fall there
regression trees in order to motivate regression trees we begin with simple example regression tree predicting baseball players salaries using regression trees we use the hitters data set to predict baseball player's salary based on years the number of years that he has played in the major leagues and hits the number of hits that he made in the previous year
we first remove observations that are missing salary values and log transform salary so that its distribution has more of typical bell shape
recall that salary is measured in thousands of dollars
figure shows regression tree fit to this data
it consists of series of splitting rules starting at the top of the tree
the top split assigns observations having years to the left branch the predicted salary for these players is given by the mean response value for the players in the data set with years
for such players the mean log salary is and so we make prediction of thousands of dollars for both years and hits are integers in these data the tree function in labels the splits at the midpoint between two adjacent values
the three region partition for the hitters data set from the regression tree illustrated in figure these players
players with years are assigned to the right branch and then that group is further subdivided by hits
overall the tree stratifies or segments the players into three regions of predictor space players who have played for four or fewer years players who have played for five or more years and who made fewer than hits last year and players who have played for five or more years and who made at least hits last year
these three regions can be written as years years hits and years hits
figure illustrates the regions as function of years and hits
the predicted salaries for these three groups are and respectively
in keeping with the tree analogy the regions and are known as terminal nodes or leaves of the tree
as is the case for figure decision terminal node trees are typically drawn upside down in the sense that the leaves are at leaf the bottom of the tree
the points along the tree where the predictor space is split are referred to as internal nodes
in figure the two internal internal node nodes are indicated by the text years and hits
we refer to the segments of the trees that connect the nodes as branches branch we might interpret the regression tree displayed in figure as follows years is the most important factor in determining salary and players with less experience earn lower salaries than more experienced players
given that player is less experienced the number of hits that he made in the previous year seems to play little role in his salary
but among players who have been in the major leagues for five or more years the number of hits made in the previous year does affect salary and players who made more
tree based methods hits last year tend to have higher salaries
the regression tree shown in figure is likely an over simplification of the true relationship between hits years and salary
however it has advantages over other types of regression models such as those seen in chapters and it is easier to interpret and has nice graphical representation
prediction via stratification of the feature space we now discuss the process of building regression tree
roughly speaking there are two steps
we divide the predictor space that is the set of possible values for xp into distinct and non overlapping regions rj
for every observation that falls into the region rj we make the same prediction which is simply the mean of the response values for the training observations in rj
for instance suppose that in step we obtain two regions and and that the response mean of the training observations in the first region is while the response mean of the training observations in the second region is
then for given observation if we will predict value of and if we will predict value of
we now elaborate on step above
how do we construct the regions rj
in theory the regions could have any shape
however we choose to divide the predictor space into high dimensional rectangles or boxes for simplicity and for ease of interpretation of the resulting predictive model
the goal is to find boxes rj that minimize the rss given by xj yi rj rj where rj is the mean response for the training observations within the jth box
unfortunately it is computationally infeasible to consider every possible partition of the feature space into boxes
for this reason we take top down greedy approach that is known as recursive binary splitting
the recursive binary approach is top down because it begins at the top of the tree at which point splitting all observations belong to single region and then successively splits the predictor space each split is indicated via two new branches further down on the tree
it is greedy because at each step of the tree building process the best split is made at that particular step rather than looking ahead and picking split that will lead to better tree in some future step
in order to perform recursive binary splitting we first select the predictor xj and the cutpoint such that splitting the predictor space into the regions xj and xj leads to the greatest possible
the basics of decision trees reduction in rss
the notation xj means the region of predictor space in which xj takes on value less than
that is we consider all predictors xp and all possible values of the cutpoint for each of the predictors and then choose the predictor and cutpoint such that the resulting tree has the lowest rss
in greater detail for any and we define the pair of half planes xj and xj and we seek the value of and that minimize the equation yi yi xi xi where is the mean response for the training observations in and is the mean response for the training observations in
finding the values of and that minimize can be done quite quickly especially when the number of features is not too large
next we repeat the process looking for the best predictor and best cutpoint in order to split the data further so as to minimize the rss within each of the resulting regions
however this time instead of splitting the entire predictor space we split one of the two previously identified regions
we now have three regions
again we look to split one of these three regions further so as to minimize the rss
the process continues until stopping criterion is reached for instance we may continue until no region contains more than five observations
once the regions rj have been created we predict the response for given test observation using the mean of the training observations in the region to which that test observation belongs
five region example of this approach is shown in figure
tree pruning the process described above may produce good predictions on the training set but is likely to overfit the data leading to poor test set performance
this is because the resulting tree might be too complex
smaller tree with fewer splits that is fewer regions rj might lead to lower variance and better interpretation at the cost of little bias
one possible alternative to the process described above is to build the tree only so long as the decrease in the rss due to each split exceeds some high threshold
this strategy will result in smaller trees but is too short sighted since seemingly worthless split early on in the tree might be followed by very good split that is split that leads to large reduction in rss later on
therefore better strategy is to grow very large tree and then prune it back in order to obtain subtree
how do we determine the best prune way to prune the tree
intuitively our goal is to select subtree that subtree
top left partition of two dimensional feature space that could not result from recursive binary splitting
top right the output of recursive binary splitting on two dimensional example
bottom left tree corresponding to the partition in the top right panel
bottom right perspective plot of the prediction surface corresponding to that tree
the basics of decision trees leads to the lowest test error rate
given subtree we can estimate its test error using cross validation or the validation set approach
however estimating the cross validation error for every possible subtree would be too cumbersome since there is an extremely large number of possible subtrees
instead we need way to select small set of subtrees for consideration
cost complexity pruning also known as weakest link pruning gives cost complexity us way to do just this
rather than considering every possible subtree pruning we consider sequence of trees indexed by nonnegative tuning parameter weakest link
for each value of there corresponds subtree such that pruning yi rm xi rm is as small as possible
here indicates the number of terminal nodes of the tree rm is the rectangle the subset of predictor space corresponding to the mth terminal node and rm is the predicted response associated with rm that is the mean of the training observations in rm
the tuning parameter controls trade off between the subtree's complexity and its fit to the training data
when then the subtree will simply equal because then just measures the training error
however as increases there is price to pay for having tree with many terminal nodes and so the quantity will tend to be minimized for smaller subtree
equation is reminiscent of the lasso from chapter in which similar formulation was used in order to control the complexity of linear model
it turns out that as we increase from zero in branches get pruned from the tree in nested and predictable fashion so obtaining the whole sequence of subtrees as function of is easy
we can select value of using validation set or using cross validation
we then return to the full data set and obtain the subtree corresponding to
this process is summarized in algorithm
figures and display the results of fitting and pruning regression tree on the hitters data using nine of the features
first we randomly divided the data set in half yielding observations in the training set and observations in the test set
we then built large regression tree on the training data and varied in in order to create subtrees with different numbers of terminal nodes
finally we performed six fold cross validation in order to estimate the cross validated mse of the trees as function of
we chose to perform six fold cross validation because is an exact multiple of six
the unpruned regression tree is shown in figure
the green curve in figure shows the cv error as function of the number
tree based methods algorithm building regression tree
use recursive binary splitting to grow large tree on the training data stopping only when each terminal node has fewer than some minimum number of observations
apply cost complexity pruning to the large tree in order to obtain sequence of best subtrees as function of
use fold cross validation to choose
for each repeat steps and on the th fraction of the training data excluding the kth fold evaluate the mean squared prediction error on the data in the left out kth fold as function of
average the results and pick to minimize the average error
return the subtree from step that corresponds to the chosen value of of leaves while the orange curve indicates the test error
also shown are standard error bars around the estimated errors
for reference the training error curve is shown in black
the cv error is reasonable approximation of the test error the cv error takes on its minimum for three node tree while the test error also dips down at the three node tree though it takes on its lowest value at the ten node tree
the pruned tree containing three terminal nodes is shown in figure
classification trees classification tree is very similar to regression tree except that it is classification tree used to predict qualitative response rather than quantitative one
recall that for regression tree the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node
in contrast for classification tree we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs
in interpreting the results of classification tree we are often interested not only in the class prediction corresponding to particular terminal node region but also in the class proportions among the training observations that fall into that region
although cv error is computed as function of it is convenient to display the result as function of the number of leaves this is based on the relationship between and in the original tree grown to all the training data
regression tree analysis for the hitters data
the unpruned tree that results from top down greedy splitting on the training data is shown
the task of growing classification tree is quite similar to the task of growing regression tree
just as in the regression setting we use recursive binary splitting to grow classification tree
however in the classification setting rss cannot be used as criterion for making the binary splits
natural alternative to rss is the classification error rate
since we plan to classification error assign an observation in given region to the most commonly occurring rate class of training observations in that region the classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class max mk
here mk represents the proportion of training observations in the mth region that are from the kth class
however it turns out that classification error is not sufficiently sensitive for tree growing and in practice two other measures are preferable
the gini index is defined by gini index
regression tree analysis for the hitters data
the training cross validation and test mse are shown as function of the number of terminal nodes in the pruned tree
standard error bands are displayed
the minimum cross validation error occurs at tree size of three
mk mk measure of total variance across the classes
it is not hard to see that the gini index takes on small value if all of the mk are close to zero or one
for this reason the gini index is referred to as measure of node purity small value indicates that node contains predominantly observations from single class
an alternative to the gini index is cross entropy given by cross entropy mk log mk
since mk it follows that mk log mk
one can show that the cross entropy will take on value near zero if the mk are all near zero or near one
therefore like the gini index the cross entropy will take on small value if the mth node is pure
in fact it turns out that the gini index and the cross entropy are quite similar numerically
when building classification tree either the gini index or the crossentropy are typically used to evaluate the quality of particular split since these two approaches are more sensitive to node purity than is the classification error rate
any of these three approaches might be used when pruning the tree but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal
figure shows an example on the heart data set
these data contain binary outcome hd for patients who presented with chest pain
heart data
top the unpruned tree
bottom left cross validation error training and test error for different sizes of the pruned tree
bottom right the pruned tree corresponding to the minimal cross validation error
tree based methods outcome value of yes indicates the presence of heart disease based on an angiographic test while no means no heart disease
there are predictors including age sex chol cholesterol measurement and other heart and lung function measurements
cross validation results in tree with six terminal nodes
in our discussion thus far we have assumed that the predictor variables take on continuous values
however decision trees can be constructed even in the presence of qualitative predictor variables
for instance in the heart data some of the predictors such as sex thal thalium stress test and chestpain are qualitative
therefore split on one of these variables amounts to assigning some of the qualitative values to one branch and assigning the remaining to the other branch
in figure some of the internal nodes correspond to splitting qualitative variables
for instance the top internal node corresponds to splitting thal
the text thal indicates that the left hand branch coming out of that node consists of observations with the first value of the thal variable normal and the right hand node consists of the remaining observations fixed or reversible defects
the text chestpain bc two splits down the tree on the left indicates that the left hand branch coming out of that node consists of observations with the second and third values of the chestpain variable where the possible values are typical angina atypical angina non anginal pain and asymptomatic
figure has surprising characteristic some of the splits yield two terminal nodes that have the same predicted value
for instance consider the split restecg near the bottom right of the unpruned tree
regardless of the value of restecg response value of yes is predicted for those observations
why then is the split performed at all
the split is performed because it leads to increased node purity
that is all of the observations corresponding to the right hand leaf have response value of yes whereas of those corresponding to the left hand leaf have response value of yes
why is node purity important
suppose that we have test observation that belongs to the region given by that right hand leaf
then we can be pretty certain that its response value is yes
in contrast if test observation belongs to the region given by the left hand leaf then its response value is probably yes but we are much less certain
even though the split restecg does not reduce the classification error it improves the gini index and the cross entropy which are more sensitive to node purity
trees versus linear models regression and classification trees have very different flavor from the more classical approaches for regression and classification presented in chapters and
in particular linear regression assumes model of the form
top row two dimensional classification example in which the true decision boundary is linear and is indicated by the shaded regions
classical approach that assumes linear boundary left will outperform decision tree that performs splits parallel to the axes right
bottom row here the true decision boundary is non linear
here linear model is unable to capture the true decision boundary left whereas decision tree is successful right whereas regression trees assume model of the form cm rm where rm represent partition of feature space as in figure
which model is better
it depends on the problem at hand
if the relationship between the features and the response is well approximated by linear model as in then an approach such as linear regression will likely work well and will outperform method such as regression tree that does not exploit this linear structure
if instead there is highly nonlinear and complex relationship between the features and the response as indicated by model then decision trees may outperform classical approaches
an illustrative example is displayed in figure
the relative performances of tree based and classical approaches can be assessed by estimating the test error using either cross validation or the validation set approach chapter
tree based methods of course other considerations beyond simply test error may come into play in selecting statistical learning method for instance in certain settings prediction using tree may be preferred for the sake of interpretability and visualization
advantages and disadvantages of trees decision trees for regression and classification have number of advantages over the more classical approaches seen in chapters and trees are very easy to explain to people
in fact they are even easier to explain than linear regression
some people believe that decision trees more closely mirror human decision making than do the regression and classification approaches seen in previous chapters
trees can be displayed graphically and are easily interpreted even by non expert especially if they are small
trees can easily handle qualitative predictors without the need to create dummy variables
unfortunately trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book
however by aggregating many decision trees using methods like bagging random forests and boosting the predictive performance of trees can be substantially improved
we introduce these concepts in the next section
bagging random forests boosting bagging random forests and boosting use trees as building blocks to construct more powerful prediction models
bagging the bootstrap introduced in chapter is an extremely powerful idea
it is used in many situations in which it is hard or even impossible to directly compute the standard deviation of quantity of interest
we see here that the bootstrap can be used in completely different context in order to improve statistical learning methods such as decision trees
the decision trees discussed in section suffer from high variance
this means that if we split the training data into two parts at random
bagging random forests boosting and fit decision tree to both halves the results that we get could be quite different
in contrast procedure with low variance will yield similar results if applied repeatedly to distinct data sets linear regression tends to have low variance if the ratio of to is moderately large
bootstrap aggregation or bagging is general purpose procedure for reducing the bagging variance of statistical learning method we introduce it here because it is particularly useful and frequently used in the context of decision trees
recall that given set of independent observations zn each with variance the variance of the mean of the observations is given by
in other words averaging set of observations reduces variance
hence natural way to reduce the variance and hence increase the prediction accuracy of statistical learning method is to take many training sets from the population build separate prediction model using each training set and average the resulting predictions
in other words we could calculate using separate training sets and average them in order to obtain single low variance statistical learning model given by avg
of course this is not practical because we generally do not have access to multiple training sets
instead we can bootstrap by taking repeated samples from the single training data set
in this approach we generate different bootstrapped training data sets
we then train our method on the bth bootstrapped training set in order to get and finally average all the predictions to obtain bag
this is called bagging
while bagging can improve predictions for many regression methods it is particularly useful for decision trees
to apply bagging to regression trees we simply construct regression trees using bootstrapped training sets and average the resulting predictions
these trees are grown deep and are not pruned
hence each individual tree has high variance but low bias
averaging these trees reduces the variance
bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into single procedure
thus far we have described the bagging procedure in the regression context to predict quantitative outcome
how can bagging be extended to classification problem where is qualitative
in that situation there are few possible approaches but the simplest is as follows
for given test observation we can record the class predicted by each of the trees and take majority vote the overall prediction is the most commonly occurring majority vote
bagging and random forest results for the heart data
the test error black and orange is shown as function of the number of bootstrapped training sets used
random forests were applied with
the dashed line indicates the test error resulting from single classification tree
the green and blue traces show the oob error which in this case is considerably lower class among the predictions
figure shows the results from bagging trees on the heart data
the test error rate is shown as function of the number of trees constructed using bootstrapped training data sets
we see that the bagging test error rate is slightly lower in this case than the test error rate obtained from single tree
the number of trees is not critical parameter with bagging using very large value of will not lead to overfitting
in practice we use value of sufficiently large that the error has settled down
using is sufficient to achieve good performance in this example
out of bag error estimation it turns out that there is very straightforward way to estimate the test error of bagged model without the need to perform cross validation or the validation set approach
recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations
one can show that on average each bagged tree makes use of around two thirds of the
bagging random forests boosting observations the remaining one third of the observations not used to fit given bagged tree are referred to as the out of bag oob observations
we out of bag can predict the response for the ith observation using each of the trees in which that observation was oob
this will yield around predictions for the ith observation
in order to obtain single prediction for the ith observation we can average these predicted responses if regression is the goal or can take majority vote if classification is the goal
this leads to single oob prediction for the ith observation
an oob prediction can be obtained in this way for each of the observations from which the overall oob mse for regression problem or classification error for classification problem can be computed
the resulting oob error is valid estimate of the test error for the bagged model since the response for each observation is predicted using only the trees that were not fit using that observation
figure displays the oob error on the heart data
it can be shown that with sufficiently large oob error is virtually equivalent to leave one out cross validation error
the oob approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross validation would be computationally onerous
variable importance measures as we have discussed bagging typically results in improved accuracy over prediction using single tree
unfortunately however it can be difficult to interpret the resulting model
recall that one of the advantages of decision trees is the attractive and easily interpreted diagram that results such as the one displayed in figure
however when we bag large number of trees it is no longer possible to represent the resulting statistical learning procedure using single tree and it is no longer clear which variables are most important to the procedure
thus bagging improves prediction accuracy at the expense of interpretability
although the collection of bagged trees is much more difficult to interpret than single tree one can obtain an overall summary of the importance of each predictor using the rss for bagging regression trees or the gini index for bagging classification trees
in the case of bagging regression trees we can record the total amount that the rss is decreased due to splits over given predictor averaged over all trees
large value indicates an important predictor
similarly in the context of bagging classification trees we can add up the total amount that the gini index is decreased by splits over given predictor averaged over all trees
graphical representation of the variable importances in the heart data variable is shown in figure
we see the mean decrease in gini index for each vari importance able relative to the largest
the variables with the largest mean decrease in gini index are thal ca and chestpain
this relates to exercise of chapter
variable importance plot for the heart data
variable importance is computed using the mean decrease in gini index and expressed relative to the maximum
random forests random forests provide an improvement over bagged trees by way of random forest small tweak that decorrelates the trees
as in bagging we build number of decision trees on bootstrapped training samples
but when building these decision trees each time split in tree is considered random sample of predictors is chosen as split candidates from the full set of predictors
the split is allowed to use only one of those predictors
fresh sample of predictors is taken at each split and typically we choose that is the number of predictors considered at each split is approximately equal to the square root of the total number of predictors out of the for the heart data
in other words in building random forest at each split in the tree the algorithm is not even allowed to consider majority of the available predictors
this may sound crazy but it has clever rationale
suppose that there is one very strong predictor in the data set along with number of other moderately strong predictors
then in the collection of bagged trees most or all of the trees will use this strong predictor in the top split
consequently all of the bagged trees will look quite similar to each other
hence the predictions from the bagged trees will be highly correlated
unfortunately averaging many highly correlated quantities does not lead to as large of reduction in variance as averaging many uncorrelated quanti
bagging random forests boosting ties
in particular this means that bagging will not lead to substantial reduction in variance over single tree in this setting
random forests overcome this problem by forcing each split to consider only subset of the predictors
therefore on average of the splits will not even consider the strong predictor and so other predictors will have more of chance
we can think of this process as decorrelating the trees thereby making the average of the resulting trees less variable and hence more reliable
the main difference between bagging and random forests is the choice of predictor subset size
for instance if random forest is built using then this amounts simply to bagging
on the heart data random forests using leads to reduction in both test error and oob error over bagging figure
using small value of in building random forest will typically be helpful when we have large number of correlated predictors
we applied random forests to high dimensional biological data set consisting of expression measurements of genes measured on tissue samples from patients
there are around genes in humans and individual genes have different levels of activity or expression in particular cells tissues and biological conditions
in this data set each of the patient samples has qualitative label with different levels either normal or one of different types of cancer
our goal was to use random forests to predict cancer type based on the genes that have the largest variance in the training set
we randomly divided the observations into training and test set and applied random forests to the training set for three different values of the number of splitting variables
the results are shown in figure
the error rate of single tree is and the null rate is we see that using trees is sufficient to give good performance and that the choice gave small improvement in test error over bagging in this example
as with bagging random forests will not overfit if we increase so in practice we use value of sufficiently large for the error rate to have settled down
boosting we now discuss boosting yet another approach for improving the predicboosting tions resulting from decision tree
like bagging boosting is general approach that can be applied to many statistical learning methods for regression or classification
here we restrict our discussion of boosting to the context of decision trees
the null rate results from simply classifying each observation to the dominant class overall which is in this case the normal class
results from random forests for the fifteen class gene expression data set with predictors
the test error is displayed as function of the number of trees
each colored line corresponds to different value of the number of predictors available for splitting at each interior tree node
random forests lead to slight improvement over bagging
single classification tree has an error rate of
recall that bagging involves creating multiple copies of the original training data set using the bootstrap fitting separate decision tree to each copy and then combining all of the trees in order to create single predictive model
notably each tree is built on bootstrap data set independent of the other trees
boosting works in similar way except that the trees are grown sequentially each tree is grown using information from previously grown trees
boosting does not involve bootstrap sampling instead each tree is fit on modified version of the original data set
consider first the regression setting
like bagging boosting involves combining large number of decision trees
boosting is described in algorithm
what is the idea behind this procedure
unlike fitting single large decision tree to the data which amounts to fitting the data hard and potentially overfitting the boosting approach instead learns slowly
given the current model we fit decision tree to the residuals from the model
that is we fit tree using the current residuals rather than the outcome as the response
we then add this new decision tree into the fitted function in order to update the residuals
each of these trees can be rather small with just few terminal nodes determined by the parameter in the algorithm
by fitting small trees to the residuals we slowly improve in areas where it does not perform well
the shrinkage parameter bb slows the process down
bagging random forests boosting algorithm boosting for regression trees
set and ri yi for all in the training set
for repeat fit tree with splits terminal nodes to the training data update by adding in shrunken version of the new tree bb
update the residuals ri ri bb xi
output the boosted model bb
even further allowing more and different shaped trees to attack the residuals
in general statistical learning approaches that learn slowly tend to perform well
note that in boosting unlike in bagging the construction of each tree depends strongly on the trees that have already been grown
we have just described the process of boosting regression trees
boosting classification trees proceeds in similar but slightly more complex way and the details are omitted here
boosting has three tuning parameters
the number of trees
unlike bagging and random forests boosting can overfit if is too large although this overfitting tends to occur slowly if at all
we use cross validation to select
the shrinkage parameter bb small positive number
this controls the rate at which boosting learns
typical values are or and the right choice can depend on the problem
very small bb can require using very large value of in order to achieve good performance
the number of splits in each tree which controls the complexity of the boosted ensemble
often works well in which case each tree is stump consisting of single split
in this case the boosted stump ensemble is fitting an additive model since each term involves only single variable
more generally is the interaction depth and controls interaction depth
results from performing boosting and random forests on the fifteen class gene expression data set in order to predict cancer versus normal
the test error is displayed as function of the number of trees
for the two boosted models bb
depth trees slightly outperform depth trees and both outperform the random forest although the standard errors are around making none of these differences significant
the test error rate for single tree is the interaction order of the boosted model since splits can involve at most variables
in figure we applied boosting to the fifteen class cancer gene expression data set in order to develop classifier that can distinguish the normal class from the fourteen cancer classes
we display the test error as function of the total number of trees and the interaction depth
we see that simple stumps with an interaction depth of one perform well if enough of them are included
this model outperforms the depth two model and both outperform random forest
this highlights one difference between boosting and random forests in boosting because the growth of particular tree takes into account the other trees that have already been grown smaller trees are typically sufficient
using smaller trees can aid in interpretability as well for instance using stumps leads to an additive model
lab decision trees
lab decision trees fitting classification trees the tree library is used to construct classification and regression trees library tree we first use classification trees to analyze the carseats data set
in these data sales is continuous variable and so we begin by recoding it as binary variable
we use the ifelse function to create variable called ifelse high which takes on value of yes if the sales variable exceeds and takes on value of no otherwise library islr attach carseats high ifelse sales no yes finally we use the data frame function to merge high with the rest of the carseats data
carseats data frame carseats high we now use the tree function to fit classification tree in order to predict tree high using all variables but sales
the syntax of the tree function is quite similar to that of the lm function tree carseats tree high sales carseats the summary function lists the variables that are used as internal nodes in the tree the number of terminal nodes and the training error rate summary tree carseats tree tree formula high sales data carseats variables actually used in tree shelveloc price income compprice population advertisin age us number of terminal nodes residual mean deviance error rate we see that the training error rate is
for classification trees the deviance reported in the output of summary is given by xx nmk log mk where nmk is the number of observations in the mth terminal node that belong to the kth class
small deviance indicates tree that provides good fit to the training data
the residual mean deviance reported is simply the deviance divided by which in this case is
one of the most attractive properties of trees is that they can be graphically displayed
we use the plot function to display the tree structure and the text function to display the node labels
the argument pretty
tree based methods instructs to include the category names for any qualitative predictors rather than simply displaying letter for each category plot tree carseats text tree carseats pretty the most important indicator of sales appears to be shelving location since the first branch differentiates good locations from bad and medium locations
if we just type the name of the tree object prints output corresponding to each branch of the tree
displays the split criterion
price the number of observations in that branch the deviance the overall prediction for the branch yes or no and the fraction of observations in that branch that take on values of yes and no
branches that lead to terminal nodes are indicated using asterisks tree carseats node split deviance yval yprob denotes terminal node root no shelveloc bad medium no price yes income no in order to properly evaluate the performance of classification tree on these data we must estimate the test error rather than simply computing the training error
we split the observations into training set and test set build the tree using the training set and evaluate its performance on the test data
the predict function can be used for this purpose
in the case of classification tree the argument type class instructs to return the actual class prediction
this approach leads to correct predictions for around of the locations in the test data set set seed train sample nrow carseats carseats test carseats train high test high train tree carseats tree high sales carseats subset train tree pred predict tree carseats carseats test type class table tree pred high test high test tree pred no yes no yes next we consider whether pruning the tree might lead to improved results
the function cv tree performs cross validation in order to detercv tree mine the optimal level of tree complexity cost complexity pruning is used in order to select sequence of trees for consideration
we use the argument
lab decision trees fun prune misclass in order to indicate that we want the classification error rate to guide the cross validation and pruning process rather than the default for the cv tree function which is deviance
the cv tree function reports the number of terminal nodes of each tree considered size as well as the corresponding error rate and the value of the cost complexity parameter used which corresponds to in set seed cv carseats cv tree tree carseats fun prune misclass names cv carseats size dev method cv carseats size dev inf method misclass attr class prune tree sequence note that despite the name dev corresponds to the cross validation error rate in this instance
the tree with nine terminal nodes results in the lowest cross validation error rate with cross validation errors
we plot the error rate as function of both size and par mfrow plot cv carseats size cv carseats dev type plot cv carseats cv carseats dev type we now apply the prune misclass function in order to prune the tree to prune misclass obtain the nine node tree prune carseats prune misclass tree carseats best plot prune carseats text prune carseats pretty how well does this pruned tree perform on the test data set
once again we apply the predict function tree pred predict prune carseats carseats test type class table tree pred high test high test tree pred no yes no yes
tree based methods now of the test observations are correctly classified so not only has the pruning process produced more interpretable tree but it has also improved the classification accuracy
if we increase the value of best we obtain larger pruned tree with lower classification accuracy prune carseats prune misclass tree carseats best plot prune carseats text prune carseats pretty tree pred predict prune carseats carseats test type class table tree pred high test high test tree pred no yes no yes fitting regression trees here we fit regression tree to the boston data set
first we create training set and fit the tree to the training data library mass set seed train sample nrow boston nrow boston tree boston tree medv boston subset train summary tree boston regression tree tree formula medv data boston subset train variables actually used in tree lstat rm dis number of terminal nodes residual mean deviance of residuals min st qu
median mean rd qu
max notice that the output of summary indicates that only three of the variables have been used in constructing the tree
in the context of regression tree the deviance is simply the sum of squared errors for the tree
we now plot the tree plot tree boston text tree boston pretty the variable lstat measures the percentage of individuals with lower socioeconomic status
the tree indicates that lower values of lstat correspond to more expensive houses
the tree predicts median house price of
lab decision trees for larger homes in suburbs in which residents have high socioeconomic status rm and lstat
now we use the cv tree function to see whether pruning the tree will improve performance cv boston cv tree tree boston plot cv boston size cv boston dev type in this case the most complex tree is selected by cross validation
however if we wish to prune the tree we could do so as follows using the prune tree function prune tree prune boston prune tree tree boston best plot prune boston text prune boston pretty in keeping with the cross validation results we use the unpruned tree to make predictions on the test set yhat predict tree boston newdata boston train boston test boston train medv plot yhat boston test abline mean yhat boston test in other words the test set mse associated with the regression tree is
the square root of the mse is therefore around indicating that this model leads to test predictions that are within around of the true median home value for the suburb
bagging and random forests here we apply bagging and random forests to the boston data using the randomforest package in
the exact results obtained in this section may depend on the version of and the version of the randomforest package installed on your computer
recall that bagging is simply special case of random forest with
therefore the randomforest function can randomforest be used to perform both random forests and bagging
we perform bagging as follows library set seed bag boston medv data boston subset train mtry importance true bag boston call formula medv data boston mtry importance true subset train type of random forest regression number of trees
tree based methods no of variables tried at each split mean of squared residuals var explained the argument mtry indicates that all predictors should be considered for each split of the tree in other words that bagging should be done
how well does this bagged model perform on the test set
yhat bag predict bag boston newdata boston train plot yhat bag boston test abline mean yhat bag boston test the test set mse associated with the bagged regression tree is almost half that obtained using using an optimally pruned single tree
we could change the number of trees grown by randomforest using the ntree argument bag boston medv data boston subset train mtry ntree yhat bag predict bag boston newdata boston train mean yhat bag boston test growing random forest proceeds in exactly the same way except that we use smaller value of the mtry argument
by default randomforest uses variables when building random forest of regression trees and variables when building random forest of classification trees
here we use mtry set seed rf boston medv data boston subset train mtry importance true yhat rf predict rf boston newdata boston train mean yhat rf boston test the test set mse is this indicates that random forests yielded an improvement over bagging in this case
using the importance function we can view the importance of each importance variable importance rf boston incmse crim zn indus chas nox rm age dis
lab decision trees rad tax ptratio black lstat two measures of variable importance are reported
the former is based upon the mean decrease of accuracy in predictions on the out of bag samples when given variable is excluded from the model
the latter is measure of the total decrease in node impurity that results from splits over that variable averaged over all trees this was plotted in figure
in the case of regression trees the node impurity is measured by the training rss and for classification trees by the deviance
plots of these importance measures can be produced using the varimpplot function varimpplot varimpplot rf boston the results indicate that across all of the trees considered in the random forest the wealth level of the community lstat and the house size rm are by far the two most important variables
boosting here we use the gbm package and within it the gbm function to fit boosted gbm regression trees to the boston data set
we run gbm with the option distribution gaussian since this is regression problem if it were binary classification problem we would use distribution bernoulli
the argument trees indicates that we want trees and the option interaction depth limits the depth of each tree library gbm set seed boost boston gbm medv data boston train gaussian trees interacti depth the summary function produces relative influence plot and also outputs the relative influence statistics summary boost boston var rel inf lstat rm dis crim nox ptratio black age tax indus chas rad
tree based methods zn we see that lstat and rm are by far the most important variables
we can also produce partial dependence plots for these two variables
these plots partial dependence illustrate the marginal effect of the selected variables on the response after plot integrating out the other variables
in this case as we might expect median house prices are increasing with rm and decreasing with lstat par mfrow plot boost boston rm plot boost boston lstat we now use the boosted model to predict medv on the test set yhat boost predict boost boston newdata boston train trees mean yhat boost boston test the test mse obtained is similar to the test mse for random forests and superior to that for bagging
if we want to we can perform boosting with different value of the shrinkage parameter bb in
the default value is but this is easily modified
here we take bb boost boston gbm medv data boston train gaussian trees interacti depth shrinkage verbose yhat boost predict boost boston newdata boston train trees mean yhat boost boston test in this case using bb leads to slightly lower test mse than bb
exercises conceptual
draw an example of your own invention of partition of twodimensional feature space that could result from recursive binary splitting
your example should contain at least six regions
draw decision tree corresponding to this partition
be sure to label all aspects of your figures including the regions the cutpoints and so forth
hint your result should look something like figures and
it is mentioned in section that boosting using depth one trees or stumps leads to an additive model that is model of the form fj xj
left partition of the predictor space corresponding to exercise
right tree corresponding to exercise
explain why this is the case
you can begin with in algorithm
consider the gini index classification error and cross entropy in simple classification setting with two classes
create single plot that displays each of these quantities as function of
the xaxis should display ranging from to and the axis should display the value of the gini index classification error and entropy
hint in setting with two classes
you could make this plot by hand but it will be much easier to make in
this question relates to the plots in figure sketch the tree corresponding to the partition of the predictor space illustrated in the left hand panel of figure
the numbers inside the boxes indicate the mean of within each region create diagram similar to the left hand panel of figure using the tree illustrated in the right hand panel of the same figure
you should divide up the predictor space into the correct regions and indicate the mean for each region
suppose we produce bootstrapped samples from data set containing red and green classes
we then apply classification tree to each bootstrapped sample and for specific value of produce estimates of class is red and
there are two common ways to combine these results together into single class prediction
one is the majority vote approach discussed in this chapter
the second approach is to classify based on the average
tree based methods probability
in this example what is the final classification under each of these two approaches
provide detailed explanation of the algorithm that is used to fit regression tree
in the lab we applied random forests to the boston data using mtry and using ntree and ntree
create plot displaying the test error resulting from random forests on this data set for more comprehensive range of values for mtry and ntree
you can model your plot after figure
describe the results obtained
in the lab classification tree was applied to the carseats data set after converting sales into qualitative response variable
now we will seek to predict sales using regression trees and related approaches treating the response as quantitative variable split the data set into training set and test set fit regression tree to the training set
plot the tree and interpret the results
what test error rate do you obtain
use cross validation in order to determine the optimal level of tree complexity
does pruning the tree improve the test error rate
use the bagging approach in order to analyze this data
what test error rate do you obtain
use the importance function to determine which variables are most important use random forests to analyze this data
what test error rate do you obtain
use the importance function to determine which variables are most important
describe the effect of the number of variables considered at each split on the error rate obtained
this problem involves the oj data set which is part of the islr package create training set containing random sample of observations and test set containing the remaining observations fit tree to the training data with purchase as the response and the other variables as predictors
use the summary function to produce summary statistics about the tree and describe the results obtained
what is the training error rate
how many terminal nodes does the tree have
exercises type in the name of the tree object in order to get detailed text output
pick one of the terminal nodes and interpret the information displayed create plot of the tree and interpret the results predict the response on the test data and produce confusion matrix comparing the test labels to the predicted test labels
what is the test error rate
apply the cv tree function to the training set in order to determine the optimal tree size produce plot with tree size on the axis and cross validated classification error rate on the axis which tree size corresponds to the lowest cross validated classification error rate
produce pruned tree corresponding to the optimal tree size obtained using cross validation compare the training error rates between the pruned and unpruned trees
which is higher
compare the test error rates between the pruned and unpruned trees
which is higher
we now use boosting to predict salary in the hitters data set remove the observations for whom the salary information is unknown and then log transform the salaries create training set consisting of the first observations and test set consisting of the remaining observations perform boosting on the training set with trees for range of values of the shrinkage parameter bb
produce plot with different shrinkage values on the axis and the corresponding training set mse on the axis produce plot with different shrinkage values on the axis and the corresponding test set mse on the axis compare the test mse of boosting to the test mse that results from applying two of the regression approaches seen in chapters and which variables appear to be the most important predictors in the boosted model
now apply bagging to the training set
what is the test set mse for this approach
this question uses the caravan data set
tree based methods create training set consisting of the first observations and test set consisting of the remaining observations fit boosting model to the training set with purchase as the response and the other variables as predictors
use trees and shrinkage value of
which predictors appear to be the most important
use the boosting model to predict the response on the test data
predict that person will make purchase if the estimated probability of purchase is greater than
form confusion matrix
what fraction of the people predicted to make purchase do in fact make one
how does this compare with the results obtained from applying knn or logistic regression to this data set
apply boosting bagging and random forests to data set of your choice
be sure to fit the models on training set and to evaluate their performance on test set
how accurate are the results compared to simple methods like linear or logistic regression
which of these approaches yields the best performance
this is page printer opaque this support vector machines in this chapter we discuss the support vector machine svm an approach for classification that was developed in the computer science community in the and that has grown in popularity since then
svms have been shown to perform well in variety of settings and are often considered one of the best out of the box classifiers
the support vector machine is generalization of simple and intuitive classifier called the maximal margin classifier which we introduce in section
though it is elegant and simple we will see that this classifier unfortunately cannot be applied to most data sets since it requires that the classes be separable by linear boundary
in section we introduce the support vector classifier an extension of the maximal margin classifier that can be applied in broader range of cases
section introduces the support vector machine which is further extension of the support vector classifier in order to accommodate non linear class boundaries
support vector machines are intended for the binary classification setting in which there are two classes in section we discuss extensions of support vector machines to the case of more than two classes
in section we discuss the close connections between support vector machines and other statistical methods such as logistic regression
people often loosely refer to the maximal margin classifier the support vector classifier and the support vector machine as support vector machines
to avoid confusion we will carefully distinguish between these three notions in this chapter
support vector machines maximal margin classifier in this section we define hyperplane and introduce the concept of an optimal separating hyperplane
what is hyperplane
in dimensional space hyperplane is flat affine subspace of dimenhyperplane sion for instance in two dimensions hyperplane is flat onedimensional subspace in other words line
in three dimensions hyperplane is flat two dimensional subspace that is plane
in dimensions it can be hard to visualize hyperplane but the notion of dimensional flat subspace still applies
the mathematical definition of hyperplane is quite simple
in two dimensions hyperplane is defined by the equation for parameters and
when we say that defines the hyperplane we mean that any for which holds is point on the hyperplane
note that is simply the equation of line since indeed in two dimensions hyperplane is line
equation can be easily extended to the dimensional setting
defines dimensional hyperplane again in the sense that if point xp in dimensional space vector of length satisfies then lies on the hyperplane
now suppose that does not satisfy rather
xp
then this tells us that lies to one side of the hyperplane
on the other hand if
xp then lies on the other side of the hyperplane
so we can think of the hyperplane as dividing dimensional space into two halves
one can easily determine on which side of the hyperplane point lies by simply calculating the sign of the left hand side of
hyperplane in two dimensional space is shown in figure
the word affine indicates that the subspace need not pass through the origin
the hyperplane is shown
the blue region is the set of points for which and the purple region is the set of points for which
classification using separating hyperplane now suppose that we have data matrix that consists of training observations in dimensional space eb eb xn ec ec ed
xn ed
xnp and that these observations fall into two classes that is yn where represents one class and the other class
we also have test observation vector of observed features
our goal is to develop classifier based on the training data that will correctly classify the test observation using its feature measurements
we have seen number of approaches for this task such as linear discriminant analysis and logistic regression in chapter and classification trees bagging and boosting in chapter
we will now see new approach that is based upon the concept of separating hyperplane separating suppose that it is possible to construct hyperplane that separates the hyperplane training observations perfectly according to their class labels
examples of three such separating hyperplanes are shown in the left hand panel of figure
we can label the observations from the blue class as yi and those from the purple class as yi
then separating hyperplane has
left there are two classes of observations shown in blue and in purple each of which has measurements on two variables
three separating hyperplanes out of many possible are shown in black
right separating hyperplane is shown in black
the blue and purple grid indicates the decision rule made by classifier based on this separating hyperplane test observation that falls in the blue portion of the grid will be assigned to the blue class and test observation that falls into the purple portion of the grid will be assigned to the purple class the property that xi xi
xip if yi and xi xi
xip if yi
equivalently separating hyperplane has the property that yi xi xi
xip for all
if separating hyperplane exists we can use it to construct very natural classifier test observation is assigned class depending on which side of the hyperplane it is located
the right hand panel of figure shows an example of such classifier
that is we classify the test observation based on the sign of

if is positive then we assign the test observation to class and if is negative then we assign it to class
we can also make use of the magnitude of
if is far from zero then this means that lies far from the hyperplane and so we can be confident about our class assignment for
on the other hand if is close to zero then is located near the hyperplane and so we are less certain about the class assignment for
not surprisingly and as we see in figure classifier that is based on separating hyperplane leads to linear decision boundary
maximal margin classifier the maximal margin classifier in general if our data can be perfectly separated using hyperplane then there will in fact exist an infinite number of such hyperplanes
this is because given separating hyperplane can usually be shifted tiny bit up or down or rotated without coming into contact with any of the observations
three possible separating hyperplanes are shown in the left hand panel of figure
in order to construct classifier based upon separating hyperplane we must have reasonable way to decide which of the infinite possible separating hyperplanes to use
natural choice is the maximal margin hyperplane also known as the maximal margin optimal separating hyperplane which is the separating hyperplane that is hyperplane farthest from the training observations
that is we can compute the per optimal separating pendicular distance from each training observation to given separating hyperplane hyperplane the smallest such distance is the minimal distance from the observations to the hyperplane and is known as the margin
the maximargin mal margin hyperplane is the separating hyperplane for which the margin is largest that is it is the hyperplane that has the farthest minimum distance to the training observations
we can then classify test observation based on which side of the maximal margin hyperplane it lies
this is known as the maximal margin classifier
we hope that classifier that has maximal margin large margin on the training data will also have large margin on the test classifier data and hence will classify the test observations correctly
although the maximal margin classifier is often successful it can also lead to overfitting when is large
if are the coefficients of the maximal margin hyperplane then the maximal margin classifier classifies the test observation based on the sign of

figure shows the maximal margin hyperplane on the data set of figure
comparing the right hand panel of figure to figure we see that the maximal margin hyperplane shown in figure does indeed result in greater minimal distance between the observations and the separating hyperplane that is larger margin
in sense the maximal margin hyperplane represents the mid line of the widest slab that we can insert between the two classes
examining figure we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin
these three observations are known as support vectors since they are vectors in dimensional space in figure support vector and they support the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well
interestingly the maximal margin hyperplane depends directly on the support vectors but not on the other observations movement to any of the other observations would not affect the separating hyperplane provided that the observation's movement does not cause it to
there are two classes of observations shown in blue and in purple
the maximal margin hyperplane is shown as solid line
the margin is the distance from the solid line to either of the dashed lines
the two blue points and the purple point that lie on the dashed lines are the support vectors and the distance from those points to the margin is indicated by arrows
the purple and blue grid indicates the decision rule made by classifier based on this separating hyperplane cross the boundary set by the margin
the fact that the maximal margin hyperplane depends directly on only small subset of the observations is an important property that will arise later in this chapter when we discuss the support vector classifier and support vector machines
construction of the maximal margin classifier we now consider the task of constructing the maximal margin hyperplane based on set of training observations xn rp and associated class labels yn
briefly the maximal margin hyperplane is the solution to the optimization problem maximize subject to yi xi xi
xip
support vector classifiers this optimization problem is actually simpler than it looks
first of all the constraint in that yi xi xi
xip guarantees that each observation will be on the correct side of the hyperplane provided that is positive
actually for each observation to be on the correct side of the hyperplane we would simply need yi xi xi
xip so the constraint in in fact requires that each observation be on the correct side of the hyperplane with some cushion provided that is positive
second note that is not really constraint on the hyperplane since if xi xi
xip defines hyperplane then so does xi xi
xip for any
however adds meaning to one can show that with this constraint the perpendicular distance from the ith observation to the hyperplane is given by yi xi xi
xip
therefore the constraints and ensure that each observation is on the correct side of the hyperplane and at least distance from the hyperplane
hence represents the margin of our hyperplane and the optimization problem chooses to maximize
this is exactly the definition of the maximal margin hyperplane
the problem can be solved efficiently but details of this optimization are outside of the scope of this book
the non separable case the maximal margin classifier is very natural way to perform classification if separating hyperplane exists
however as we have hinted in many cases no separating hyperplane exists and so there is no maximal margin classifier
in this case the optimization problem has no solution with
an example is shown in figure
in this case we cannot exactly separate the two classes
however as we will see in the next section we can extend the concept of separating hyperplane in order to develop hyperplane that almost separates the classes using so called soft margin
the generalization of the maximal margin classifier to the non separable case is known as the support vector classifier
support vector classifiers overview of the support vector classifier in figure we see that observations that belong to two classes are not necessarily separable by hyperplane
in fact even if separating hyper
there are two classes of observations shown in blue and in purple
in this case the two classes are not separable by hyperplane and so the maximal margin classifier cannot be used plane does exist then there are instances in which classifier based on separating hyperplane might not be desirable
classifier based on separating hyperplane will necessarily perfectly classify all of the training observations this can lead to sensitivity to individual observations
an example is shown in figure
the addition of single observation in the right hand panel of figure leads to dramatic change in the maximal margin hyperplane
the resulting maximal margin hyperplane is not satisfactory for one thing it has only tiny margin
this is problematic because as discussed previously the distance of an observation from the hyperplane can be seen as measure of our confidence that the observation was correctly classified
moreover the fact that the maximal margin hyperplane is extremely sensitive to change in single observation suggests that it may have overfit the training data
that is it could be worthwhile to misclassify few training observations in order to do better job in classifying the remaining observations
the support vector classifier sometimes called soft margin classifier support vector does exactly this
rather than seeking the largest possible margin so that classifier soft margin classifier
left two classes of observations are shown in blue and in purple along with the maximal margin hyperplane
right an additional blue observation has been added leading to dramatic shift in the maximal margin hyperplane shown as solid line
the dashed line indicates the maximal margin hyperplane that was obtained in the absence of this additional point every observation is not only on the correct side of the hyperplane but also on the correct side of the margin we instead allow some observations to be on the incorrect side of the margin or even the incorrect side of the hyperplane
the margin is soft because it can be violated by some of the training observations
an example is shown in the left hand panel of figure
most of the observations are on the correct side of the margin
however small subset of the observations are on the wrong side of the margin
an observation can be not only on the wrong side of the margin but also on the wrong side of the hyperplane
in fact when there is no separating hyperplane such situation is inevitable
observations on the wrong side of the hyperplane correspond to training observations that are misclassified by the support vector classifier
the right hand panel of figure illustrates such scenario
details of the support vector classifier the support vector classifier classifies test observation depending on which side of hyperplane it lies
the hyperplane is chosen to correctly separate most of the training observations into the two classes but may
left support vector classifier was fit to small data set
the hyperplane is shown as solid line and the margins are shown as dashed lines
purple observations observations and are on the correct side of the margin observation is on the margin and observation is on the wrong side of the margin
blue observations observations and are on the correct side of the margin observation is on the margin and observation is on the wrong side of the margin
no observations are on the wrong side of the hyperplane
right same as left panel with two additional points and
these two observations are on the wrong side of the hyperplane and the wrong side of the margin misclassify few observations
it is the solution to the optimization problem maximize on subject to yi xi xi
xip oi oi oi where is nonnegative tuning parameter
as in is the width of the margin we seek to make this quantity as large as possible
in on are slack variables that allow individual observations to be on slack variable the wrong side of the margin or the hyperplane we will explain them in greater detail momentarily
once we have solved we classify test observation as before by simply determining on which side of the hyperplane it lies
that is we classify the test observation based on the sign of

the problem seems complex but insight into its behavior can be made through series of simple observations presented below
first of all the slack variable oi tells us where the ith observation is located
support vector classifiers relative to the hyperplane and relative to the margin
if oi then the ith observation is on the correct side of the margin as we saw in section
if oi then the ith observation is on the wrong side of the margin and we say that the ith observation has violated the margin
if oi then it is on the wrong side of the hyperplane
we now consider the role of the tuning parameter
in bounds the sum of the oi and so it determines the number and severity of the violations to the margin and to the hyperplane that we will tolerate
we can think of as budget for the amount that the margin can be violated by the observations
if then there is no budget for violations to the margin and it must be the case that on in which case simply amounts to the maximal margin hyperplane optimization problem
of course maximal margin hyperplane exists only if the two classes are separable
for no more than observations can be on the wrong side of the hyperplane because if an observation is onp the wrong side of the hyperplane then oi and requires that oi
as the budget increases we become more tolerant of violations to the margin and so the margin will widen
conversely as decreases we become less tolerant of violations to the margin and so the margin narrows
an example in shown in figure
in practice is treated as tuning parameter that is generally chosen via cross validation
as with the tuning parameters that we have seen throughout this book controls the bias variance trade off of the statistical learning technique
when is small we seek narrow margins that are rarely violated this amounts to classifier that is highly fit to the data which may have low bias but high variance
on the other hand when is larger the margin is wider and we allow more violations to it this amounts to fitting the data less hard and obtaining classifier that is potentially more biased but may have lower variance
the optimization problem has very interesting property it turns out that only observations that either lie on the margin or that violate the margin will affect the hyperplane and hence the classifier obtained
in other words an observation that lies strictly on the correct side of the margin does not affect the support vector classifier
changing the position of that observation would not change the classifier at all provided that its position remains on the correct side of the margin
observations that lie directly on the margin or on the wrong side of the margin for their class are known as support vectors
these observations do affect the support vector classifier
the fact that only support vectors affect the classifier is in line with our previous assertion that controls the bias variance trade off of the support vector classifier
when the tuning parameter is large then the margin is wide many observations violate the margin and so there are many support vectors
in this case many observations are involved in determining the hyperplane
the top left panel in figure illustrates this setting this
support vector classifier was fit using four different values of the tuning parameter in
the largest value of was used in the top left panel and smaller values were used in the top right bottom left and bottom right panels
when is large then there is high tolerance for observations being on the wrong side of the margin and so the margin will be large
as decreases the tolerance for observations being on the wrong side of the margin decreases and the margin narrows
support vector machines classifier has low variance since many observations are support vectors but potentially high bias
in contrast if is small then there will be fewer support vectors and hence the resulting classifier will have low bias but high variance
the bottom right panel in figure illustrates this setting with only eight support vectors
the fact that the support vector classifier's decision rule is based only on potentially small subset of the training observations the support vectors means that it is quite robust to the behavior of observations that are far away from the hyperplane
this property is distinct from some of the other classification methods that we have seen in preceding chapters such as linear discriminant analysis
recall that the lda classification rule depends on the mean of all of the observations within each class as well as the within class covariance matrix computed using all of the observations
in contrast logistic regression unlike lda has very low sensitivity to observations far from the decision boundary
in fact we will see in section that the support vector classifier and logistic regression are closely related
support vector machines we first discuss general mechanism for converting linear classifier into one that produces non linear decision boundaries
we then introduce the support vector machine which does this in an automatic way
classification with non linear decision boundaries the support vector classifier is natural approach for classification in the two class setting if the boundary between the two classes is linear
however in practice we are sometimes faced with non linear class boundaries
for instance consider the data in the left hand panel of figure
it is clear that support vector classifier or any linear classifier will perform poorly here
indeed the support vector classifier shown in the right hand panel of figure is useless here
in chapter we are faced with an analogous situation
we see there that the performance of linear regression can suffer when there is nonlinear relationship between the predictors and the outcome
in that case we consider enlarging the feature space using functions of the predictors such as quadratic and cubic terms in order to address this non linearity
in the case of the support vector classifier we could address the problem of possibly non linear boundaries between classes in similar way by enlarging the feature space using quadratic cubic and even higher order polynomial functions of the predictors
for instance rather than fitting support vector classifier using features xp
left the observations fall into two classes with non linear boundary between them
right the support vector classifier seeks linear boundary and consequently performs very poorly we could instead fit support vector classifier using features xp xp
in the enlarged feature space the decision boundary that results from is in fact linear
but in the original feature space the decision boundary is of the form where is quadratic polynomial and its solutions are generally non linear
one might additionally want to enlarge the feature space with higher order polynomial terms or with interaction terms of the form xj xj for
alternatively other functions of the predictors could be considered rather than polynomials
it is not hard to see that there are many possible ways to enlarge the feature space and that unless we are careful we could end up with huge number of features
then computations would become unmanageable
the support vector machine which we present next allows us to enlarge the feature space used by the support vector classifier in way that leads to efficient computations
support vector machines the support vector machine the support vector machine svm is an extension of the support vector support vector classifier that results from enlarging the feature space in specific way machine using kernels
we will now discuss this extension the details of which are kernel somewhat complex and beyond the scope of this book
however the main idea is described in section we may want to enlarge our feature space in order to accommodate non linear boundary between the classes
the kernel approach that we describe here is simply an efficient computational approach for enacting this idea
we have not discussed exactly how the support vector classifier is computed because the details become somewhat technical
however it turns out that the solution to the support vector classifier problem involves only the inner products of the observations as opposed to the observations themselves
pr the inner product of two vectors and is defined as ha bi ai bi
thus the inner product of two observations xi xi is given by hxi xi xij xi
notice that in in order to evaluate the function we need to compute the inner product between the new point and each of the training points xi
however it turns out that is nonzero only for the support vectors in the solution that is if training observation is not support vector then its equals zero
so if is the collection of indices of these support points we can rewrite any solution function of the form as hx xi
support vector machines which typically involves far fewer terms than in to summarize in representing the linear classifier and in computing its coefficients all we need are inner products
now suppose that every time the inner product appears in the representation or in calculation of the solution for the support vector classifier we replace it with generalization of the inner product of the form xi xi where is some function that we will refer to as kernel
kernel is kernel function that quantifies the similarity of two observations
for instance we could simply take xi xi xij xi which would just give us back the support vector classifier
equation is known as linear kernel because the support vector classifier is linear in the features the linear kernel essentially quantifies the similarity of pair of observations using pearson standard correlation
but one could instead choose another pp form for
for instance one could replace every instance of xij xi with the quantity xi xi xij xi
this is known as polynomial kernel of degree where is positive polynomial kernel integer
using such kernel with instead of the standard linear kernel in the support vector classifier algorithm leads to much more flexible decision boundary
it essentially amounts to fitting support vector classifier in higher dimensional space involving polynomials of degree rather than in the original feature space
when the support vector classifier is combined with non linear kernel such as the resulting classifier is known as support vector machine
note that in this case the non linear function has the form xi
the left hand panel of figure shows an example of an svm with polynomial kernel applied to the non linear data from figure
the fit is substantial improvement over the linear support vector classifier
when then the svm reduces to the support vector classifier seen earlier in this chapter
by expanding each of the inner products in it is easy to see that is linear function of the coordinates of
doing so also establishes the correspondence between the and the original parameters
left an svm with polynomial kernel of degree is applied to the non linear data from figure resulting in far more appropriate decision rule
right an svm with radial kernel is applied
in this example either kernel is capable of capturing the decision boundary
the polynomial kernel shown in is one example of possible nonlinear kernel but alternatives abound
another popular choice is the radial kernel which takes the form radial kernel xi xi exp xij xi
in is positive constant
the right hand panel of figure shows an example of an svm with radial kernel on this non linear data it also does good job in separating the two classes
how does the radial kernel actually work
if given test observation is far from training observation xi in terms of euclidean distance then xij will be large and so xi pp exp xij will be very tiny
this means that in xi will play virtually no role in
recall that the predicted class label for the test observation is based on the sign of
in other words training observations that are far from will play essentially no role in the predicted class label for
this means that the radial kernel has very local behavior in the sense that only nearby training observations have an effect on the class label of test observation
what is the advantage of using kernel rather than simply enlarging the feature space using functions of the original features as in
one advantage is computational and it amounts to the fact that using kernels one need only compute xi for all distinct pairs
this can be done without explicitly working in the enlarged feature space
this is important because in many applications of svms the enlarged feature space
roc curves for the heart data training set
left the support vector classifier and lda are compared
right the support vector classifier is compared to an svm using radial basis kernel with and is so large that computations are intractable
for some kernels such as the radial kernel the feature space is implicit and infinite dimensional so we could never do the computations there anyway
an application to the heart disease data in chapter we apply decision trees and related methods to the heart data
the aim is to use predictors such as age sex and chol in order to predict whether an individual has heart disease
we now investigate how an svm compares to lda on this data
the data consist of subjects which we randomly split into training and test observations
we first fit lda and the support vector classifier to the training data
note that the support vector classifier is equivalent to svm using polynomial kernel of degree
the left hand panel of figure displays roc curves described in section for the training set predictions for both lda and the support vector classifier
both classifiers compute scores of the form
xp for each observation
for any given cutoff we classify observations into the heart disease or no heart disease categories depending on whether or
the roc curve is obtained by forming these predictions and computing the false positive and true positive rates for range of values of
an optimal classifier will hug the top left corner of the roc plot
in this instance lda and the support vector classifier both perform well though there is suggestion that the support vector classifier may be slightly superior
the right hand panel of figure displays roc curves for svms using radial kernel with various values of
as increases and the fit becomes more non linear the roc curves improve
using appears to give
roc curves for the test set of the heart data
left the support vector classifier and lda are compared
right the support vector classifier is compared to an svm using radial basis kernel with and an almost perfect roc curve
however these curves represent training error rates which can be misleading in terms of performance on new test data
figure displays roc curves computed on the test observations
we observe some differences from the training roc curves
in the left hand panel of figure the support vector classifier appears to have small advantage over lda although these differences are not statistically significant
in the right hand panel the svm using which showed the best results on the training data produces the worst estimates on the test data
this is once again evidence that while more flexible method will often produce lower training error rates this does not necessarily lead to improved performance on test data
the svms with and perform comparably to the support vector classifier and all three outperform the svm with
svms with more than two classes so far our discussion has been limited to the case of binary classification that is classification in the two class setting
how can we extend svms to the more general case where we have some arbitrary number of classes
it turns out that the concept of separating hyperplanes upon which svms are based does not lend itself naturally to more than two classes
though number of proposals for extending svms to the class case have been made the two most popular are the one versus one and one versus all approaches
we briefly discuss those two approaches here
support vector machines one versus one classification suppose that we would like to perform classification using svms and there are classes
one versus one or all pairs approach constructs one versus one svms each of which compares pair of classes
for example one such svm might compare the kth class coded as to the th class coded as
we classify test observation using each of the classifiers and we tally the number of times that the test observation is assigned to each of the classes
the final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these pairwise classifications
one versus all classification the one versus all approach is an alternative procedure for applying svms one versus all in the case of classes
we fit svms each time comparing one of the classes to the remaining classes
let pk denote the parameters that result from fitting an svm comparing the kth class coded as to the others coded as
let denote test observation
we assign the observation to the class for which
pk is largest as this amounts to high level of confidence that the test observation belongs to the kth class rather than to any of the other classes
relationship to logistic regression when svms were first introduced in the mid they made quite splash in the statistical and machine learning communities
this was due in part to their good performance good marketing and also to the fact that the underlying approach seemed both novel and mysterious
the idea of finding hyperplane that separates the data as well as possible while allowing some violations to this separation seemed distinctly different from classical approaches for classification such as logistic regression and linear discriminant analysis
moreover the idea of using kernel to expand the feature space in order to accommodate non linear class boundaries appeared to be unique and valuable characteristic
however since that time deep connections between svms and other more classical statistical methods have emerged
it turns out that one can rewrite the criterion for fitting the support vector classifier
xp as fc xn fd minimize max yi xi bb fe
relationship to logistic regression where bb is nonnegative tuning parameter
when bb is large then are small more violations to the margin are tolerated and low variance but high bias classifier will result
when bb is small then few violations to the margin will occur this amounts to high variance but low bias pp of bb in amounts to small value of classifier
thus small value in
note that the bb term in is the ridge penalty term from section and plays similar role in controlling the bias variance trade off for the support vector classifier
now takes the loss penalty form that we have seen repeatedly throughout this book minimize bb
in is some loss function quantifying the extent to which the model parametrized by fits the data and is penalty function on the parameter vector whose effect is controlled by nonnegative tuning parameter bb
for instance ridge regression and the lasso both take this form with eb xn ed xij pp pp and with for ridge regression and for the lasso
in the case of the loss function instead takes the form max yi xi
xip this is known as hinge loss and is depicted in figure
however it hinge loss turns out that the hinge loss function is closely related to the loss function used in logistic regression also shown in figure
an interesting characteristic of the support vector classifier is that only support vectors play role in the classifier obtained observations on the correct side of the margin do not affect it
this is due to the fact that the loss function shown in figure is exactly zero for observations for which yi xi
xip these correspond to observations that are on the correct side of the margin in contrast the loss function for logistic regression shown in figure is not exactly zero anywhere
but it is very small for observations that are far from the decision boundary
due to the similarities between their loss functions logistic regression and the support vector classifier often give very similar results
when the classes are well separated svms tend to behave better than logistic regression in more overlapping regimes logistic regression is often preferred
with this hinge loss penalty representation the margin corresponds to the value one and the width of the margin is determined by
the svm and logistic regression loss functions are compared as function of yi xi
xip
when yi xi
xip is greater than then the svm loss is zero since this corresponds to an observation that is on the correct side of the margin
overall the two loss functions have quite similar behavior
when the support vector classifier and svm were first introduced it was thought that the tuning parameter in was an unimportant nuisance parameter that could be set to some default value like
however the loss penalty formulation for the support vector classifier indicates that this is not the case
the choice of tuning parameter is very important and determines the extent to which the model underfits or overfits the data as illustrated for example in figure
we have established that the support vector classifier is closely related to logistic regression and other preexisting statistical methods
is the svm unique in its use of kernels to enlarge the feature space to accommodate non linear class boundaries
the answer to this question is no
we could just as well perform logistic regression or many of the other classification methods seen in this book using non linear kernels this is closely related to some of the non linear approaches seen in chapter
however for historical reasons the use of non linear kernels is much more widespread in the context of svms than in the context of logistic regression or other methods
though we have not addressed it here there is in fact an extension of the svm for regression for quantitative rather than qualitative response called support vector regression
in chapter we saw that support vector least squares regression seeks coefficients such that the sum regression of squared residuals is as small as possible
recall from chapter that residuals are defined as yi xi xip
support vector re
lab support vector machines gression instead seeks coefficients that minimize different type of loss where only residuals larger in absolute value than some positive constant contribute to the loss function
this is an extension of the margin used in support vector classifiers to the regression setting
lab support vector machines we use the library in to demonstrate the support vector classifier and the svm
another option is the liblinear library which is useful for very large linear problems
support vector classifier the library contains implementations for number of statistical learning methods
in particular the svm function can be used to fit svm support vector classifier when the argument kernel linear is used
this function uses slightly different formulation from and for the support vector classifier
cost argument allows us to specify the cost of violation to the margin
when the cost argument is small then the margins will be wide and many support vectors will be on the margin or will violate the margin
when the cost argument is large then the margins will be narrow and there will be few support vectors on the margin or violating the margin
we now use the svm function to fit the support vector classifier for given value of the cost parameter
here we demonstrate the use of this function on two dimensional example so that we can plot the resulting decision boundary
we begin by generating the observations which belong to two classes set seed matrix rnorm ncol rep rep we begin by checking whether the classes are linearly separable plot col they are not
next we fit the support vector classifier
note that in order for the svm function to perform classification as opposed to svm based regression we must encode the response as factor variable
we now create data frame with the response coded as factor dat data frame as factor library svmfit svm data dat kernel linear cost scale false
support vector machines the argument scale false tells the svm function not to scale each feature to have mean zero or standard deviation one depending on the application one might prefer to use scale true
we can now plot the support vector classifier obtained plot svmfit dat note that the two arguments to the plot svm function are the output of the call to svm as well as the data used in the call to svm
the region of feature space that will be assigned to the class is shown in light blue and the region that will be assigned to the class is shown in purple
the decision boundary between the two classes is linear because we used the argument kernel linear though due to the way in which the plotting function is implemented in this library the decision boundary looks somewhat jagged in the plot
we see that in this case only one observation is misclassified
note that here the second feature is plotted on the axis and the first feature is plotted on the axis in contrast to the behavior of the usual plot function in
the support vectors are plotted as crosses and the remaining observations are plotted as circles we see here that there are seven support vectors
we can determine their identities as follows svmfit index we can obtain some basic information about the support vector classifier fit using the summary command summary svmfit call svm formula data dat kernel linear cost scale false parameters svm type svm kernel linear cost gamma number of support vectors number of classes levels this tells us for instance that linear kernel was used with cost and that there were seven support vectors four in one class and three in the other
what if we instead used smaller value of the cost parameter
svmfit svm data dat kernel linear cost scale false plot svmfit dat svmfit index
lab support vector machines now that smaller value of the cost parameter is being used we obtain larger number of support vectors because the margin is now wider
unfortunately the svm function does not explicitly output the coefficients of the linear decision boundary obtained when the support vector classifier is fit nor does it output the width of the margin
the library includes built in function tune to perform crosstune validation
by default tune performs ten fold cross validation on set of models of interest
in order to use this function we pass in relevant information about the set of models that are under consideration
the following command indicates that we want to compare svms with linear kernel using range of values of the cost parameter set seed tune out tune svm data dat kernel linear ranges list cost we can easily access the cross validation errors for each of these models using the summary command summary tune out parameter tuning of svm sampling method fold cross validatio best parameters cost best performanc detailed performan results cost error dispersion we see that cost results in the lowest cross validation error rate
the tune function stores the best model obtained which can be accessed as follows bestmod tune out best model summary bestmod the predict function can be used to predict the class label on set of test observations at any given value of the cost parameter
we begin by generating test data set xtest matrix rnorm ncol ytest sample rep true xtest ytest xtest ytest testdat data frame xtest as factor ytest now we predict the class labels of these test observations
here we use the best model obtained through cross validation in order to make predictions
support vector machines ypred predict bestmod testdat table predict ypred truth testdat truth predict thus with this value of cost of the test observations are correctly classified
what if we had instead used cost
svmfit svm data dat kernel linear cost scale false ypred predict svmfit testdat table predict ypred truth testdat truth predict in this case one additional observation is misclassified
now consider situation in which the two classes are linearly separable
then we can find separating hyperplane using the svm function
we first further separate the two classes in our simulated data so that they are linearly separable plot col pch now the observations are just barely linearly separable
we fit the support vector classifier and plot the resulting hyperplane using very large value of cost so that no observations are misclassified dat data frame as factor svmfit svm data dat kernel linear cost summary svmfit call svm formula data dat kernel linear cost parameters svm type svm kernel linear cost gamma number of support vectors number of classes levels plot svmfit dat no training errors were made and only three support vectors were used
however we can see from the figure that the margin is very narrow because the observations that are not support vectors indicated as circles are very
lab support vector machines close to the decision boundary
it seems likely that this model will perform poorly on test data
we now try smaller value of cost svmfit svm data dat kernel linear cost summary svmfit plot svmfit dat using cost we misclassify training observation but we also obtain much wider margin and make use of seven support vectors
it seems likely that this model will perform better on test data than the model with cost
support vector machine in order to fit an svm using non linear kernel we once again use the svm function
however now we use different value of the parameter kernel
to fit an svm with polynomial kernel we use kernel polynomial and to fit an svm with radial kernel we use kernel radial
in the former case we also use the degree argument to specify degree for the polynomial kernel this is in and in the latter case we use gamma to specify value of for the radial basis kernel
we first generate some data with non linear class boundary as follows set seed matrix rnorm ncol rep rep dat data frame as factor plotting the data makes it clear that the class boundary is indeed nonlinear plot col the data is randomly split into training and testing groups
we then fit the training data using the svm function with radial kernel and train sample svmfit svm data dat train kernel radial gamma cost plot svmfit dat train the plot shows that the resulting svm has decidedly non linear boundary
the summary function can be used to obtain some information about the svm fit summary svmfit call svm formula data dat kernel radial gamma cost parameters svm type
support vector machines svm kernel radial cost gamma number of support vectors number of classes levels we can see from the figure that there are fair number of training errors in this svm fit
if we increase the value of cost we can reduce the number of training errors
however this comes at the price of more irregular decision boundary that seems to be at risk of overfitting the data svmfit svm data dat train kernel radial gamma cost plot svmfit dat train we can perform cross validation using tune to select the best choice of and cost for an svm with radial kernel set seed tune out tune svm data dat train kernel radial ranges list cost gamma summary tune out parameter tuning of svm sampling method fold cross validatio best parameters cost gamma best performanc detailed performan results cost gamma error dispersio
therefore the best choice of parameters involves cost and gamma
we can view the test set predictions for this model by applying the predict function to the data
notice that to do this we subset the dataframe dat using train as an index set table true dat train pred predict tune out best model newx dat train of test observations are misclassified by this svm
lab support vector machines roc curves the rocr package can be used to produce roc curves such as those in figures and
we first write short function to plot an roc curve given vector containing numerical score for each observation pred and vector containing the class label for each observation truth library rocr rocplot function pred truth predob prediction pred truth perf performan predob tpr fpr plot perf
svms and support vector classifiers output class labels for each observation
however it is also possible to obtain fitted values for each observation which are the numerical scores used to obtain the class labels
for instance in the case of support vector classifier the fitted value for an observation xp takes the form
xp
for an svm with non linear kernel the equation that yields the fitted value is given in
in essence the sign of the fitted value determines on which side of the decision boundary the observation lies
therefore the relationship between the fitted value and the class prediction for given observation is simple if the fitted value exceeds zero then the observation is assigned to one class and if it is less than zero than it is assigned to the other
in order to obtain the fitted values for given svm model fit we use decision values true when fitting svm
then the predict function will output the fitted values svmfit opt svm data dat train kernel radial gamma cost decision values fitted attribute predict svmfit opt dat train decision values true decision values now we can produce the roc plot par mfrow rocplot fitted dat train main training data svm appears to be producing accurate predictions
by increasing we can produce more flexible fit and generate further improvements in accuracy svmfit flex svm data dat train kernel radial gamma cost decision values fitted attribute predict svmfit flex dat train decision values decision values rocplot fitted dat train add col red however these roc curves are all on the training data
we are really more interested in the level of prediction accuracy on the test data
when we compute the roc curves on the test data the model with appears to provide the most accurate results
support vector machines fitted attribute predict svmfit opt dat train decision values decision values rocplot fitted dat train main test data fitted attribute predict svmfit flex dat train decision values decision values rocplot fitted dat train add col red svm with multiple classes if the response is factor containing more than two levels then the svm function will perform multi class classification using the one versus one approach
we explore that setting here by generating third class of observations set seed rbind matrix rnorm ncol rep dat data frame as factor par mfrow plot col we now fit an svm to the data svmfit svm data dat kernel radial cost gamma plot svmfit dat the library can also be used to perform support vector regression if the response vector that is passed in to svm is numerical rather than factor
application to gene expression data we now examine the khan data set which consists of number of tissue samples corresponding to four distinct types of small round blue cell tumors
for each tissue sample gene expression measurements are available
the data set consists of training data xtrain and ytrain and testing data xtest and ytest
we examine the dimension of the data library islr names khan xtrain xtest ytrain ytest dim khan xtra dim khan xtes length khan ytra length khan ytes
lab support vector machines this data set consists of expression measurements for genes
the training and test sets consist of and observations respectively table khan ytra in table khan ytest we will use support vector approach to predict cancer subtype using gene expression measurements
in this data set there are very large number of features relative to the number of observations
this suggests that we should use linear kernel because the additional flexibility that will result from using polynomial or radial kernel is unnecessary dat data frame khan xtrain as factor khan ytra out svm data dat kernel linear cost summary out call svm formula data dat kernel linear cost parameters svm type svm kernel linear cost gamma number of support vectors number of classes levels table out fitted dat we see that there are no training errors
in fact this is not surprising because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes
we are most interested not in the support vector classifier's performance on the training observations but rather its performance on the test observations dat te data frame khan xtest as factor khan ytest pred te predict out newdata dat te table pred te dat te pred te
support vector machines we see that using cost yields two test set errors on this data
exercises conceptual
this problem involves hyperplanes in two dimensions sketch the hyperplane
indicate the set of points for which as well as the set of points for which on the same plot sketch the hyperplane
indicate the set of points for which as well as the set of points for which
we have seen that in dimensions linear decision boundary takes the form
we now investigate non linear decision boundary sketch the curve on your sketch indicate the set of points for which as well as the set of points for which suppose that classifier assigns an observation to the blue class if and to the red class otherwise
to what class is the observation classified
argue that while the decision boundary in is not linear in terms of and it is linear in terms of and
here we explore the maximal margin classifier on toy data set we are given observations in dimensions
for each observation there is an associated class label
exercises obs
red red red red blue blue blue sketch the observations sketch the optimal separating hyperplane and provide the equation for this hyperplane of the form describe the classification rule for the maximal margin classifier
it should be something along the lines of classify to red if and classify to blue otherwise
provide the values for and on your sketch indicate the margin for the maximal margin hyperplane
how wide is the margin
indicate the support vectors for the maximal margin classifier argue that slight movement of the seventh observation would not affect the maximal margin hyperplane sketch hyperplane that is not the optimal separating hyperplane and provide the equation for this hyperplane draw an additional observation on the plot so that the two classes are no longer separable by hyperplane
generate simulated two class data set with observations and two features in which there is visible but non linear separation between the two classes
show that in this setting support vector machine with polynomial kernel with degree greater than or radial kernel will outperform support vector classifier on the training data
which technique performs best on the test data
make plots and report training and test error rates in order to back up your assertions
we have seen that we can fit an svm with non linear kernel in order to perform classification using non linear decision boundary
we will now see that we can also obtain non linear decision boundary by performing logistic regression using non linear transformations of the features
support vector machines generate data set with and such that the observations belong to two classes with quadratic decision boundary between them
for instance you can do this as follows runif runif plot the observations colored according to their class labels
your plot should display on the axis and on the yaxis fit logistic regression model to the data using and as predictors apply this model to the training data in order to obtain predicted class label for each training observation
plot the observations colored according to the predicted class labels
the decision boundary should be linear now fit logistic regression model to the data using non linear functions of and as predictors
log and so forth apply this model to the training data in order to obtain predicted class label for each training observation
plot the observations colored according to the predicted class labels
the decision boundary should be obviously non linear
if it is not then repeat until you come up with an example in which the predicted class labels are obviously non linear fit support vector classifier to the data with and as predictors
obtain class prediction for each training observation
plot the observations colored according to the predicted class labels fit svm using non linear kernel to the data
obtain class prediction for each training observation
plot the observations colored according to the predicted class labels comment on your results
at the end of section it is claimed that in the case of data that is just barely linearly separable support vector classifier with small value of cost that misclassifies couple of training observations may perform better on test data than one with huge value of cost that does not misclassify any training observations
you will now investigate this claim generate two class data with in such way that the classes are just barely linearly separable
exercises compute the cross validation error rates for support vector classifiers with range of cost values
how many training errors are misclassified for each value of cost considered and how does this relate to the cross validation errors obtained
generate an appropriate test data set and compute the test errors corresponding to each of the values of cost considered
which value of cost leads to the fewest test errors and how does this compare to the value of cost that yield the fewest training errors and the fewest cross validation errors
discuss your results
in this problem you will use support vector approaches in order to predict whether given car gets high or low gas mileage based on the auto data set create binary variable that takes on for cars with gas mileage above the median and for cars with gas mileage below the median fit support vector classifier to the data with various values of cost in order to predict whether car gets high or low gas mileage
report the cross validation errors associated with different values of this parameter
comment on your results now repeat this time using svms with radial and polynomial basis kernels with different values of gamma and degree and cost
comment on your results make some plots to back up your assertions in and
hint in the lab we used the plot function for svm objects only in cases with
when you can use the plot function to create plots displaying pairs of variables at time
essentially instead of typing plot svmfit dat where svmfit contains your fitted model and dat is data frame containing your data you can type plot svmfit dat in order to plot just the first and fourth variables
however you must replace and with the correct variable names
to find out more type
plot svm
this problem involves the oj data set which is part of the islr package
support vector machines create training set containing random sample of observations and test set containing the remaining observations fit support vector classifier to the training data using cost with purchase as the response and the other variables as predictors
use the summary function to produce summary statistics about the svm and describe the results obtained what are the training and test error rates
use the tune function to select an optimal cost
consider values in the range to compute the training and test error rates using this new value for cost repeat parts through using support vector machine with radial kernel
use the default value for gamma repeat parts through using support vector machine with polynomial kernel
set degree for part overall which approach seems to give the best results on this data
this is page printer opaque this unsupervised learning most of this book concerns supervised learning methods such as regression and classification
in the supervised learning setting we typically have access to set of features xp measured on observations and response also measured on those same observations
the goal is then to predict using xp
this chapter will instead focus on unsupervised learning set of statistical tools intended for the setting in which we have only set of features xp measured on observations
we are not interested in prediction because we do not have an associated response variable
rather the goal is to discover interesting things about the measurements on xp
is there an informative way to visualize the data
can we discover subgroups among the variables or among the observations
unsupervised learning refers to diverse set of techniques for answering questions such as these
in this chapter we will focus on two particular types of unsupervised learning principal components analysis tool used for data visualization or data pre processing before supervised techniques are applied and clustering broad class of methods for discovering unknown subgroups in data
the challenge of unsupervised learning supervised learning is well understood area
in fact if you have read the preceding chapters in this book then you should by now have good
unsupervised learning grasp of supervised learning
for instance if you are asked to predict binary outcome from data set you have very well developed set of tools at your disposal such as logistic regression linear discriminant analysis classification trees support vector machines and more as well as clear understanding of how to assess the quality of the results obtained using cross validation validation on an independent test set and so forth
in contrast unsupervised learning is often much more challenging
the exercise tends to be more subjective and there is no simple goal for the analysis such as prediction of response
unsupervised learning is often performed as part of an exploratory data analysis
furthermore it can exploratory data be hard to assess the results obtained from unsupervised learning meth analysis ods since there is no universally accepted mechanism for performing crossvalidation or computing validating results on an independent data set
the reason for this difference is simple
if we fit predictive model using supervised learning technique then it is possible to check our work by seeing how well our model predicts the response on observations not used in fitting the model
however in unsupervised learning there is no way to check our work because we don't know the true answer the problem is unsupervised
techniques for unsupervised learning are of growing importance in number of fields
cancer researcher might assay gene expression levels in hundred patients with breast cancer
he or she might then look for subgroups among the breast cancer samples or among the genes in order to obtain better understanding of the disease
an online shopping site might try to identify groups of shoppers with similar browsing and purchase histories as well as items that are of particular interest to the shoppers within each group
then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested based on the purchase histories of similar shoppers
search engine might choose what search results to display to particular individual based on the click histories of other individuals with similar search patterns
these statistical learning tasks and many more can be performed via unsupervised learning techniques
principal components analysis principal components are discussed in section in the context of principal components regression
when faced with large set of correlated variables principal components allow us to summarize this set with smaller number of representative variables that collectively explain most of the variability in the original set
the principal component directions are presented in section as directions in feature space along which the original data are highly variable
these directions also define lines and subspaces that are
principal components analysis as close as possible to the data cloud
to perform principal components regression we simply use principal components as predictors in regression model in place of the original larger set of variables
principal component analysis pca refers to the process by which prinprincipal cipal components are computed and the subsequent use of these compo component analysis nents in understanding the data
pca is an unsupervised approach since it involves only set of features xp and no associated response
apart from producing derived variables for use in supervised learning problems pca also serves as tool for data visualization visualization of the observations or visualization of the variables
we now discuss pca in greater detail focusing on the use of pca as tool for unsupervised data exploration in keeping with the topic of this chapter
what are principal components
suppose that we wish to visualize observations with measurements on set of features xp as part of an exploratory data analysis
we could do this by examining two dimensional scatterplots of the data each of which contains the observations measurements on two of the features
however there are such scatterplots for example with there are plots
if is large then it will certainly not be possible to look at all of them moreover most likely none of them will be informative since they each contain just small fraction of the total information present in the data set
clearly better method is required to visualize the observations when is large
in particular we would like to find low dimensional representation of the data that captures as much of the information as possible
for instance if we can obtain two dimensional representation of the data that captures most of the information then we can plot the observations in this low dimensional space
pca provides tool to do just this
it finds low dimensional representation of data set that contains as much as possible of the variation
the idea is that each of the observations lives in dimensional space but not all of these dimensions are equally interesting
pca seeks small number of dimensions that are as interesting as possible where the concept of interesting is measured by the amount that the observations vary along each dimension
each of the dimensions found by pca is linear combination of the features
we now explain the manner in which these dimensions or principal components are found
the first principal component of set of features xp is the normalized linear combination of the features
xp pp that has the largest variance
by normalized we mean that
we refer to the elements as the loadings of the first principal loading
unsupervised learning component together the loadings make up the principal component loading vector

we constrain the loadings so that their sum of squares is equal to one since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance
given data set how do we compute the first principal component
since we are only interested in variance we assume that each of the variables in has been centered to have mean zero that is the column means of are zero
we then look for the linear combination of the sample feature values of the form zi xi xi
xip pp that has largest sample variance subject to the constraint that
in other words the first principal component loading vector solves the optimization problem eb fc xn fd maximize ed xij subject to
fe pn from we can write the objective in as zi
since xij the average of the will be zero as well
hence the objective that we are maximizing in is just the sample variance of the values of zi
we refer to zn as the scores of the first prin score cipal component
problem can be solved via an eigen decomposition standard technique in linear algebra but details are outside of the scope of this book
there is nice geometric interpretation for the first principal component
the loading vector with elements defines direction in feature space along which the data vary the most
if we project the data points xn onto this direction the projected values are the principal component scores zn themselves
for instance figure on page displays the first principal component loading vector green solid line on an advertising data set
in these data there are only two features and so the observations as well as the first principal component loading vector can be easily displayed
as can be seen from in that data set and
after the first principal component of the features has been determined we can find the second principal component
the second principal component is the linear combination of xp that has maximal variance out of all linear combinations that are uncorrelated with
the second principal component scores zn take the form zi xi xi
xip
principal components analysis pc pc murder assault urbanpop rape table
the principal component loading vectors and for the usarrests data
these are also displayed in figure where is the second principal component loading vector with elements
it turns out that constraining to be uncorrelated with is equivalent to constraining the direction to be orthogonal perpendicular to the direction
in the example in figure the observations lie in two dimensional space since and so once we have found there is only one possibility for which is shown as blue dashed line
from section we know that and
but in larger data set with variables there are multiple distinct principal components and they are defined in similar manner
to find we solve problem similar to with replacing and with the additional constraint that is orthogonal to once we have computed the principal components we can plot them against each other in order to produce low dimensional views of the data
for instance we can plot the score vector against against against and so forth
geometrically this amounts to projecting the original data down onto the subspace spanned by and and plotting the projected points
we illustrate the use of pca on the usarrests data set
for each of the fifty states in the united states the data set contains the number of arrests per residents for each of three crimes assault murder and rape
we also record urbanpop the percent of the population in each state living in urban areas
the principal component score vectors have length and the principal component loading vectors have length
pca was performed after standardizing each variable to have mean zero and standard deviation one
figure plots the first two principal components of these data
the figure represents both the principal component scores and the loading vectors in single biplot display
the loadings are also given in biplot table
in figure we see that the first loading vector places approximately equal weight on assault murder and rape with much less weight on urbanpop
hence this component roughly corresponds to measure of overall rates of serious crimes
the second loading vector places most of its weight on on technical note the principal component directions are the or dered sequence of eigenvectors of the matrix xt and the variances of the components are the eigenvalues
there are at most min principal components
the first two principal components for the usarrests data
the blue state names represent the scores for the first two principal components
the orange arrows indicate the first two principal component loading vectors with axes on the top and right
for example the loading for rape on the first component is and its loading on the second principal component the word rape is centered at the point
this figure is known as biplot because it displays both the principal component scores and the principal component loadings
principal components analysis urbanpop and much less weight on the other three features
hence this component roughly corresponds to the level of urbanization of the state
overall we see that the crime related variables murder assault and rape are located close to each other and that the urbanpop variable is far from the other three
this indicates that the crime related variables are correlated with each other states with high murder rates tend to have high assault and rape rates and that the urbanpop variable is less correlated with the other three
we can examine differences between the states via the two principal component score vectors shown in figure
our discussion of the loading vectors suggests that states with large positive scores on the first component such as california nevada and florida have high crime rates while states like north dakota with negative scores on the first component have low crime rates
california also has high score on the second component indicating high level of urbanization while the opposite is true for states like mississippi
states close to zero on both components such as indiana have approximately average levels of both crime and urbanization
another interpretation of principal components the first two principal component loading vectors in simulated threedimensional data set are shown in the left hand panel of figure these two loading vectors span plane along which the observations have the highest variance
in the previous section we describe the principal component loading vectors as the directions in feature space along which the data vary the most and the principal component scores as projections along these directions
however an alternative interpretation for principal components can also be useful principal components provide low dimensional linear surfaces that are closest to the observations
we expand upon that interpretation here
the first principal component loading vector has very special property it is the line in dimensional space that is closest to the observations using average squared euclidean distance as measure of closeness
this interpretation can be seen in the left hand panel of figure the dashed lines indicate the distance between each observation and the first principal component loading vector
the appeal of this interpretation is clear we seek single dimension of the data that lies as close as possible to all of the data points since such line will likely provide good summary of the data
the notion of principal components as the dimensions that are closest to the observations extends beyond just the first principal component
for instance the first two principal components of data set span the plane that is closest to the observations in terms of average squared euclidean distance
an example is shown in the left hand panel of figure
left the first two principal component directions span the plane that best fits the data
it minimizes the sum of squared distances from each point to the plane
right the first two principal component score vectors give the coordinates of the projection of the observations onto the plane
the variance in the plane is maximized first three principal components of data set span the three dimensional hyperplane that is closest to the observations and so forth
using this interpretation together the first principal component score vectors and the first principal component loading vectors provide the best dimensional approximation in terms of euclidean distance to the ith observation xij
this representation can be written xij zim jm assuming the original data matrix is column centered
in other words together the principal component score vectors and principal component loading vectors can give good approximation to the data when is sufficiently large
when min then the representation pm is exact xij zim jm
more on pca scaling the variables we have already mentioned that before pca is performed the variables should be centered to have mean zero
furthermore the results obtained when we perform pca will also depend on whether the variables have been individually scaled each multiplied by different constant
this is in con
two principal component biplots for the usarrests data
left the same as figure with the variables scaled to have unit standard deviations
right principal components using unscaled data
assault has by far the largest loading on the first principal component because it has the highest variance among the four variables
in general scaling the variables to have standard deviation one is recommended trast to some other supervised and unsupervised learning techniques such as linear regression in which scaling the variables has no effect
in linear regression multiplying variable by factor of will simply lead to multiplication of the corresponding coefficient estimate by factor of and thus will have no substantive effect on the model obtained
for instance figure was obtained after scaling each of the variables to have standard deviation one
this is reproduced in the left hand plot in figure
why does it matter that we scaled the variables
in these data the variables are measured in different units murder rape and assault are reported as the number of occurrences per people and urbanpop is the percentage of the state's population that lives in an urban area
these four variables have variance and respectively
consequently if we perform pca on the unscaled variables then the first principal component loading vector will have very large loading for assault since that variable has by far the highest variance
the righthand plot in figure displays the first two principal components for the usarrests data set without scaling the variables to have standard deviation one
as predicted the first principal component loading vector places almost all of its weight on assault while the second principal component loading vector places almost all of its weight on urpanpop
comparing this
unsupervised learning to the left hand plot we see that scaling does indeed have substantial effect on the results obtained
however this result is simply consequence of the scales on which the variables were measured
for instance if assault were measured in units of the number of occurrences per people rather than number of occurrences per people then this would amount to dividing all of the elements of that variable by
then the variance of the variable would be tiny and so the first principal component loading vector would have very small value for that variable
because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling we typically scale each variable to have standard deviation one before we perform pca
in certain settings however the variables may be measured in the same units
in this case we might not wish to scale the variables to have standard deviation one before performing pca
for instance suppose that the variables in given data set correspond to expression levels for genes
then since expression is measured in the same units for each gene we might choose not to scale the genes to each have standard deviation one
uniqueness of the principal components each principal component loading vector is unique up to sign flip
this means that two different software packages will yield the same principal component loading vectors although the signs of those loading vectors may differ
the signs may differ because each principal component loading vector specifies direction in dimensional space flipping the sign has no effect as the direction does not change
consider figure the principal component loading vector is line that extends in either direction and flipping its sign would have no effect
similarly the score vectors are unique up to sign flip since the variance of is the same as the variance of
it is worth noting that when we use to approximate xij we multiply zim by jm
hence if the sign is flipped on both the loading and score vectors the final product of the two quantities is unchanged
the proportion of variance explained in figure we performed pca on three dimensional data set lefthand panel and projected the data onto the first two principal component loading vectors in order to obtain two dimensional view of the data the principal component score vectors right hand panel
we see that this two dimensional representation of the three dimensional data does successfully capture the major pattern in the data the orange green and cyan observations that are near each other in three dimensional space remain nearby in the two dimensional representation
similarly we have seen on the usarrests data set that we can summarize the observations and four
principal components analysis variables using just the first two principal component score vectors and the first two principal component loading vectors
we can now ask natural question how much of the information in given data set is lost by projecting the observations onto the first few principal components
that is how much of the variance in the data is not contained in the first few principal components
more generally we are interested in knowing the proportion of variance explained pve by each proportion of principal component
the total variance present in data set assuming variance explained that the variables have been centered to have mean zero is defined as var xj ij and the variance explained by the mth principal component is eb ed zim jm xij
therefore the pve of the mth principal component is given by pn jm xij pp pn
xij the pve of each principal component is positive quantity
in order to compute the cumulative pve of the first principal components we can simply sum over each of the first pves
in total there are min principal components and their pves sum to one
in the usarrests data the first principal component explains of the variance in the data and the next principal component explains of the variance
together the first two principal components explain almost of the variance in the data and the last two principal components explain only of the variance
this means that figure provides pretty accurate summary of the data using just two dimensions
the pve of each principal component as well as the cumulative pve is shown in figure
the left hand panel is known as scree plot and will be scree plot discussed next
deciding how many principal components to use in general data matrix has min distinct principal components
however we usually are not interested in all of them rather we would like to use just the first few principal components in order to visualize or interpret the data
in fact we would like to use the smallest number of principal components required to get good understanding of the
unsupervised learning cumulative prop
variance explained prop
left scree plot depicting the proportion of variance explained by each of the four principal components in the usarrests data
right the cumulative proportion of variance explained by the four principal components in the usarrests data data
how many principal components are needed
unfortunately there is no single or simple
answer to this question
we typically decide on the number of principal components required to visualize the data by examining scree plot such as the one shown in the left hand panel of figure
we choose the smallest number of principal components that are required in order to explain sizable amount of the variation in the data
this is done by eyeballing the scree plot and looking for point at which the proportion of variance explained by each subsequent principal component drops off
this is often referred to as an elbow in the scree plot
for instance by inspection of figure one might conclude that fair amount of variance is explained by the first two principal components and that there is an elbow after the second component
after all the third principal component explains less than ten percent of the variance in the data and the fourth principal component explains less than half that and so is essentially worthless
however this type of visual analysis is inherently ad hoc
unfortunately there is no well accepted objective way to decide how many principal components are enough
in fact the question of how many principal components are enough is inherently ill defined and will depend on the specific area of application and the specific data set
in practice we tend to look at the first few principal components in order to find interesting patterns in the data
if no interesting patterns are found in the first few principal components then further principal components are unlikely to be of interest
conversely if the first few principal components are interesting then we typically continue to look at subsequent principal components until no
clustering methods further interesting patterns are found
this is admittedly subjective approach and is reflective of the fact that pca is generally used as tool for exploratory data analysis
on the other hand if we compute principal components for use in supervised analysis such as the principal components regression presented in section then there is simple and objective way to determine how many principal components to use we can treat the number of principal component score vectors to be used in the regression as tuning parameter to be selected via cross validation or related approach
the comparative simplicity of selecting the number of principal components for supervised analysis is one manifestation of the fact that supervised analyses tend to be more clearly defined and more objectively evaluated than unsupervised analyses
other uses for principal components we saw in section that we can perform regression using the principal component score vectors as features
in fact many statistical techniques such as regression classification and clustering can be easily adapted to use the matrix whose columns are the first principal component score vectors rather than using the full data matrix
this can lead to less noisy results since it is often the case that the signal as opposed to the noise in data set is concentrated in its first few principal components
clustering methods clustering refers to very broad set of techniques for finding subgroups or clustering clusters in data set
when we cluster the observations of data set we seek to partition them into distinct groups so that the observations within each group are quite similar to each other while observations in different groups are quite different from each other
of course to make this concrete we must define what it means for two or more observations to be similar or different
indeed this is often domain specific consideration that must be made based on knowledge of the data being studied
for instance suppose that we have set of observations each with features
the observations could correspond to tissue samples for patients with breast cancer and the features could correspond to measurements collected for each tissue sample these could be clinical measurements such as tumor stage or grade or they could be gene expression measurements
we may have reason to believe that there is some heterogeneity among the tissue samples for instance perhaps there are few different unknown subtypes of breast cancer
clustering could be used to find these subgroups
unsupervised learning this is an unsupervised problem because we are trying to discover structure in this case distinct clusters on the basis of data set
the goal in supervised problems on the other hand is to try to predict some outcome vector such as survival time or response to drug treatment
another application of clustering arises in marketing
we may have access to large number of measurements median household income occupation distance from nearest urban area and so forth for large number of people
our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to particular form of advertising or more likely to purchase particular product
the task of performing market segmentation amounts to clustering the people in the data set
since clustering is popular in many fields there exist great number of clustering methods
in this section we focus on perhaps the two best known clustering approaches means clustering and hierarchical clustering
in means clustering means clustering we seek to partition the observations into pre specified hierarchical number of clusters
on the other hand in hierarchical clustering we do clustering not know in advance how many clusters we want in fact we end up with tree like visual representation of the observations called dendrogram dendrogram that allows us to view at once the clusterings obtained for each possible number of clusters from to
there are advantages and disadvantages to each of these clustering approaches which we highlight in this chapter
in general we can cluster observations on the basis of the features in order to identify subgroups among the observations or we can cluster features on the basis of the observations in order to discover subgroups among the features
in what follows for simplicity we will discuss clustering observations on the basis of the features though the converse can be performed by simply transposing the data matrix
means clustering means clustering is simple and elegant approach for partitioning data set into distinct non overlapping clusters
to perform means clustering we must first specify the desired number of clusters then the means algorithm will assign each observation to exactly one of the clusters
figure shows the results obtained from performing means
simulated data set with observations in dimensional space
panels show the results of applying means clustering with different values of the number of clusters
the color of each observation indicates the cluster to which it was assigned using the means clustering algorithm
note that there is no ordering of the clusters so the cluster coloring is arbitrary
these cluster labels were not used in clustering instead they are the outputs of the clustering procedure clustering on simulated example consisting of observations in two dimensions using three different values of
the means clustering procedure results from simple and intuitive mathematical problem
we begin by defining some notation
let ck denote sets containing the indices of the observations in each cluster
these sets satisfy two properties

ck
in other words each observation belongs to at least one of the clusters
ck ck for all
in other words the clusters are nonoverlapping no observation belongs to more than one cluster
for instance if the ith observation is in the kth cluster then ck
the idea behind means clustering is that good clustering is one for which the within cluster variation is as small as possible
the within cluster variation for cluster ck is measure ck of the amount by which the observations within cluster differ from each other
hence we want to solve the problem minimize ck
ck in words this formula says that we want to partition the observations into clusters such that the total within cluster variation summed over all clusters is as small as possible
unsupervised learning solving seems like reasonable idea but in order to make it actionable we need to define the within cluster variation
there are many possible ways to define this concept but by far the most common choice involves squared euclidean distance
that is we define ck xij xi ck ck where ck denotes the number of observations in the kth cluster
in other words the within cluster variation for the kth cluster is the sum of all of the pairwise squared euclidean distances between the observations in the kth cluster divided by the total number of observations in the kth cluster
combining and gives the optimization problem that defines means clustering fc xk fd minimize xij xi
ck ck fe ck now we would like to find an algorithm to solve that is method to partition the observations into clusters such that the objective of is minimized
this is in fact very difficult problem to solve precisely since there are almost ways to partition observations into clusters
this is huge number unless and are tiny
fortunately very simple algorithm can be shown to provide local optimum pretty good solution to the means optimization problem
this approach is laid out in algorithm
algorithm means clustering
randomly assign number from to to each of the observations
these serve as initial cluster assignments for the observations
iterate until the cluster assignments stop changing for each of the clusters compute the cluster centroid
the kth cluster centroid is the vector of the feature means for the observations in the kth cluster assign each observation to the cluster whose centroid is closest where closest is defined using euclidean distance
algorithm is guaranteed to decrease the value of the objective at each step
to understand why the following identity is illuminating xp xxp xij xi xij kj ck ck ck
clustering methods where kj ck xij is the mean for feature in cluster ck
in step the cluster means for each feature are the constants that minimize the sum of squared deviations and in step reallocating the observations can only improve
this means that as the algorithm is run the clustering obtained will continually improve until the result no longer changes the objective of will never increase
when the result no longer changes local optimum has been reached
figure shows the progression of the algorithm on the toy example from figure
means clustering derives its name from the fact that in step the cluster centroids are computed as the mean of the observations assigned to each cluster
because the means algorithm finds local rather than global optimum the results obtained will depend on the initial random cluster assignment of each observation in step of algorithm
for this reason it is important to run the algorithm multiple times from different random initial configurations
then one selects the best solution that for which the objective is smallest
figure shows the local optima obtained by running means clustering six times using six different initial cluster assignments using the toy data from figure
in this case the best clustering is the one with an objective value of
as we have seen to perform means clustering we must decide how many clusters we expect in the data
the problem of selecting is far from simple
this issue along with other practical considerations that arise in performing means clustering is addressed in section
hierarchical clustering one potential disadvantage of means clustering is that it requires us to pre specify the number of clusters
hierarchical clustering is an alternative approach which does not require that we commit to particular choice of
hierarchical clustering has an added advantage over means clustering in that it results in an attractive tree based representation of the observations called dendrogram
in this section we describe bottom up or agglomerative clustering
this bottom up is the most common type of hierarchical clustering and refers to the fact agglomerative that dendrogram generally depicted as an upside down tree see figure is built starting from the leaves and combining clusters up to the trunk
we will begin with discussion of how to interpret dendrogram and then discuss how hierarchical clustering is actually performed that is how the dendrogram is built
interpreting dendrogram we begin with the simulated data set shown in figure consisting of observations in two dimensional space
the data were generated from
the progress of the means algorithm on the example of figure with
top left the observations are shown
top center in step of the algorithm each observation is randomly assigned to cluster
top right in step the cluster centroids are computed
these are shown as large colored disks
initially the centroids are almost completely overlapping because the initial cluster assignments were chosen at random
bottom left in step each observation is assigned to the nearest centroid
bottom center step is once again performed leading to new cluster centroids
bottom right the results obtained after iterations
means clustering performed six times on the data from figure with each time with different random assignment of the observations in step of the means algorithm
above each plot is the value of the objective
three different local optima were obtained one of which resulted in smaller value of the objective and provides better separation between the clusters
those labeled in red all achieved the same best solution with an objective value of
in reality there are three distinct classes shown in separate colors
however we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data three class model the true class labels for each observation are shown in distinct colors
however suppose that the data were observed without the class labels and that we wanted to perform hierarchical clustering of the data
hierarchical clustering with complete linkage to be discussed later yields the result shown in the left hand panel of figure
how can we interpret this dendrogram
in the left hand panel of figure each leaf of the dendrogram represents one of the observations in figure
however as we move up the tree some leaves begin to fuse into branches
these correspond to observations that are similar to each other
as we move higher up the tree branches themselves fuse either with leaves or other branches
the earlier lower in the tree fusions occur the more similar the groups of observations are to each other
on the other hand observations that fuse later near the top of the tree can be quite different
in fact this statement can be made precise for any two observations we can look for the point in the tree where branches containing those two observations are first fused
the height of this fusion as measured on the vertical axis indicates how different the two observations are
thus observations that fuse at the very bottom of the tree are quite similar to each other whereas observations that fuse close to the top of the tree will tend to be quite different
this highlights very important point in interpreting dendrograms that is often misunderstood
consider the left hand panel of figure which shows simple dendrogram obtained from hierarchically clustering nine ob
left dendrogram obtained from hierarchically clustering the data from figure with complete linkage and euclidean distance
center the dendrogram from the left hand panel cut at height of indicated by the dashed line
this cut results in two distinct clusters shown in different colors
right the dendrogram from the left hand panel now cut at height of
this cut results in three distinct clusters shown in different colors
note that the colors were not used in clustering but are simply used for display purposes in this figure servations
one can see that observations and are quite similar to each other since they fuse at the lowest point on the dendrogram
observations and are also quite similar to each other
however it is tempting but incorrect to conclude from the figure that observations and are quite similar to each other on the basis that they are located near each other on the dendrogram
in fact based on the information contained in the dendrogram observation is no more similar to observation than it is to observations and
this can be seen from the right hand panel of figure in which the raw data are displayed
to put it mathematically there are possible reorderings of the dendrogram where is the number of leaves
this is because at each of the points where fusions occur the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram
therefore we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis
rather we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused
now that we understand how to interpret the left hand panel of figure we can move on to the issue of identifying clusters on the basis of dendrogram
in order to do this we make horizontal cut across the den
an illustration of how to properly interpret dendrogram with nine observations in two dimensional space
left dendrogram generated using euclidean distance and complete linkage
observations and are quite similar to each other as are observations and
however observation is no more similar to observation than it is to observations and even though observations and are close together in terms of horizontal distance
this is because observations and all fuse with observation at the same height approximately
right the raw data used to generate the dendrogram can be used to confirm that indeed observation is no more similar to observation than it is to observations and drogram as shown in the center and right hand panels of figure
the distinct sets of observations beneath the cut can be interpreted as clusters
in the center panel of figure cutting the dendrogram at height of results in two clusters shown in distinct colors
in the right hand panel cutting the dendrogram at height of results in three clusters
further cuts can be made as one descends the dendrogram in order to obtain any number of clusters between corresponding to no cut and corresponding to cut at height so that each observation is in its own cluster
in other words the height of the cut to the dendrogram serves the same role as the in means clustering it controls the number of clusters obtained
figure therefore highlights very attractive aspect of hierarchical clustering one single dendrogram can be used to obtain any number of clusters
in practice people often look at the dendrogram and select by eye sensible number of clusters based on the heights of the fusion and the number of clusters desired
in the case of figure one might choose to select either two or three clusters
however often the choice of where to cut the dendrogram is not so clear
the term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height
however on
clustering methods an arbitrary data set this assumption of hierarchical structure might be unrealistic
for instance suppose that our observations correspond to group of people with split of males and females evenly split among americans japanese and french
we can imagine scenario in which the best division into two groups might split these people by gender and the best division into three groups might split them by nationality
in this case the true clusters are not nested in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups
consequently this situation could not be well represented by hierarchical clustering
due to situations such as this one hierarchical clustering can sometimes yield worse less accurate results than means clustering for given number of clusters
the hierarchical clustering algorithm the hierarchical clustering dendrogram is obtained via an extremely simple algorithm
we begin by defining some sort of dissimilarity measure between each pair of observations
most often euclidean distance is used we will discuss the choice of dissimilarity measure later in this chapter
the algorithm proceeds iteratively
starting out at the bottom of the dendrogram each of the observations is treated as its own cluster
the two clusters that are most similar to each other are then fused so that there now are clusters
next the two clusters that are most similar to each other are fused again so that there now are clusters
the algorithm proceeds in this fashion until all of the observations belong to one single cluster and the dendrogram is complete
figure depicts the first few steps of the algorithm for the data from figure
to summarize the hierarchical clustering algorithm is given in algorithm
algorithm hierarchical clustering
begin with observations and measure such as euclidean distance of all the pairwise dissimilarities
treat each observation as its own cluster
for examine all pairwise inter cluster dissimilarities among the clusters and identify the pair of clusters that are least dissimilar that is most similar
fuse these two clusters
the dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed compute the new pairwise inter cluster dissimilarities among the remaining clusters
an illustration of the first few steps of the hierarchical clustering algorithm using the data from figure with complete linkage and euclidean distance
top left initially there are nine distinct clusters
top right the two clusters that are closest together and are fused into single cluster
bottom left the two clusters that are closest together and are fused into single cluster
bottom right the two clusters that are closest together using complete linkage and the cluster are fused into single cluster
average complete and single linkage applied to an example data set
average and complete linkage tend to yield more balanced clusters
this algorithm seems simple enough but one issue has not been addressed
consider the bottom right panel in figure
how did we determine that the cluster should be fused with the cluster
we have concept of the dissimilarity between pairs of observations but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations
the concept of dissimilarity between pair of observations needs to be extended to pair of groups of observations
this extension is achieved by developing the notion of linkage which linkage defines the dissimilarity between two groups of observations
the four most common types of linkage complete average single and centroid are briefly described in table
average complete and single linkage are most popular among statisticians
average and complete linkage are generally preferred over single linkage as they tend to yield more balanced dendrograms
centroid linkage is often used in genomics but suffers from major drawback in that an inversion can occur whereby two clusters are inversion fused at height below either of the individual clusters in the dendrogram
this can lead to difficulties in visualization as well as in interpretation of the dendrogram
the dissimilarities computed in step of the hierarchical clustering algorithm will depend on the type of linkage used as well as on the choice of dissimilarity measure
hence the resulting dendrogram typically depends quite strongly on the type of linkage used as is shown in figure
unsupervised learning table
summary of the four most commonly used types of linkage in hierarchical clustering
linkage description maximal intercluster dissimilarity
compute all pairwise dissimilarities between the observations in cluster and the complete observations in cluster and record the largest of these dissimilarities
minimal intercluster dissimilarity
compute all pairwise dissimilarities between the observations in cluster and the single observations in cluster and record the smallest of these dissimilarities
single linkage can result in extended trailing clusters in which single observations are fused one at time
mean intercluster dissimilarity
compute all pairwise dissimilarities between the observations in cluster and the average observations in cluster and record the average of these dissimilarities
dissimilarity between the centroid for cluster mean centroid vector of length and the centroid for cluster
centroid linkage can result in undesirable inversions
choice of dissimilarity measure thus far the examples in this chapter have used euclidean distance as the dissimilarity measure
but sometimes other dissimilarity measures might be preferred
for example correlation based distance considers two observations to be similar if their features are highly correlated even though the observed values may be far apart in terms of euclidean distance
this is an unusual use of correlation which is normally computed between variables here it is computed between the observation profiles for each pair of observations
figure illustrates the difference between euclidean and correlation based distance
correlation based distance focuses on the shapes of observation profiles rather than their magnitudes
the choice of dissimilarity measure is very important as it has strong effect on the resulting dendrogram
in general careful attention should be paid to the type of data being clustered and the scientific question at hand
these considerations should determine what type of dissimilarity measure is used for hierarchical clustering
for instance consider an online retailer interested in clustering shoppers based on their past shopping histories
the goal is to identify subgroups of similar shoppers so that shoppers within each subgroup can be shown items and advertisements that are particularly likely to interest them
suppose the data takes the form of matrix where the rows are the shoppers and the columns are the items available for purchase the elements of the data matrix indicate the number of times given shopper has purchased
three observations with measurements on variables are shown
observations and have similar values for each variable and so there is small euclidean distance between them
but they are very weakly correlated so they have large correlation based distance
on the other hand observations and have quite different values for each variable and so there is large euclidean distance between them
but they are highly correlated so there is small correlation based distance between them given item if the shopper has never purchased this item if the shopper has purchased it once etc
what type of dissimilarity measure should be used to cluster the shoppers
if euclidean distance is used then shoppers who have bought very few items overall infrequent users of the online shopping site will be clustered together
this may not be desirable
on the other hand if correlation based distance is used then shoppers with similar preferences shoppers who have bought items and but never items or will be clustered together even if some shoppers with these preferences are higher volume shoppers than others
therefore for this application correlation based distance may be better choice
in addition to carefully selecting the dissimilarity measure used one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed
to illustrate this point we continue with the online shopping example just described
some items may be purchased more frequently than others for instance shopper might buy ten pairs of socks year but computer very rarely
high frequency purchases like socks therefore tend to have much larger effect on the inter shopper dissimilarities and hence on the clustering ultimately obtained than rare purchases like computers
this may not be desirable
if the variables are scaled to have standard de
unsupervised learning viation one before the inter observation dissimilarities are computed then each variable will in effect be given equal importance in the hierarchical clustering performed
we might also want to scale the variables to have standard deviation one if they are measured on different scales otherwise the choice of units centimeters versus kilometers for particular variable will greatly affect the dissimilarity measure obtained
it should come as no surprise that whether or not it is good decision to scale the variables before computing the dissimilarity measure depends on the application at hand
an example is shown in figure
we note that the issue of whether or not to scale the variables before performing clustering applies to means clustering as well
practical issues in clustering clustering can be very useful tool for data analysis in the unsupervised setting
however there are number of issues that arise in performing clustering
we describe some of these issues here
small decisions with big consequences in order to perform clustering some decisions must be made
for instance maybe the variables should be centered to have mean zero and scaled to have standard deviation one
each of these decisions can have strong impact on the results obtained
in practice we try several different choices and look for the one with the most useful or interpretable solution
with these methods there is no single right answer any solution that exposes some interesting aspects of the data should be considered
validating the clusters obtained any time clustering is performed on data set we will find clusters
but we really want to know whether the clusters that have been found represent true subgroups in the data or whether they are simply result of clustering
an eclectic online retailer sells two items socks and computers
left the number of pairs of socks and computers purchased by eight online shoppers is displayed
each shopper is shown in different color
if inter observation dissimilarities are computed using euclidean distance on the raw variables then the number of socks purchased by an individual will drive the dissimilarities obtained and the number of computers purchased will have little effect
this might be undesirable since computers are more expensive than socks and so the online retailer may be more interested in encouraging shoppers to buy computers than socks and large difference in the number of socks purchased by two shoppers may be less informative about the shoppers overall shopping preferences than small difference in the number of computers purchased
center the same data is shown after scaling each variable by its standard deviation
now the number of computers purchased will have much greater effect on the inter observation dissimilarities obtained
right the same data are displayed but now the axis represents the number of dollars spent by each online shopper on socks and on computers
since computers are much more expensive than socks now computer purchase history will drive the inter observation dissimilarities obtained
unsupervised learning the noise
for instance if we were to obtain an independent set of observations then would those observations also display the same set of clusters
this is hard question to answer
there exist number of techniques for assigning value to cluster in order to assess whether there is more evidence for the cluster than one would expect due to chance
however there has been no consensus on single best approach
more details can be found in hastie et al
other considerations in clustering both means and hierarchical clustering will assign each observation to cluster
however sometimes this might not be appropriate
for instance suppose that most of the observations truly belong to small number of unknown subgroups and small subset of the observations are quite different from each other and from all other observations
then since kmeans and hierarchical clustering force every observation into cluster the clusters found may be heavily distorted due to the presence of outliers that do not belong to any cluster
mixture models are an attractive approach for accommodating the presence of such outliers
these amount to soft version of means clustering and are described in hastie et al
in addition clustering methods generally are not very robust to perturbations to the data
for instance suppose that we cluster observations and then cluster the observations again after removing subset of the observations at random
one would hope that the two sets of clusters obtained would be quite similar but often this is not the case
tempered approach to interpreting the results of clustering we have described some of the issues associated with clustering
however clustering can be very useful and valid statistical tool if used properly
we mentioned that small decisions in how clustering is performed such as how the data are standardized and what type of linkage is used can have large effect on the results
therefore we recommend performing clustering with different choices of these parameters and looking at the full set of results in order to see what patterns consistently emerge
since clustering can be non robust we recommend clustering subsets of the data in order to get sense of the robustness of the clusters obtained
most importantly we must be careful about how the results of clustering analysis are reported
these results should not be taken as the absolute truth about data set
rather they should constitute starting point for the development of scientific hypothesis and further study preferably on an independent data set
lab principal components analysis lab principal components analysis in this lab we perform pca on the usarrests data set which is part of the base package
the rows of the data set contain the fifty states in alphabetical order states row names usarrests states the columns of the data set contain the four variables names usarrests murder assault urbanpop rape we first briefly examine the data
we notice that the variables have vastly different means apply usarrests mean murder assault urbanpop rape note that the apply function allows us to apply function in this case the mean function to each row or column of the data set
the second input here denotes whether we wish to compute the mean of the rows or the columns
we see that there are on average three times as many rapes as murders and more than eight times as many assaults as rapes
we can also examine the variances of the four variables using the apply function apply usarrests var murder assault urbanpop rape not surprisingly the variables also have vastly different variances
the broad ranges of means and variances among the variables are not surprising the urbanpop variable measures the percentage of the population in each state living in an urban area which is not comparable number to the number of rapes in each state per individuals
if we failed to scale the variables before performing pca then most of the principal components that we observed would be driven by the assault variable since it has by far the largest mean and variance
thus it is important to standardize the variables to have mean zero and standard deviation one before performing pca
we now perform principal components analysis using the prcomp funcprcomp tion which is one of several functions in that perform pca pr out prcomp usarrests scale true by default the prcomp function centers the variables to have mean zero
by using the option scale true we scale the variables to have standard deviation one
the output from prcomp contains number of useful quantities
unsupervised learning names pr out sdev rotation center scale the center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing pca pr out center murder assault urbanpop rape pr out scale murder assault urbanpop rape the rotation matrix provides the principal component loadings each column of pr out rotation contains the corresponding principal component loading vector pr pc pc pc pc murder assault urbanpop rape we see that there are four distinct principal components
this is to be expected because there are in general min informative principal components in data set with observations and variables
using the prcomp function we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors
rather the matrix has as its columns the principal component score vectors
that is the kth column is the kth principal component score vector dim pr out we can plot the first two principal components as follows biplot pr out scale the scale argument to biplot ensures that the arrows are scaled to biplot represent the loadings other values for scale give slightly different biplots with different interpretations
notice that this figure is mirror image of figure
recall that the principal components are only unique up to sign change so we can reproduce figure by making few small changes this function names it the rotation matrix because when we matrix multiply the matrix by pr out rotation it gives us the coordinates of the data in the rotated coordinate system
these coordinates are the principal component scores
lab clustering pr pr pr out pr out biplot pr out scale the prcomp function also outputs the standard deviation of each principal component
for instance on the usarrests data set we can access these standard deviations as follows pr out sdev the variance explained by each principal component is obtained by squaring these pr var pr out sdev pr var to compute the proportion of variance explained by each principal component we simply divide the variance explained by each principal component by the total variance explained by all four principal components pve pr var sum pr var pve we see that the first principal component explains of the variance in the data the next principal component explains of the variance and so forth
we can plot the pve explained by each component as well as the cumulative pve as follows plot pve xlab principal component ylab proportion of variance explained ylim type plot cumsum pve xlab principal component ylab cumulativ proportion of variance explained ylim type the result is shown in figure
note that the function cumsum comcumsum putes the cumulative sum of the elements of numeric vector
for instance cumsum lab clustering means clustering the function kmeans performs means clustering in
we begin with kmeans simple simulated example in which there truly are two clusters in the data the first observations have mean shift relative to the next observations
unsupervised learning set seed matrix rnorm ncol we now perform means clustering with km out kmeans nstart the cluster assignments of the observations are contained in km out cluster km out clust er the means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kmeans
we can plot the data with each observation colored according to its cluster assignment plot col km out clust main means clustering results with xlab ylab pch cex here the observations can be easily plotted because they are two dimensional
if there were more than two variables then we could instead perform pca and plot the first two principal components score vectors
in this example we knew that there really were two clusters because we generated the data
however for real data in general we do not know the true number of clusters
we could instead have performed means clustering on this example with set seed km out kmeans nstart km out means clustering with clusters of sizes cluster means clustering vector within cluster sum of squares by cluster between ss total ss available components cluster centers totss withinss tot withinss betweenss size
lab clustering plot col km out clust main means clustering results with xlab ylab pch cex when means clustering splits up the two clusters
to run the kmeans function in with multiple initial cluster assignments we use the nstart argument
if value of nstart greater than one is used then means clustering will be performed using multiple random assignments in step of algorithm and the kmeans function will report only the best results
here we compare using nstart to nstart set seed km out kmeans nstart km out tot withinss km out kmeans nstart km out tot withinss note that km out tot withinss is the total within cluster sum of squares which we seek to minimize by performing means clustering equation
the individual within cluster sum of squares are contained in the vector km out withinss
we strongly recommend always running means clustering with large value of nstart such as or since otherwise an undesirable local optimum may be obtained
when performing means clustering in addition to using multiple initial cluster assignments it is also important to set random seed using the set seed function
this way the initial cluster assignments in step can be replicated and the means output will be fully reproducible
hierarchical clustering the hclust function implements hierarchical clustering in
in the folhclust lowing example we use the data from section to plot the hierarchical clustering dendrogram using complete single and average linkage clustering with euclidean distance as the dissimilarity measure
we begin by clustering observations using complete linkage
the dist function is used dist to compute the inter observation euclidean distance matrix hc complete hclust dist method complete we could just as easily perform hierarchical clustering with average or single linkage instead hc average hclust dist method average hc single hclust dist method single we can now plot the dendrograms obtained using the usual plot function
the numbers at the bottom of the plot identify each observation
unsupervised learning par mfrow plot hc complete main complete linkage xlab sub cex plot hc average main average linkage xlab sub cex plot hc single main single linkage xlab sub cex to determine the cluster labels for each observation associated with given cut of the dendrogram we can use the cutree function cutree cutree hc complete cutree hc average cutree hc single for this data complete and average linkage generally separate the observations into their correct groups
however single linkage identifies one point as belonging to its own cluster
more sensible answer is obtained when clusters are selected although there are still two singletons cutree hc single to scale the variables before performing hierarchical clustering of the observations we use the scale function scale xsc scale center false scale true plot hclust dist xsc method complete main clustering with scaled note that one could instead do xsc scale plot hclust dist xsc method complete main clustering with scaled since centering the variables will not affect the euclidean distance
the default arguments for the scale function are center true scale true
correlation based distance can be computed using the as dist funcas dist tion which converts an arbitrary square symmetric matrix into form that the hclust function recognizes as distance matrix
however this only makes sense for data with at least three features since the correlation between any two points is always
hence we will cluster three dimensional data set matrix rnorm ncol dd as dist cor plot hclust dd method complete main complete linkage with correlation based distance xlab sub
lab nci data example lab nci data example unsupervised techniques are often used in the analysis of genomic data
in particular pca and hierarchical clustering are popular tools
we illustrate these techniques on the nci cancer cell line microarray data which consists of gene expression measurements on cancer cell lines library islr nci labs nci labs nci data nci data each cell line is labeled with cancer type
we do not make use of the cancer types in performing pca and clustering as these are unsupervised techniques
but after performing pca and clustering we will check to see the extent to which these cancer types agree with the results of these unsupervised techniques
the data has rows and columns dim nci data we begin by examining the cancer types for the cell lines nci labs cns cns cns renal table nci labs nci labs breast cns colon repro repro leukemia mcf repro mcf repro melanoma nsclc ovarian prostate renal unknown pca on the nci data we first perform pca on the data after scaling the variables genes to have standard deviation one although one could reasonably argue that it is better not to scale the genes pr out prcomp nci data scale true we now plot the first few principal component score vectors in order to visualize the data
the observations cell lines corresponding to given cancer type will be plotted in the same color so that we can see to what extent the observations within cancer type are similar to each other
we first create simple function that assigns distinct color to each element of numeric vector
the function will be used to assign color to each of the cell lines based on the cancer type to which it corresponds
cols function vec
projections of the nci cancer cell lines onto the first three principal components in other words the scores for the first three principal components
on the whole observations belonging to single cancer type tend to lie near each other in this low dimensional space
it would not have been possible to visualize the data without using dimension reduction method such as pca since based on the full data set there are possible scatterplots none of which would have been particularly informative cols rainbow length unique vec return cols as numeric as factor vec note that the rainbow function takes as its argument positive integer rainbow and returns vector containing that number of distinct colors
we now can plot the principal component score vectors par mfrow plot pr out col cols nci labs pch xlab ylab plot pr out col cols nci labs pch xlab ylab the resulting plots are shown in figure
on the whole cell lines corresponding to single cancer type do tend to have similar values on the first few principal component score vectors
this indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels
we can obtain summary of the proportion of variance explained pve of the first few principal components using the summary method for prcomp object we have truncated the printout summary pr out importance of components pc pc pc pc pc
the pve of the principal components of the nci cancer cell line microarray data set
left the pve of each principal component is shown
right the cumulative pve of the principal components is shown
together all principal components explain of the variance
standard deviation proportion of variance cumulative proportion using the plot function we can also plot the variance explained by the first few principal components plot pr out note that the height of each bar in the bar plot is given by squaring the corresponding element of pr out sdev
however it is more informative to plot the pve of each principal component scree plot and the cumulative pve of each principal component
this can be done with just little work pve pr out sdev sum pr out sdev par mfrow plot pve type ylab pve xlab principal component col blue plot cumsum pve type ylab cumulativ pve xlab principal component col brown note that the elements of pve can also be computed directly from the summary summary pr out importance and the elements of cumsum pve are given by summary pr out importance
the resulting plots are shown in figure
we see that together the first seven principal components explain around of the variance in the data
this is not huge amount of the variance
however looking at the scree plot we see that while each of the first seven principal components explain substantial amount of variance there is marked decrease in the variance explained by further
unsupervised learning principal components
that is there is an elbow in the plot after approximately the seventh principal component
this suggests that there may be little benefit to examining more than seven or so principal components though even examining seven principal components may be difficult
clustering the observations of the nci data we now proceed to hierarchically cluster the cell lines in the nci data with the goal of finding out whether or not the observations cluster into distinct types of cancer
to begin we scale the variables to have standard deviation one
as mentioned earlier this step is optional and should be performed only if we want each gene to be on the same scale sd data scale nci data false true we now perform hierarchical clustering of the observations using complete single and average linkage
euclidean distance is used as the dissimilarity measure par mfrow data dist dist sd data plot hclust data dist labels nci labs main complete linkage xlab sub ylab plot hclust data dist method average labels nci labs main average linkage xlab sub ylab plot hclust data dist method single labels nci labs main single linkage xlab sub ylab the results are shown in figure
we see that the choice of linkage certainly does affect the results obtained
typically single linkage will tend to yield trailing clusters very large clusters onto which individual samples attach one by one
on the other hand complete and average linkage tend to yield more balanced attractive clusters
for this reason complete and average linkage are generally preferred to single linkage
clearly cell lines within single cancer type do tend to cluster together although the clustering is not perfect
we will use complete linkage hierarchical clustering for the analysis that follows
we can cut the dendrogram at the height that will yield particular number of clusters say four hc out hclust dist sd data hc clusters cutree hc out table hc clusters nci labs there are some clear patterns
all the leukemia cell lines fall in cluster while the breast cancer cell lines are spread out over three different clusters
we can plot the cut on the dendrogram that produces these four clusters par mfrow plot hc out labels nci labs abline col red
fused one by one whereas single linkage tends to yield extended clusters to which single leaves are ilarity measure
the nci cancer cell line microarray data clustered with av leukemia leukemia breast renal leukemia breast breast leukemia cns leukemia leukemia cns leukemia leukemia renal cns leukemia breast leukemia repro nsclc leukemia repro renal repro renal melanoma repro nsclc ovarian nsclc breast ovarian leukemia nsclc nsclc ovarian breast ovarian nsclc mcf repro colon cns breast colon breast mcf repro ovarian nsclc colon prostate ovarian colon nsclc colon colon nsclc breast renal nsclc melanoma melanoma prostate renal melanoma nsclc melanoma breast melanoma breast breast renal breast melanoma renal melanoma melanoma renal complete linkage average linkage melanoma melanoma ovarian single linkage melanoma melanoma unknown melanoma melanoma ovarian melanoma ovarian nsclc lab nci data example breast ovarian cns ovarian nsclc cns colon ovarian cns mcf repro unknown nsclc breast ovarian renal mcf repro nsclc renal unknown melanoma renal ovarian cns renal nsclc cns nsclc nsclc cns melanoma prostate renal melanoma melanoma renal melanoma colon renal melanoma ovarian renal melanoma nsclc renal melanoma renal renal breast colon renal breast prostate prostate colon colon nsclc colon ovarian nsclc colon colon nsclc colon colon nsclc colon nsclc ovarian breast nsclc prostate mcf repro renal nsclc breast nsclc colon mcf repro renal colon leukemia renal ovarian leukemia renal colon leukemia renal colon leukemia renal cns repro cns cns repro cns breast leukemia cns breast leukemia
unsupervised learning the abline function draws straight line on top of any existing plot in
the argument plots horizontal line at height on the dendrogram this is the height that results in four distinct clusters
it is easy to verify that the resulting clusters are the same as the ones we obtained using cutree hc out
printing the output of hclust gives useful brief summary of the object hc out call hclust dist dat cluster method complete distance euclidean number of objects we claimed earlier in section that means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters can yield very different results
how do these nci hierarchical clustering results compare to what we get if we perform means clustering with
set seed km out kmeans sd data nstart km clusters km out clust table km clusters hc clusters hc clusters km clusters we see that the four clusters obtained using hierarchical clustering and kmeans clustering are somewhat different
cluster in means clustering is identical to cluster in hierarchical clustering
however the other clusters differ for instance cluster in means clustering contains portion of the observations assigned to cluster by hierarchical clustering as well as all of the observations assigned to cluster by hierarchical clustering
rather than performing hierarchical clustering on the entire data matrix we can simply perform hierarchical clustering on the first few principal component score vectors as follows hc out hclust dist pr out plot hc out labels nci labs main hier
clust on first five score vectors table cutree hc out nci labs not surprisingly these results are different from the ones that we obtained when we performed hierarchical clustering on the full data set
sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data
in this
exercises situation we might view the principal component step as one of denoising the data
we could also perform means clustering on the first few principal component score vectors rather than the full data set
exercises conceptual
this problem involves the means clustering algorithm prove on the basis of this identity argue that the means clustering algorithm algorithm decreases the objective at each iteration
suppose that we have four observations for which we compute dissimilarity matrix given by ee ef fa ef fa
fb for instance the dissimilarity between the first and second observations is and the dissimilarity between the second and fourth observations is on the basis of this dissimilarity matrix sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage
be sure to indicate on the plot the height at which each fusion occurs as well as the observations corresponding to each leaf in the dendrogram repeat this time using single linkage clustering suppose that we cut the dendogram obtained in such that two clusters result
which observations are in each cluster
suppose that we cut the dendogram obtained in such that two clusters result
which observations are in each cluster
it is mentioned in the chapter that at each fusion in the dendrogram the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram
draw dendrogram that is equivalent to the dendrogram in for which two or more of the leaves are repositioned but for which the meaning of the dendrogram is the same
unsupervised learning
in this problem you will perform means clustering manually with on small example with observations and features
the observations are as follows
plot the observations randomly assign cluster label to each observation
you can use the sample command in to do this
report the cluster labels for each observation compute the centroid for each cluster assign each observation to the centroid to which it is closest in terms of euclidean distance
report the cluster labels for each observation repeat and until the answers obtained stop changing in your plot from color the observations according to the cluster labels obtained
suppose that for particular data set we perform hierarchical clustering using single linkage and using complete linkage
we obtain two dendrograms at certain point on the single linkage dendrogram the clusters and fuse
on the complete linkage dendrogram the clusters and also fuse at certain point
which fusion will occur higher on the tree or will they fuse at the same height or is there not enough information to tell
at certain point on the single linkage dendrogram the clusters and fuse
on the complete linkage dendrogram the clusters and also fuse at certain point
which fusion will occur higher on the tree or will they fuse at the same height or is there not enough information to tell
in words describe the results that you would expect if you performed means clustering of the eight shoppers in figure on the basis of their sock and computer purchases with
give three answers one for each of the variable scalings displayed
researcher collects expression measurements for genes in tissue samples
the data be written as matrix which we call in which each row represents gene and each column tissue sample
each tissue sample was processed on different day and the columns of are ordered so that the samples that were processed earliest are on the left and the samples that were processed later are on the right
the tissue samples belong to two groups control and treatment
the and samples were processed in random order across the days
the researcher wishes to determine whether each gene's expression measurements differ between the treatment and control groups
as pre analysis before comparing versus the researcher performs principal component analysis of the data and finds that the first principal component vector of length has strong linear trend from left to right and explains of the variation
the researcher now remembers that each patient sample was run on one of two machines and and machine was used more often in the earlier times while was used more often later
the researcher has record of which sample was run on which machine explain what it means that the first principal component explains of the variation the researcher decides to replace the th element of with xij zi where zi is the ith score and is the jth loading for the first principal component
he will then perform two sample test on each gene in this new data set in order to determine whether its expression differs between the two conditions
critique this idea and suggest better approach design and run small simulation experiment to demonstrate the superiority of your idea
in the chapter we mentioned the use of euclidean distance and squared euclidean distance as dissimilarity measures for hierarchical clustering
it turns out that these two measures are almost equivalent if each observation has been centered to have mean zero and standard deviation one and if we let rij denote the correlation between the ith and jth observations then the quantity rij is proportional to the squared euclidean distance between the ith and jth observations
unsupervised learning on the usarrests data show that this proportionality holds
hint the euclidean distance can be calculated using the dist function and correlations can be calculated using the cor function
in section formula for calculating pve was given in equation
we also saw that the pve can be obtained using the sdev output of the prcomp function
on the usarrests data calculate pve in two ways using the sdev output of the prcomp function as was done in section by applying equation directly
that is use the prcomp function to compute the principal component loadings
then use those loadings in equation to obtain the pve
these two approaches should give the same results
hint you will only obtain the same results in and if the same data is used in both cases
for instance if in you performed prcomp using centered and scaled variables then you must center and scale the variables before applying equation in
consider the usarrests data
we will now perform hierarchical clustering on the states using hierarchical clustering with complete linkage and euclidean distance cluster the states cut the dendrogram at height that results in three distinct clusters
which states belong to which clusters
hierarchically cluster the states using complete linkage and euclidean distance after scaling the variables to have standard deviation one what effect does scaling the variables have on the hierarchical clustering obtained
in your opinion should the variables be scaled before the inter observation dissimilarities are computed
provide justification for your answer
in this problem you will generate simulated data and then perform pca and means clustering on the data generate simulated data set with observations in each of three classes observations total and variables
hint there are number of functions in that you can use to generate data
one example is the rnorm function runif is another option
be sure to add mean shift to the observations in each class so that there are three distinct classes
exercises perform pca on the observations and plot the first two principal component score vectors
use different color to indicate the observations in each of the three classes
if the three classes appear separated in this plot then continue on to part
if not then return to part and modify the simulation so that there is greater separation between the three classes
do not continue to part until the three classes show at least some separation in the first two principal component score vectors perform means clustering of the observations with
how well do the clusters that you obtained in means clustering compare to the true class labels
hint you can use the table function in to compare the true class labels to the class labels obtained by clustering
be careful how you interpret the results means clustering will arbitrarily number the clusters so you cannot simply check whether the true class labels and clustering labels are the same perform means clustering with
describe your results now perform means clustering with and describe your results now perform means clustering with on the first two principal component score vectors rather than on the raw data
that is perform means clustering on the matrix of which the first column is the first principal component score vector and the second column is the second principal component score vector
comment on the results using the scale function perform means clustering with on the data after scaling each variable to have standard deviation one
how do these results compare to those obtained in
on the book website www statlearning com there is gene expression data set ch ex csv that consists of tissue samples with measurements on genes
the first samples are from healthy patients while the second are from diseased group load in the data using read csv
you will need to select header apply hierarchical clustering to the samples using correlationbased distance and plot the dendrogram
do the genes separate the samples into the two groups
do your results depend on the type of linkage used
your collaborator wants to know which genes differ the most across the two groups
suggest way to answer this question and apply it here
this is page printer opaque this index cp backfitting backward stepwise selection norm norm bagging additive baseline additivity basis function adjusted bayes classifier advertising data set decision boundary error bayes theorem bayesian agglomerative clustering bayesian information criterion akaike information criterion best subset selection alternative hypothesis analysis of variance bias area under the curve bias variance argument decomposition auc trade off auto data set binary biplot
index boolean curse of dimensionality boosting bootstrap boston data set data sets advertising bottom up clustering boxplot branch auto caravan data set carseats data set boston categorical classification caravan error rate carseats tree classifier college cluster analysis credit clustering means default agglomerative bottom up hierarchical heart coefficient college data set hitters collinearity conditional probability confidence interval income khan confounding confusion matrix nci continuous oj contour plot portfolio contrast smarket correlation credit data set usarrests cross entropy wage cross validation fold weekly leave one out decision tree
index default data set for loop forward stepwise selection degrees of freedom function dendrogram gaussian normal distribution density function dependent variable generalized additive model derivative deviance generalized linear model dimension reduction discriminant function gini index dissimilarity distance heart data set correlation based euclidean heatmap heteroscedasticity double exponential distribution hierarchical clustering dummy variable dendrogram inversion linkage effective degrees of freedom hierarchical principle elbow high dimensional error hinge loss irreducible histogram rate hitters data set reducible term euclidean distance hold out set hyperplane expected value hypothesis test exploratory data analysis income data set statistic independent variable factor indicator function false discovery proportion inference false negative inner product false positive input variable false positive rate integral feature interaction feature selection fisher's linear discriminant intercept fit interpretability fitted value inversion flexible irreducible error
index means clustering logistic regression nearest neighbors classifier regression multiple kernel logit linear loss function non linear low dimensional polynomial radial main effects majority vote kernel trick mallow's cp khan data set knot margin matrix multiplication laplace distribution maximal margin lasso classifier hyperplane leaf maximum likelihood least squares mean squared error misclassification error line missing data weighted mixed selection level model assessment leverage model selection likelihood function multicollinearity linear multivariate gaussian linear combination multivariate normal natural spline linear discriminant analysis nci data set negative predictive value node linear kernel internal linear model purity linear regression terminal multiple noise simple non linear linkage decision boundary average kernel centroid non parametric complete single normal gaussian distribution local regression logistic null function hypothesis
index model regression odds score vector oj data set scree plot one standard error rule prior one versus all distribution one versus one probability optimal separating hyperplane projection optimism of training error pruning ordered categorical variable cost complexity orthogonal weakest link basis out of bag quadratic outlier quadratic discriminant analysis output variable overfitting qualitative variable quantitative value parameter functions parametric abline partial least squares anova apply path algorithm as dist perpendicular as factor polynomial attach kernel biplot regression boot population regression line bs portfolio data set positive predictive value cbind posterior coef distribution confint mode contour probability contrasts power cor precision cumsum prediction cut interval cutree predictor cv glm principal components cv glmnet analysis cv tree loading vector data frame proportion of variance explained data frame dev off
index dim persp dist plot fix for gam plot gam gbm plot svm glm plsr points glmnet poly hatvalues prcomp hclust predict hist identify ifelse print image prune misclass importance prune tree is na jitter qda jpeg quantile kmeans rainbow knn randomforest lda range legend read csv length read table library regsubsets lines residuals lm return rm rnorm lo loadhistory rstudent loess runif ls matrix sample mean savehistory median scale model matrix sd na omit seq names set seed ns smooth spline pairs sqrt par sum pcr summary pdf
index roc curve rug plot svm scale invariant table scatterplot text scatterplot matrix title scree plot tree elbow tune seed update semi supervised learning validationplot sensitivity var separating hyperplane varimpplot shrinkage vif penalty which max signal which min slack variable write table slope radial kernel smarket data set random forest smoother recall smoothing spline receiver operating characteristic roc soft margin classifier recursive binary splitting soft thresholding sparse reducible error sparsity regression specificity local spline piecewise polynomial cubic polynomial linear spline natural tree regression regularization smoothing replacement thin plate resampling standard error residual standardize plot statistical model standard error step function stepwise model selection studentized sum of squares stump residuals subset selection response subtree ridge regression supervised learning robust support vector
index classifier output machine qualitative regression selection synergy variance systematic inflation factor varying coefficient model distribution vector statistic test wage data set error mse observations weakest link pruning set weekly data set time series weighted least squares total sum of squares within class covariance tracking workspace train wrapper training data error mse tree tree based method true negative true positive true positive rate truncated power basis tuning parameter type error type ii error unsupervised learning usarrests data set validation set approach variable dependent dummy importance independent indicator input
